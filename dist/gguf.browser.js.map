{"version":3,"file":"gguf.browser.js","sources":["../src/core/model.js","../src/utils/logger.js","../src/core/loader.js","../src/core/inference.js","../src/core/tokenizer.js","../src/adapters/node.js","../src/adapters/browser.js","../src/adapters/react.js","../src/utils/memory.js","../src/utils/streaming.js","../src/utils/formats.js","../src/index.js"],"sourcesContent":["/**\r\n * Core model representation for GGUF models\r\n * @module core/model\r\n */\r\n\r\n/**\r\n * Class representing a GGUF model\r\n */\r\nclass GGUFModel {\r\n    /**\r\n     * Create a GGUF model\r\n     * @param {Object} options - Model options\r\n     * @param {string} options.id - Unique identifier for the model\r\n     * @param {string} options.path - Path or URL to the model file\r\n     * @param {Object} options.metadata - Model metadata\r\n     * @param {Object} options.session - Model session (implementation specific)\r\n     * @param {Object} options.tokenizer - Tokenizer for the model\r\n     */\r\n    constructor({ id, path, metadata = {}, session = null, tokenizer = null }) {\r\n      this.id = id;\r\n      this.path = path;\r\n      this.metadata = metadata;\r\n      this.session = session;\r\n      this.tokenizer = tokenizer;\r\n      this.lastUsed = Date.now();\r\n      this.isLoaded = false;\r\n      \r\n      // Add default metadata if not provided\r\n      if (!this.metadata.name) {\r\n        this.metadata.name = id || path.split('/').pop();\r\n      }\r\n      \r\n      if (!this.metadata.createdAt) {\r\n        this.metadata.createdAt = new Date().toISOString();\r\n      }\r\n    }\r\n    \r\n    /**\r\n     * Get model basic information\r\n     * @returns {Object} Model info\r\n     */\r\n    getInfo() {\r\n      return {\r\n        id: this.id,\r\n        name: this.metadata.name,\r\n        path: this.path,\r\n        isLoaded: this.isLoaded,\r\n        lastUsed: this.lastUsed,\r\n        ...this.metadata\r\n      };\r\n    }\r\n    \r\n    /**\r\n     * Updates the model's last used timestamp\r\n     */\r\n    updateLastUsed() {\r\n      this.lastUsed = Date.now();\r\n    }\r\n    \r\n    /**\r\n     * Unload the model to free resources\r\n     */\r\n    unload() {\r\n      this.session = null;\r\n      this.isLoaded = false;\r\n    }\r\n  }\r\n  \r\n  /**\r\n   * Generation options for text inference\r\n   */\r\n  class GenerationOptions {\r\n    /**\r\n     * Create generation options\r\n     * @param {Object} options - Generation parameters\r\n     * @param {number} [options.maxTokens=100] - Maximum tokens to generate\r\n     * @param {number} [options.temperature=0.7] - Sampling temperature (0-2)\r\n     * @param {number} [options.topP=0.9] - Top-p sampling parameter (0-1)\r\n     * @param {number} [options.topK=40] - Top-k sampling parameter\r\n     * @param {number} [options.repetitionPenalty=1.1] - Penalty for repetition\r\n     * @param {number} [options.seed] - Random seed for reproducibility\r\n     * @param {string[]} [options.stopSequences] - Sequences to stop generation\r\n     */\r\n    constructor({\r\n      maxTokens = 100,\r\n      temperature = 0.7,\r\n      topP = 0.9,\r\n      topK = 40,\r\n      repetitionPenalty = 1.1,\r\n      seed,\r\n      stopSequences = []\r\n    } = {}) {\r\n      this.maxTokens = maxTokens;\r\n      this.temperature = temperature;\r\n      this.topP = topP;\r\n      this.topK = topK;\r\n      this.repetitionPenalty = repetitionPenalty;\r\n      this.seed = seed;\r\n      this.stopSequences = stopSequences;\r\n    }\r\n  }\r\n  \r\n  /**\r\n   * Model context for token sequence processing\r\n   */\r\n  class ModelContext {\r\n    /**\r\n     * Create a model context\r\n     * @param {Object} options - Context options\r\n     * @param {number[]} [options.tokens=[]] - Input tokens\r\n     * @param {number} [options.contextSize=2048] - Maximum context size\r\n     */\r\n    constructor({ tokens = [], contextSize = 2048 } = {}) {\r\n      this.tokens = tokens;\r\n      this.contextSize = contextSize;\r\n      this.outputTokens = [];\r\n    }\r\n    \r\n    /**\r\n     * Add new tokens to the context\r\n     * @param {number[]} tokens - Tokens to add\r\n     */\r\n    addTokens(tokens) {\r\n      this.tokens = [...this.tokens, ...tokens];\r\n      \r\n      // Truncate if context exceeds limit\r\n      if (this.tokens.length > this.contextSize) {\r\n        this.tokens = this.tokens.slice(-this.contextSize);\r\n      }\r\n    }\r\n    \r\n    /**\r\n     * Add an output token\r\n     * @param {number} token - Token to add\r\n     */\r\n    addOutputToken(token) {\r\n      this.outputTokens.push(token);\r\n    }\r\n    \r\n    /**\r\n     * Get the current context length\r\n     * @returns {number} Context length\r\n     */\r\n    getContextLength() {\r\n      return this.tokens.length;\r\n    }\r\n    \r\n    /**\r\n     * Get the remaining context space\r\n     * @returns {number} Remaining space\r\n     */\r\n    getRemainingSpace() {\r\n      return this.contextSize - this.tokens.length;\r\n    }\r\n    \r\n    /**\r\n     * Clear all tokens from context\r\n     */\r\n    clear() {\r\n      this.tokens = [];\r\n      this.outputTokens = [];\r\n    }\r\n  }\r\n  \r\n  /**\r\n   * Create a model ID from a path\r\n   * @param {string} path - Model path\r\n   * @returns {string} Model ID\r\n   */\r\n  function createModelId(path) {\r\n    // Extract filename without extension\r\n    const filename = path.split('/').pop().split('.')[0];\r\n    \r\n    // Create a simple hash from the full path\r\n    let hash = 0;\r\n    for (let i = 0; i < path.length; i++) {\r\n      hash = ((hash << 5) - hash) + path.charCodeAt(i);\r\n      hash = hash & hash; // Convert to 32-bit integer\r\n    }\r\n    \r\n    return `${filename}-${Math.abs(hash).toString(16).substring(0, 8)}`;\r\n  }\r\n  \r\n  module.exports = {\r\n    GGUFModel,\r\n    GenerationOptions,\r\n    ModelContext,\r\n    createModelId\r\n  };","/**\r\n * Simple logging utilities for GGUF.js\r\n * @module utils/logger\r\n */\r\n\r\n// Log levels\r\nconst LOG_LEVELS = {\r\n    NONE: 0,\r\n    ERROR: 1,\r\n    WARN: 2,\r\n    INFO: 3,\r\n    DEBUG: 4\r\n  };\r\n  \r\n  // Default configuration\r\n  let config = {\r\n    level: process.env.NODE_ENV === 'production' ? LOG_LEVELS.INFO : LOG_LEVELS.DEBUG,\r\n    prefix: 'GGUF.js',\r\n    useColors: true,\r\n    timestamp: true,\r\n    showInConsole: true\r\n  };\r\n  \r\n  // Color codes for console output\r\n  const COLORS = {\r\n    reset: '\\x1b[0m',\r\n    black: '\\x1b[30m',\r\n    red: '\\x1b[31m',\r\n    green: '\\x1b[32m',\r\n    yellow: '\\x1b[33m',\r\n    blue: '\\x1b[34m',\r\n    magenta: '\\x1b[35m',\r\n    cyan: '\\x1b[36m',\r\n    white: '\\x1b[37m',\r\n    \r\n    // Level-specific colors\r\n    error: '\\x1b[31m', // red\r\n    warn: '\\x1b[33m',  // yellow\r\n    info: '\\x1b[36m',  // cyan\r\n    debug: '\\x1b[90m', // gray\r\n    prefix: '\\x1b[35m' // magenta\r\n  };\r\n  \r\n  // Store for log entries (can be used for log retrieval in testing/debug UIs)\r\n  const logStore = [];\r\n  const MAX_LOG_STORE = 1000; // Maximum number of entries to keep\r\n  \r\n  /**\r\n   * Configure the logger\r\n   * @param {Object} options - Configuration options\r\n   * @param {string} options.level - Log level ('none', 'error', 'warn', 'info', 'debug')\r\n   * @param {string} options.prefix - Prefix for log messages\r\n   * @param {boolean} options.useColors - Whether to use colors in console output\r\n   * @param {boolean} options.timestamp - Whether to include timestamps\r\n   * @param {boolean} options.showInConsole - Whether to show logs in console\r\n   */\r\n  function configure(options = {}) {\r\n    if (options.level !== undefined) {\r\n      if (typeof options.level === 'string') {\r\n        const levelName = options.level.toUpperCase();\r\n        if (LOG_LEVELS[levelName] !== undefined) {\r\n          config.level = LOG_LEVELS[levelName];\r\n        }\r\n      } else if (typeof options.level === 'number') {\r\n        config.level = options.level;\r\n      }\r\n    }\r\n    \r\n    if (options.prefix !== undefined) config.prefix = options.prefix;\r\n    if (options.useColors !== undefined) config.useColors = !!options.useColors;\r\n    if (options.timestamp !== undefined) config.timestamp = !!options.timestamp;\r\n    if (options.showInConsole !== undefined) config.showInConsole = !!options.showInConsole;\r\n  }\r\n  \r\n  /**\r\n   * Format a log message\r\n   * @private\r\n   * @param {string} level - Log level name\r\n   * @param {string} message - Log message\r\n   * @param {Object|undefined} data - Additional data to log\r\n   * @returns {string} Formatted log message\r\n   */\r\n  function formatMessage(level, message, data) {\r\n    const parts = [];\r\n    \r\n    // Add timestamp if enabled\r\n    if (config.timestamp) {\r\n      const now = new Date().toISOString();\r\n      if (config.useColors) {\r\n        parts.push(`${COLORS.debug}[${now}]${COLORS.reset}`);\r\n      } else {\r\n        parts.push(`[${now}]`);\r\n      }\r\n    }\r\n    \r\n    // Add prefix if set\r\n    if (config.prefix) {\r\n      if (config.useColors) {\r\n        parts.push(`${COLORS.prefix}[${config.prefix}]${COLORS.reset}`);\r\n      } else {\r\n        parts.push(`[${config.prefix}]`);\r\n      }\r\n    }\r\n    \r\n    // Add level\r\n    if (config.useColors) {\r\n      parts.push(`${COLORS[level.toLowerCase()]}[${level}]${COLORS.reset}`);\r\n    } else {\r\n      parts.push(`[${level}]`);\r\n    }\r\n    \r\n    // Add message\r\n    parts.push(message);\r\n    \r\n    // Format as string\r\n    return parts.join(' ');\r\n  }\r\n  \r\n  /**\r\n   * Log a message at a specific level\r\n   * @private\r\n   * @param {string} level - Log level name\r\n   * @param {string} message - Log message\r\n   * @param {Object|undefined} data - Additional data to log\r\n   */\r\n  function log(level, message, data) {\r\n    const levelValue = LOG_LEVELS[level.toUpperCase()];\r\n    \r\n    // Skip if log level is too low\r\n    if (levelValue > config.level) return;\r\n    \r\n    // Format the message\r\n    const formattedMessage = formatMessage(level, message, data);\r\n    \r\n    // Store in log history\r\n    const logEntry = {\r\n      level,\r\n      message,\r\n      data,\r\n      timestamp: new Date(),\r\n      formatted: formattedMessage\r\n    };\r\n    \r\n    logStore.push(logEntry);\r\n    if (logStore.length > MAX_LOG_STORE) {\r\n      logStore.shift(); // Remove oldest entry\r\n    }\r\n    \r\n    // Output to console if enabled\r\n    if (config.showInConsole) {\r\n      const consoleMethod = level.toLowerCase();\r\n      \r\n      if (console[consoleMethod]) {\r\n        if (data !== undefined) {\r\n          console[consoleMethod](formattedMessage, data);\r\n        } else {\r\n          console[consoleMethod](formattedMessage);\r\n        }\r\n      } else {\r\n        console.log(formattedMessage);\r\n      }\r\n    }\r\n    \r\n    return logEntry;\r\n  }\r\n  \r\n  /**\r\n   * Log an error message\r\n   * @param {string} message - Error message\r\n   * @param {Error|Object|undefined} error - Error object or additional data\r\n   */\r\n  function error(message, error) {\r\n    let errorData;\r\n    \r\n    if (error instanceof Error) {\r\n      errorData = {\r\n        name: error.name,\r\n        message: error.message,\r\n        stack: error.stack\r\n      };\r\n    } else {\r\n      errorData = error;\r\n    }\r\n    \r\n    return log('ERROR', message, errorData);\r\n  }\r\n  \r\n  /**\r\n   * Log a warning message\r\n   * @param {string} message - Warning message\r\n   * @param {Object|undefined} data - Additional data\r\n   */\r\n  function warn(message, data) {\r\n    return log('WARN', message, data);\r\n  }\r\n  \r\n  /**\r\n   * Log an info message\r\n   * @param {string} message - Info message\r\n   * @param {Object|undefined} data - Additional data\r\n   */\r\n  function info(message, data) {\r\n    return log('INFO', message, data);\r\n  }\r\n  \r\n  /**\r\n   * Log a debug message\r\n   * @param {string} message - Debug message\r\n   * @param {Object|undefined} data - Additional data\r\n   */\r\n  function debug(message, data) {\r\n    return log('DEBUG', message, data);\r\n  }\r\n  \r\n  /**\r\n   * Get all stored log entries\r\n   * @returns {Array} Array of log entries\r\n   */\r\n  function getLogs() {\r\n    return [...logStore];\r\n  }\r\n  \r\n  /**\r\n   * Clear stored log entries\r\n   */\r\n  function clearLogs() {\r\n    logStore.length = 0;\r\n  }\r\n  \r\n  /**\r\n   * Create a logger instance with its own prefix\r\n   * @param {string} prefix - Logger prefix\r\n   * @returns {Object} Logger instance\r\n   */\r\n  function createLogger(prefix) {\r\n    return {\r\n      error: (message, data) => error(`[${prefix}] ${message}`, data),\r\n      warn: (message, data) => warn(`[${prefix}] ${message}`, data),\r\n      info: (message, data) => info(`[${prefix}] ${message}`, data),\r\n      debug: (message, data) => debug(`[${prefix}] ${message}`, data)\r\n    };\r\n  }\r\n  \r\n  module.exports = {\r\n    configure,\r\n    error,\r\n    warn,\r\n    info,\r\n    debug,\r\n    getLogs,\r\n    clearLogs,\r\n    createLogger,\r\n    LOG_LEVELS\r\n  };","/**\r\n * GGUF model loader\r\n * @module core/loader\r\n */\r\n\r\nconst { GGUFModel, createModelId } = require('./model');\r\nconst logger = require('../utils/logger');\r\n\r\n/**\r\n * Model loading options\r\n * @typedef {Object} LoadOptions\r\n * @property {boolean} [useCache=true] - Whether to cache the model\r\n * @property {string} [quantization='q4_0'] - Quantization format to use\r\n * @property {number} [contextSize=2048] - Model context size\r\n * @property {boolean} [lowMemory=false] - Whether to use low memory mode\r\n * @property {string} [modelId] - Custom model ID (auto-generated if not provided)\r\n * @property {Object} [metadata] - Additional model metadata\r\n */\r\n\r\n/**\r\n * Registry of loaded models\r\n * @type {Map<string, GGUFModel>}\r\n */\r\nconst loadedModels = new Map();\r\n\r\n/**\r\n * Load a GGUF model\r\n * @param {string} path - Path or URL to the model file\r\n * @param {LoadOptions} [options={}] - Model loading options\r\n * @param {Object} [adapter] - Environment-specific adapter\r\n * @returns {Promise<GGUFModel>} Loaded model\r\n */\r\nasync function loadModel(path, options = {}, adapter) {\r\n  if (!adapter) {\r\n    throw new Error('No adapter provided. Please use an environment-specific loader.');\r\n  }\r\n  \r\n  const {\r\n    useCache = true,\r\n    quantization = 'q4_0',\r\n    contextSize = 2048,\r\n    lowMemory = false,\r\n    modelId: customModelId,\r\n    metadata = {}\r\n  } = options;\r\n  \r\n  // Generate or use provided model ID\r\n  const modelId = customModelId || createModelId(path);\r\n  \r\n  // Check if model is already loaded\r\n  if (useCache && loadedModels.has(modelId)) {\r\n    logger.info(`Using cached model: ${modelId}`);\r\n    const cachedModel = loadedModels.get(modelId);\r\n    cachedModel.updateLastUsed();\r\n    return cachedModel;\r\n  }\r\n  \r\n  logger.info(`Loading model from: ${path}`);\r\n  logger.debug('Loading options:', { quantization, contextSize, lowMemory });\r\n  \r\n  try {\r\n    // Create model instance\r\n    const model = new GGUFModel({\r\n      id: modelId,\r\n      path,\r\n      metadata: {\r\n        ...metadata,\r\n        quantization,\r\n        contextSize,\r\n        lowMemory\r\n      }\r\n    });\r\n    \r\n    // Initialize the model with the adapter\r\n    const { session, tokenizer } = await adapter.initializeModel(path, {\r\n      quantization,\r\n      contextSize,\r\n      lowMemory\r\n    });\r\n    \r\n    model.session = session;\r\n    model.tokenizer = tokenizer;\r\n    model.isLoaded = true;\r\n    \r\n    // Store in cache if requested\r\n    if (useCache) {\r\n      loadedModels.set(modelId, model);\r\n    }\r\n    \r\n    logger.info(`Model loaded successfully: ${modelId}`);\r\n    return model;\r\n  } catch (error) {\r\n    logger.error(`Failed to load model: ${error.message}`);\r\n    throw new Error(`Failed to load GGUF model: ${error.message}`);\r\n  }\r\n}\r\n\r\n/**\r\n * Unload a model from memory\r\n * @param {string} modelId - Model ID to unload\r\n * @param {Object} [adapter] - Environment-specific adapter\r\n * @returns {boolean} Whether the model was unloaded\r\n */\r\nfunction unloadModel(modelId, adapter) {\r\n  if (loadedModels.has(modelId)) {\r\n    const model = loadedModels.get(modelId);\r\n    \r\n    // Use adapter to free resources if provided\r\n    if (adapter && adapter.freeModel) {\r\n      adapter.freeModel(model);\r\n    }\r\n    \r\n    // Unload the model\r\n    model.unload();\r\n    loadedModels.delete(modelId);\r\n    \r\n    logger.info(`Model unloaded: ${modelId}`);\r\n    return true;\r\n  }\r\n  \r\n  logger.warn(`Model not found for unloading: ${modelId}`);\r\n  return false;\r\n}\r\n\r\n/**\r\n * Get a loaded model by ID\r\n * @param {string} modelId - Model ID\r\n * @returns {GGUFModel|null} The model or null if not found\r\n */\r\nfunction getModel(modelId) {\r\n  if (loadedModels.has(modelId)) {\r\n    const model = loadedModels.get(modelId);\r\n    model.updateLastUsed();\r\n    return model;\r\n  }\r\n  return null;\r\n}\r\n\r\n/**\r\n * List all loaded models\r\n * @returns {GGUFModel[]} Array of loaded models\r\n */\r\nfunction listModels() {\r\n  return Array.from(loadedModels.values())\r\n    .map(model => model.getInfo())\r\n    .sort((a, b) => b.lastUsed - a.lastUsed);\r\n}\r\n\r\n/**\r\n * Clear all loaded models from memory\r\n * @param {Object} [adapter] - Environment-specific adapter\r\n */\r\nfunction clearModels(adapter) {\r\n  for (const [modelId, model] of loadedModels.entries()) {\r\n    // Use adapter to free resources if provided\r\n    if (adapter && adapter.freeModel) {\r\n      adapter.freeModel(model);\r\n    }\r\n    \r\n    model.unload();\r\n  }\r\n  \r\n  loadedModels.clear();\r\n  logger.info('All models unloaded from memory');\r\n}\r\n\r\nmodule.exports = {\r\n  loadModel,\r\n  unloadModel,\r\n  getModel,\r\n  listModels,\r\n  clearModels\r\n};","/**\r\n * GGUF inference engine\r\n * @module core/inference\r\n */\r\n\r\nconst { ModelContext, GenerationOptions } = require('./model');\r\nconst logger = require('../utils/logger');\r\n\r\n/**\r\n * Global cancellation flag for inference\r\n * @type {Map<string, boolean>}\r\n */\r\nconst cancellationFlags = new Map();\r\n\r\n/**\r\n * Run text generation with a model\r\n * @param {GGUFModel} model - The loaded model\r\n * @param {string} prompt - Input prompt\r\n * @param {Object} [options={}] - Generation options\r\n * @param {Object} [adapter] - Environment-specific adapter\r\n * @returns {Promise<string>} Generated text\r\n */\r\nasync function generate(model, prompt, options = {}, adapter) {\r\n  if (!adapter) {\r\n    throw new Error('No adapter provided. Please use an environment-specific inference engine.');\r\n  }\r\n  \r\n  if (!model || !model.isLoaded || !model.session) {\r\n    throw new Error('Model not loaded properly');\r\n  }\r\n  \r\n  // Reset cancellation flag\r\n  cancellationFlags.set(model.id, false);\r\n  \r\n  // Normalize options\r\n  const generationOptions = new GenerationOptions(options);\r\n  \r\n  logger.info(`Generating with model: ${model.id}`);\r\n  logger.debug('Prompt:', prompt.substring(0, 100) + (prompt.length > 100 ? '...' : ''));\r\n  logger.debug('Options:', generationOptions);\r\n  \r\n  try {\r\n    // Tokenize the prompt\r\n    const tokens = await adapter.tokenize(model, prompt);\r\n    logger.debug(`Tokenized prompt: ${tokens.length} tokens`);\r\n    \r\n    // Create model context\r\n    const context = new ModelContext({\r\n      tokens,\r\n      contextSize: model.metadata.contextSize || 2048\r\n    });\r\n    \r\n    // Check context length\r\n    if (context.getContextLength() >= context.contextSize) {\r\n      logger.warn(`Prompt exceeds context size: ${context.getContextLength()} tokens`);\r\n      // Truncate context if necessary\r\n      context.tokens = context.tokens.slice(-context.contextSize + 100); // Leave room for generation\r\n    }\r\n    \r\n    // Reserve tokens for generation\r\n    const maxNewTokens = Math.min(\r\n      generationOptions.maxTokens,\r\n      context.getRemainingSpace() - 10 // Leave some buffer\r\n    );\r\n    \r\n    // Run the inference\r\n    const outputTokens = await adapter.runInference(\r\n      model,\r\n      context,\r\n      {\r\n        ...generationOptions,\r\n        maxTokens: maxNewTokens\r\n      },\r\n      () => cancellationFlags.get(model.id)\r\n    );\r\n    \r\n    // Detokenize the output\r\n    const outputText = await adapter.detokenize(model, outputTokens);\r\n    \r\n    logger.info(`Generated ${outputTokens.length} tokens`);\r\n    return outputText;\r\n  } catch (error) {\r\n    if (cancellationFlags.get(model.id)) {\r\n      logger.info('Generation was cancelled');\r\n      return '';\r\n    }\r\n    \r\n    logger.error(`Generation failed: ${error.message}`);\r\n    throw new Error(`Text generation failed: ${error.message}`);\r\n  } finally {\r\n    model.updateLastUsed();\r\n  }\r\n}\r\n\r\n/**\r\n * Token callback type \r\n * @callback TokenCallback\r\n * @param {string} token - Generated token\r\n * @param {boolean} isDone - Whether generation is complete\r\n */\r\n\r\n/**\r\n * Run streaming text generation with a model\r\n * @param {GGUFModel} model - The loaded model\r\n * @param {string} prompt - Input prompt\r\n * @param {TokenCallback} onToken - Callback for each token\r\n * @param {Object} [options={}] - Generation options\r\n * @param {Object} [adapter] - Environment-specific adapter\r\n * @returns {Promise<void>}\r\n */\r\nasync function streamGenerate(model, prompt, onToken, options = {}, adapter) {\r\n  if (!adapter) {\r\n    throw new Error('No adapter provided. Please use an environment-specific inference engine.');\r\n  }\r\n  \r\n  if (!model || !model.isLoaded || !model.session) {\r\n    throw new Error('Model not loaded properly');\r\n  }\r\n  \r\n  // Reset cancellation flag\r\n  cancellationFlags.set(model.id, false);\r\n  \r\n  // Normalize options\r\n  const generationOptions = new GenerationOptions(options);\r\n  \r\n  logger.info(`Streaming with model: ${model.id}`);\r\n  logger.debug('Prompt:', prompt.substring(0, 100) + (prompt.length > 100 ? '...' : ''));\r\n  \r\n  try {\r\n    // Tokenize the prompt\r\n    const tokens = await adapter.tokenize(model, prompt);\r\n    \r\n    // Create model context\r\n    const context = new ModelContext({\r\n      tokens,\r\n      contextSize: model.metadata.contextSize || 2048\r\n    });\r\n    \r\n    // Check context length\r\n    if (context.getContextLength() >= context.contextSize) {\r\n      logger.warn(`Prompt exceeds context size: ${context.getContextLength()} tokens`);\r\n      context.tokens = context.tokens.slice(-context.contextSize + 100);\r\n    }\r\n    \r\n    // Reserve tokens for generation\r\n    const maxNewTokens = Math.min(\r\n      generationOptions.maxTokens,\r\n      context.getRemainingSpace() - 10\r\n    );\r\n    \r\n    // Token callback wrapper\r\n    const tokenCallback = async (tokenId) => {\r\n      try {\r\n        const tokenText = await adapter.detokenizeToken(model, tokenId);\r\n        onToken(tokenText, false);\r\n      } catch (error) {\r\n        logger.error(`Token callback error: ${error.message}`);\r\n      }\r\n    };\r\n    \r\n    // Run streaming inference\r\n    await adapter.runInferenceStreaming(\r\n      model,\r\n      context,\r\n      tokenCallback,\r\n      {\r\n        ...generationOptions,\r\n        maxTokens: maxNewTokens\r\n      },\r\n      () => cancellationFlags.get(model.id)\r\n    );\r\n    \r\n    // Signal completion\r\n    onToken('', true);\r\n    logger.info('Streaming generation complete');\r\n  } catch (error) {\r\n    if (cancellationFlags.get(model.id)) {\r\n      logger.info('Streaming was cancelled');\r\n      onToken('', true);\r\n      return;\r\n    }\r\n    \r\n    logger.error(`Streaming failed: ${error.message}`);\r\n    onToken('', true);\r\n    throw new Error(`Streaming text generation failed: ${error.message}`);\r\n  } finally {\r\n    model.updateLastUsed();\r\n  }\r\n}\r\n\r\n/**\r\n * Cancel an ongoing generation\r\n * @param {GGUFModel|string} modelOrId - Model or model ID\r\n * @returns {boolean} Whether cancellation was set\r\n */\r\nfunction cancelGeneration(modelOrId) {\r\n  const modelId = typeof modelOrId === 'string' ? modelOrId : modelOrId.id;\r\n  \r\n  if (!modelId) {\r\n    logger.warn('No model ID provided for cancellation');\r\n    return false;\r\n  }\r\n  \r\n  logger.info(`Cancelling generation for model: ${modelId}`);\r\n  cancellationFlags.set(modelId, true);\r\n  return true;\r\n}\r\n\r\nmodule.exports = {\r\n  generate,\r\n  streamGenerate,\r\n  cancelGeneration\r\n};","/**\r\n * Tokenization utilities for GGUF models\r\n * @module core/tokenizer\r\n */\r\n\r\nconst logger = require('../utils/logger');\r\n\r\n/**\r\n * Base tokenizer interface\r\n */\r\nclass BaseTokenizer {\r\n  /**\r\n   * Encode text to token IDs\r\n   * @param {string} text - Text to encode\r\n   * @returns {Array<number>} Token IDs\r\n   */\r\n  encode(text) {\r\n    throw new Error('encode() method must be implemented by subclass');\r\n  }\r\n\r\n  /**\r\n   * Decode token IDs to text\r\n   * @param {Array<number>} tokens - Token IDs to decode\r\n   * @returns {string} Decoded text\r\n   */\r\n  decode(tokens) {\r\n    throw new Error('decode() method must be implemented by subclass');\r\n  }\r\n\r\n  /**\r\n   * Decode a single token ID to text\r\n   * @param {number} token - Token ID to decode\r\n   * @returns {string} Decoded text for the token\r\n   */\r\n  decodeToken(token) {\r\n    return this.decode([token]);\r\n  }\r\n}\r\n\r\n/**\r\n * Simple fallback tokenizer that works at character level\r\n * Used when a model doesn't provide its own tokenizer\r\n */\r\nclass CharacterTokenizer extends BaseTokenizer {\r\n  constructor() {\r\n    super();\r\n    this.encoder = new Map();\r\n    this.decoder = new Map();\r\n    \r\n    // Initialize with basic ASCII characters\r\n    for (let i = 0; i < 128; i++) {\r\n      const char = String.fromCharCode(i);\r\n      this.encoder.set(char, i);\r\n      this.decoder.set(i, char);\r\n    }\r\n\r\n    // Add special tokens\r\n    this.specialTokens = {\r\n      BOS: 128, // Beginning of sequence\r\n      EOS: 129, // End of sequence\r\n      PAD: 130, // Padding\r\n      UNK: 131, // Unknown\r\n    };\r\n\r\n    Object.entries(this.specialTokens).forEach(([name, id]) => {\r\n      this.decoder.set(id, `<${name}>`);\r\n    });\r\n  }\r\n\r\n  /**\r\n   * Encode text to token IDs\r\n   * @param {string} text - Text to encode\r\n   * @returns {Array<number>} Token IDs\r\n   */\r\n  encode(text) {\r\n    return Array.from(text).map(char => {\r\n      if (this.encoder.has(char)) {\r\n        return this.encoder.get(char);\r\n      }\r\n      return this.specialTokens.UNK; // Unknown token\r\n    });\r\n  }\r\n\r\n  /**\r\n   * Decode token IDs to text\r\n   * @param {Array<number>} tokens - Token IDs to decode\r\n   * @returns {string} Decoded text\r\n   */\r\n  decode(tokens) {\r\n    return tokens.map(token => {\r\n      if (this.decoder.has(token)) {\r\n        return this.decoder.get(token);\r\n      }\r\n      return ''; // Skip unknown tokens\r\n    }).join('');\r\n  }\r\n}\r\n\r\n/**\r\n * BPE (Byte-Pair Encoding) tokenizer\r\n * Basic implementation of BPE tokenization used by many LLMs\r\n */\r\nclass BPETokenizer extends BaseTokenizer {\r\n  /**\r\n   * Create a BPE tokenizer\r\n   * @param {Object} config - Tokenizer configuration\r\n   * @param {Object} config.vocab - Vocabulary mapping (token string to ID)\r\n   * @param {Array<Array<string>>} config.merges - BPE merge rules\r\n   */\r\n  constructor(config) {\r\n    super();\r\n    this.vocab = new Map(Object.entries(config.vocab || {}).map(([k, v]) => [k, Number(v)]));\r\n    this.decoder = new Map(Array.from(this.vocab.entries()).map(([k, v]) => [v, k]));\r\n    this.merges = config.merges || [];\r\n    this.specialTokens = config.specialTokens || {};\r\n    \r\n    // Build merge pattern cache\r\n    this.mergePatterns = new Map();\r\n    this.buildMergePatterns();\r\n  }\r\n\r\n  /**\r\n   * Build cached patterns for BPE merges\r\n   */\r\n  buildMergePatterns() {\r\n    this.merges.forEach((pair, i) => {\r\n      const [first, second] = pair;\r\n      const pattern = new RegExp(`(?<=^|\\\\s)(${first})(${second})(?=\\\\s|$)`, 'g');\r\n      this.mergePatterns.set(i, {\r\n        pattern,\r\n        replacement: `${first}${second}`\r\n      });\r\n    });\r\n  }\r\n\r\n  /**\r\n   * Encode text using BPE algorithm\r\n   * @param {string} text - Text to encode\r\n   * @returns {Array<number>} Token IDs\r\n   */\r\n  encode(text) {\r\n    if (!text) return [];\r\n    \r\n    // First, split into words and handle them separately\r\n    return text.split(/(\\s+)/).flatMap(part => {\r\n      if (!part.trim()) {\r\n        // For whitespace, simply look up in the vocabulary\r\n        const whitespaceId = this.vocab.get(part) || this.vocab.get('<UNK>') || 0;\r\n        return [whitespaceId];\r\n      }\r\n      \r\n      // Start with characters\r\n      let tokens = Array.from(part).map(c => c);\r\n      \r\n      // Apply BPE merges\r\n      for (let i = 0; i < this.merges.length; i++) {\r\n        const [first, second] = this.merges[i];\r\n        \r\n        let j = 0;\r\n        while (j < tokens.length - 1) {\r\n          if (tokens[j] === first && tokens[j + 1] === second) {\r\n            tokens = [\r\n              ...tokens.slice(0, j),\r\n              first + second,\r\n              ...tokens.slice(j + 2)\r\n            ];\r\n          } else {\r\n            j++;\r\n          }\r\n        }\r\n      }\r\n      \r\n      // Convert subwords to token IDs\r\n      return tokens.map(token => {\r\n        if (this.vocab.has(token)) {\r\n          return this.vocab.get(token);\r\n        }\r\n        // Fall back to unknown token\r\n        return this.vocab.get('<UNK>') || 0;\r\n      });\r\n    });\r\n  }\r\n\r\n  /**\r\n   * Decode token IDs back to text\r\n   * @param {Array<number>} tokens - Token IDs to decode\r\n   * @returns {string} Decoded text\r\n   */\r\n  decode(tokens) {\r\n    return tokens.map(token => {\r\n      if (this.decoder.has(token)) {\r\n        return this.decoder.get(token);\r\n      }\r\n      return ''; // Skip unknown tokens\r\n    }).join('');\r\n  }\r\n}\r\n\r\n/**\r\n * Creates a tokenizer from a vocabulary or model config\r\n * @param {Object} config - Tokenizer configuration\r\n * @returns {BaseTokenizer} Appropriate tokenizer instance\r\n */\r\nfunction createTokenizer(config) {\r\n  if (!config) {\r\n    logger.warn('No tokenizer configuration provided, using fallback character tokenizer');\r\n    return new CharacterTokenizer();\r\n  }\r\n  \r\n  // Determine tokenizer type from config\r\n  if (config.tokenizer_type === 'bpe' || config.merges) {\r\n    logger.info('Creating BPE tokenizer');\r\n    return new BPETokenizer(config);\r\n  }\r\n  \r\n  // Add more tokenizer types here as needed\r\n  \r\n  // Fallback to character tokenizer\r\n  logger.warn('Unknown tokenizer type, using fallback character tokenizer');\r\n  return new CharacterTokenizer();\r\n}\r\n\r\n/**\r\n * Load a tokenizer from a file\r\n * @param {string} path - Path to tokenizer file\r\n * @param {Object} adapter - Environment adapter\r\n * @returns {Promise<BaseTokenizer>} Loaded tokenizer\r\n */\r\nasync function loadTokenizerFromFile(path, adapter) {\r\n  try {\r\n    if (!adapter || !adapter.readFile) {\r\n      throw new Error('Environment adapter does not support file reading');\r\n    }\r\n    \r\n    const data = await adapter.readFile(path);\r\n    const config = JSON.parse(data);\r\n    return createTokenizer(config);\r\n  } catch (error) {\r\n    logger.error(`Failed to load tokenizer from ${path}: ${error.message}`);\r\n    return new CharacterTokenizer();\r\n  }\r\n}\r\n\r\n/**\r\n * Extract a tokenizer from a GGUF model\r\n * This is model-specific and depends on the adapter implementation\r\n * @param {Object} model - GGUF model object\r\n * @param {Object} adapter - Environment adapter\r\n * @returns {Promise<BaseTokenizer>} Extracted tokenizer\r\n */\r\nasync function extractTokenizerFromModel(model, adapter) {\r\n  try {\r\n    if (!adapter || !adapter.extractTokenizer) {\r\n      throw new Error('Environment adapter does not support tokenizer extraction');\r\n    }\r\n    \r\n    const config = await adapter.extractTokenizer(model);\r\n    return createTokenizer(config);\r\n  } catch (error) {\r\n    logger.error(`Failed to extract tokenizer from model: ${error.message}`);\r\n    return new CharacterTokenizer();\r\n  }\r\n}\r\n\r\nmodule.exports = {\r\n  BaseTokenizer,\r\n  CharacterTokenizer,\r\n  BPETokenizer,\r\n  createTokenizer,\r\n  loadTokenizerFromFile,\r\n  extractTokenizerFromModel\r\n};","/**\r\n * Node.js adapter for GGUF models\r\n * @module adapters/node\r\n */\r\n\r\nconst fs = require('fs');\r\nconst path = require('path');\r\nconst logger = require('../utils/logger');\r\n\r\n// We'll use node-llama-cpp for GGUF model handling\r\n// This needs to be installed as a dependency\r\nlet LlamaModel;\r\ntry {\r\n  const llamaCpp = require('node-llama-cpp');\r\n  LlamaModel = llamaCpp.LlamaModel;\r\n} catch (error) {\r\n  logger.warn('node-llama-cpp not found. Install it with: npm install node-llama-cpp');\r\n  LlamaModel = null;\r\n}\r\n\r\n/**\r\n * Initialize a GGUF model in Node.js environment\r\n * @param {string} modelPath - Path to the model file\r\n * @param {Object} options - Model options\r\n * @returns {Promise<Object>} Session and tokenizer\r\n */\r\nasync function initializeModel(modelPath, options = {}) {\r\n  if (!LlamaModel) {\r\n    throw new Error('node-llama-cpp is required but not installed');\r\n  }\r\n  \r\n  // Verify the model file exists\r\n  if (!fs.existsSync(modelPath)) {\r\n    throw new Error(`Model file not found: ${modelPath}`);\r\n  }\r\n  \r\n  logger.info(`Initializing GGUF model from: ${modelPath}`);\r\n  \r\n  // Default options\r\n  const {\r\n    contextSize = 2048,\r\n    quantization = 'q4_0',\r\n    lowMemory = false,\r\n    seed = Math.floor(Math.random() * 4294967295),\r\n    threads = Math.max(1, require('os').cpus().length / 2)\r\n  } = options;\r\n  \r\n  // Normalize path to absolute\r\n  const absolutePath = path.resolve(modelPath);\r\n  \r\n  try {\r\n    // Initialize llama.cpp model\r\n    const model = new LlamaModel({\r\n      modelPath: absolutePath,\r\n      contextSize,\r\n      seed,\r\n      threads,\r\n      useMlock: !lowMemory,\r\n      batchSize: 512,\r\n      // Ensure proper mapping for quantization\r\n      gpuLayers: 0 // Node.js typically doesn't use GPU\r\n    });\r\n    \r\n    // Create a tokenizer interface\r\n    const tokenizer = {\r\n      encode: async (text) => model.tokenize(text),\r\n      decode: async (tokens) => model.detokenize(tokens),\r\n      decodeToken: async (token) => model.detokenize([token])\r\n    };\r\n    \r\n    logger.info(`Model initialized successfully (context size: ${contextSize}, threads: ${threads})`);\r\n    \r\n    return {\r\n      session: model,\r\n      tokenizer\r\n    };\r\n  } catch (error) {\r\n    logger.error(`Failed to initialize model: ${error.message}`);\r\n    throw error;\r\n  }\r\n}\r\n\r\n/**\r\n * Tokenize text with the model\r\n * @param {Object} model - The GGUF model\r\n * @param {string} text - Text to tokenize\r\n * @returns {Promise<number[]>} Token IDs\r\n */\r\nasync function tokenize(model, text) {\r\n  if (!model.session) {\r\n    throw new Error('Model session not initialized');\r\n  }\r\n  \r\n  return model.session.tokenize(text);\r\n}\r\n\r\n/**\r\n * Detokenize tokens to text\r\n * @param {Object} model - The GGUF model\r\n * @param {number[]} tokens - Tokens to detokenize\r\n * @returns {Promise<string>} Detokenized text\r\n */\r\nasync function detokenize(model, tokens) {\r\n  if (!model.session) {\r\n    throw new Error('Model session not initialized');\r\n  }\r\n  \r\n  return model.session.detokenize(tokens);\r\n}\r\n\r\n/**\r\n * Detokenize a single token\r\n * @param {Object} model - The GGUF model\r\n * @param {number} token - Token to detokenize\r\n * @returns {Promise<string>} Detokenized token text\r\n */\r\nasync function detokenizeToken(model, token) {\r\n  if (!model.session) {\r\n    throw new Error('Model session not initialized');\r\n  }\r\n  \r\n  return model.session.detokenize([token]);\r\n}\r\n\r\n/**\r\n * Run inference with a model\r\n * @param {Object} model - The GGUF model\r\n * @param {Object} context - Model context\r\n * @param {Object} options - Generation options\r\n * @param {function} isCancelled - Function to check if generation is cancelled\r\n * @returns {Promise<number[]>} Generated token IDs\r\n */\r\nasync function runInference(model, context, options, isCancelled) {\r\n  if (!model.session) {\r\n    throw new Error('Model session not initialized');\r\n  }\r\n  \r\n  // Configure generation parameters\r\n  const params = {\r\n    nPredict: options.maxTokens,\r\n    temperature: options.temperature,\r\n    topP: options.topP,\r\n    topK: options.topK,\r\n    repeatPenalty: options.repetitionPenalty,\r\n    seed: options.seed,\r\n    // Include stop sequences if provided\r\n    stop: options.stopSequences || []\r\n  };\r\n  \r\n  // Create a completion using the existing tokens\r\n  return new Promise((resolve, reject) => {\r\n    try {\r\n      // Use the node-llama-cpp API for completion\r\n      const completion = model.session.generate({\r\n        tokens: context.tokens,\r\n        ...params\r\n      });\r\n      \r\n      const outputTokens = [];\r\n      \r\n      // Process token by token\r\n      for (const token of completion) {\r\n        // Check for cancellation\r\n        if (isCancelled && isCancelled()) {\r\n          logger.info('Generation cancelled');\r\n          break;\r\n        }\r\n        \r\n        outputTokens.push(token);\r\n        \r\n        // Check if we've reached the maximum tokens\r\n        if (outputTokens.length >= params.nPredict) {\r\n          break;\r\n        }\r\n      }\r\n      \r\n      resolve(outputTokens);\r\n    } catch (error) {\r\n      reject(error);\r\n    }\r\n  });\r\n}\r\n\r\n/**\r\n * Run streaming inference with a model\r\n * @param {Object} model - The GGUF model\r\n * @param {Object} context - Model context\r\n * @param {function} onToken - Callback for each token\r\n * @param {Object} options - Generation options\r\n * @param {function} isCancelled - Function to check if generation is cancelled\r\n * @returns {Promise<void>}\r\n */\r\nasync function runInferenceStreaming(model, context, onToken, options, isCancelled) {\r\n  if (!model.session) {\r\n    throw new Error('Model session not initialized');\r\n  }\r\n  \r\n  // Configure generation parameters\r\n  const params = {\r\n    nPredict: options.maxTokens,\r\n    temperature: options.temperature,\r\n    topP: options.topP,\r\n    topK: options.topK,\r\n    repeatPenalty: options.repetitionPenalty,\r\n    seed: options.seed,\r\n    stop: options.stopSequences || []\r\n  };\r\n  \r\n  return new Promise((resolve, reject) => {\r\n    try {\r\n      // Use the node-llama-cpp API for streaming\r\n      const completion = model.session.generate({\r\n        tokens: context.tokens,\r\n        ...params\r\n      });\r\n      \r\n      // Process tokens in a streaming fashion\r\n      let tokenCount = 0;\r\n      \r\n      for (const token of completion) {\r\n        // Check for cancellation\r\n        if (isCancelled && isCancelled()) {\r\n          logger.info('Streaming generation cancelled');\r\n          break;\r\n        }\r\n        \r\n        // Call token callback\r\n        onToken(token);\r\n        tokenCount++;\r\n        \r\n        // Check if we've reached the maximum tokens\r\n        if (tokenCount >= params.nPredict) {\r\n          break;\r\n        }\r\n      }\r\n      \r\n      resolve();\r\n    } catch (error) {\r\n      reject(error);\r\n    }\r\n  });\r\n}\r\n\r\n/**\r\n * Free resources used by a model\r\n * @param {Object} model - The model to free\r\n */\r\nfunction freeModel(model) {\r\n  if (model.session && typeof model.session.dispose === 'function') {\r\n    model.session.dispose();\r\n    logger.info(`Model resources freed: ${model.id}`);\r\n  }\r\n}\r\n\r\n/**\r\n * Discover GGUF models in a directory\r\n * @param {string} directory - Directory to search\r\n * @returns {Promise<Object[]>} Array of model info objects\r\n */\r\nasync function discoverModels(directory) {\r\n  if (!fs.existsSync(directory)) {\r\n    logger.warn(`Model directory not found: ${directory}`);\r\n    return [];\r\n  }\r\n  \r\n  logger.info(`Searching for GGUF models in: ${directory}`);\r\n  \r\n  try {\r\n    const files = fs.readdirSync(directory);\r\n    const modelFiles = files.filter(file => file.toLowerCase().endsWith('.gguf'));\r\n    \r\n    // Create model info objects\r\n    const models = modelFiles.map(file => {\r\n      const filePath = path.join(directory, file);\r\n      const stats = fs.statSync(filePath);\r\n      \r\n      // Parse basic model info from filename\r\n      // Typical format: model-name.Q4_K_M.gguf\r\n      const baseName = path.basename(file, '.gguf');\r\n      const parts = baseName.split('.');\r\n      const name = parts[0];\r\n      const quantization = parts.length > 1 ? parts[1] : 'unknown';\r\n      \r\n      return {\r\n        id: baseName,\r\n        name: name,\r\n        path: filePath,\r\n        quantization,\r\n        size: stats.size,\r\n        createdAt: stats.birthtime.toISOString()\r\n      };\r\n    });\r\n    \r\n    logger.info(`Found ${models.length} GGUF models`);\r\n    return models;\r\n  } catch (error) {\r\n    logger.error(`Error discovering models: ${error.message}`);\r\n    return [];\r\n  }\r\n}\r\n\r\nmodule.exports = {\r\n  initializeModel,\r\n  tokenize,\r\n  detokenize,\r\n  detokenizeToken,\r\n  runInference,\r\n  runInferenceStreaming,\r\n  freeModel,\r\n  discoverModels\r\n};","/**\r\n * Browser-specific adapter for GGUF.js\r\n * @module adapters/browser\r\n */\r\n\r\nconst logger = require('../utils/logger');\r\n\r\n// Check if WebAssembly is available\r\nconst hasWebAssembly = typeof WebAssembly === 'object' && \r\n                        typeof WebAssembly.instantiate === 'function';\r\n\r\n// WASM module and instance\r\nlet wasmModule = null;\r\nlet wasmInstance = null;\r\nlet isInitialized = false;\r\n\r\n// Default WASM URL - in production, this would point to your hosted WASM build\r\nconst DEFAULT_WASM_URL = 'https://cdn.jsdelivr.net/npm/gguf.js/dist/wasm/gguf-web.wasm';\r\n\r\n/**\r\n * Initialize the WASM environment\r\n * \r\n * @param {Object} options - Initialization options\r\n * @param {string} [options.wasmUrl] - URL to the WASM binary\r\n * @returns {Promise<boolean>} Whether initialization was successful\r\n */\r\nasync function initializeWasm(options = {}) {\r\n  if (isInitialized) {\r\n    return true;\r\n  }\r\n  \r\n  if (!hasWebAssembly) {\r\n    logger.error('WebAssembly is not supported in this browser');\r\n    return false;\r\n  }\r\n  \r\n  const wasmUrl = options.wasmUrl || DEFAULT_WASM_URL;\r\n  \r\n  try {\r\n    logger.info(`Loading WASM from ${wasmUrl}`);\r\n    \r\n    // Fetch the WASM binary\r\n    const response = await fetch(wasmUrl);\r\n    if (!response.ok) {\r\n      throw new Error(`Failed to fetch WASM: ${response.statusText}`);\r\n    }\r\n    \r\n    const wasmBinary = await response.arrayBuffer();\r\n    \r\n    // Compile the WASM module\r\n    wasmModule = await WebAssembly.compile(wasmBinary);\r\n    \r\n    // Create memory and import object\r\n    const memory = new WebAssembly.Memory({ initial: 256, maximum: 2048 });\r\n    \r\n    // Define imports needed by the WASM module\r\n    const importObject = {\r\n      env: {\r\n        memory,\r\n        // Add required JavaScript functions that the WASM module can call\r\n        consoleLog: (ptr, len) => {\r\n          const bytes = new Uint8Array(memory.buffer, ptr, len);\r\n          const text = new TextDecoder().decode(bytes);\r\n          console.log(`[WASM] ${text}`);\r\n        },\r\n        consoleError: (ptr, len) => {\r\n          const bytes = new Uint8Array(memory.buffer, ptr, len);\r\n          const text = new TextDecoder().decode(bytes);\r\n          console.error(`[WASM] ${text}`);\r\n        }\r\n      }\r\n    };\r\n    \r\n    // Instantiate the WASM module\r\n    wasmInstance = await WebAssembly.instantiate(wasmModule, importObject);\r\n    \r\n    isInitialized = true;\r\n    logger.info('WASM initialized successfully');\r\n    return true;\r\n  } catch (error) {\r\n    logger.error(`Failed to initialize WASM: ${error.message}`);\r\n    return false;\r\n  }\r\n}\r\n\r\n/**\r\n * Initialize a GGUF model in the browser\r\n * \r\n * @param {string|ArrayBuffer} modelPathOrData - Path to model or model data\r\n * @param {Object} options - Model options\r\n * @returns {Promise<Object>} Session and tokenizer objects\r\n */\r\nasync function initializeModel(modelPathOrData, options = {}) {\r\n  // Ensure WASM is initialized\r\n  if (!isInitialized) {\r\n    const initialized = await initializeWasm(options);\r\n    if (!initialized) {\r\n      throw new Error('WebAssembly initialization failed');\r\n    }\r\n  }\r\n  \r\n  let modelData;\r\n  \r\n  // Check if modelPathOrData is a path or ArrayBuffer\r\n  if (typeof modelPathOrData === 'string') {\r\n    logger.info(`Fetching model from ${modelPathOrData}`);\r\n    \r\n    try {\r\n      const response = await fetch(modelPathOrData);\r\n      if (!response.ok) {\r\n        throw new Error(`Failed to fetch model: ${response.statusText}`);\r\n      }\r\n      \r\n      modelData = await response.arrayBuffer();\r\n      logger.info(`Model fetched: ${modelData.byteLength} bytes`);\r\n    } catch (error) {\r\n      throw new Error(`Failed to fetch model: ${error.message}`);\r\n    }\r\n  } else if (modelPathOrData instanceof ArrayBuffer) {\r\n    modelData = modelPathOrData;\r\n    logger.info(`Using provided model data: ${modelData.byteLength} bytes`);\r\n  } else {\r\n    throw new Error('Model must be a URL string or ArrayBuffer');\r\n  }\r\n  \r\n  try {\r\n    // Create a shared buffer for the model data\r\n    const modelBuffer = new Uint8Array(modelData);\r\n    \r\n    // In a real implementation, we'd now load the model into WASM memory\r\n    // and call initialization functions\r\n    \r\n    // Get exports from WASM instance\r\n    const exports = wasmInstance.exports;\r\n    \r\n    // Allocate memory for the model in WASM\r\n    const modelPtr = exports.allocateMemory(modelBuffer.length);\r\n    \r\n    // Create a view into the WASM memory\r\n    const memory = exports.memory;\r\n    const memoryView = new Uint8Array(memory.buffer);\r\n    \r\n    // Copy model data to WASM memory\r\n    memoryView.set(modelBuffer, modelPtr);\r\n    \r\n    // Call WASM function to load the model\r\n    const modelId = exports.loadModel(modelPtr, modelBuffer.length, options.contextSize || 2048);\r\n    \r\n    if (modelId < 0) {\r\n      throw new Error(`Failed to load model: error code ${modelId}`);\r\n    }\r\n    \r\n    logger.info(`Model loaded with ID: ${modelId}`);\r\n    \r\n    // Create tokenizer interface\r\n    const tokenizer = {\r\n      encode: async (text) => {\r\n        // Convert text to Uint8Array\r\n        const textBytes = new TextEncoder().encode(text);\r\n        \r\n        // Allocate memory for the text\r\n        const textPtr = exports.allocateMemory(textBytes.length);\r\n        \r\n        // Copy text to WASM memory\r\n        memoryView.set(textBytes, textPtr);\r\n        \r\n        // Call WASM tokenize function\r\n        const resultPtr = exports.tokenize(modelId, textPtr, textBytes.length);\r\n        \r\n        // Read result from memory\r\n        const resultView = new DataView(memory.buffer);\r\n        const tokenCount = resultView.getInt32(resultPtr, true);\r\n        const tokensPtr = resultView.getInt32(resultPtr + 4, true);\r\n        \r\n        // Read tokens\r\n        const tokens = [];\r\n        for (let i = 0; i < tokenCount; i++) {\r\n          tokens.push(resultView.getInt32(tokensPtr + i * 4, true));\r\n        }\r\n        \r\n        // Free memory\r\n        exports.freeMemory(textPtr);\r\n        exports.freeMemory(resultPtr);\r\n        \r\n        return tokens;\r\n      },\r\n      \r\n      decode: async (tokens) => {\r\n        // Allocate memory for tokens\r\n        const tokensPtr = exports.allocateMemory(tokens.length * 4);\r\n        \r\n        // Copy tokens to WASM memory\r\n        const tokensView = new Int32Array(memory.buffer, tokensPtr, tokens.length);\r\n        tokens.forEach((token, i) => tokensView[i] = token);\r\n        \r\n        // Call WASM detokenize function\r\n        const resultPtr = exports.detokenize(modelId, tokensPtr, tokens.length);\r\n        \r\n        // Read result\r\n        const resultView = new DataView(memory.buffer);\r\n        const textLength = resultView.getInt32(resultPtr, true);\r\n        const textPtr = resultView.getInt32(resultPtr + 4, true);\r\n        \r\n        // Extract text\r\n        const textBytes = new Uint8Array(memory.buffer, textPtr, textLength);\r\n        const text = new TextDecoder().decode(textBytes);\r\n        \r\n        // Free memory\r\n        exports.freeMemory(tokensPtr);\r\n        exports.freeMemory(resultPtr);\r\n        \r\n        return text;\r\n      },\r\n      \r\n      decodeToken: async (token) => {\r\n        // Allocate memory for token\r\n        const tokenPtr = exports.allocateMemory(4);\r\n        \r\n        // Write token to memory\r\n        const tokenView = new Int32Array(memory.buffer, tokenPtr, 1);\r\n        tokenView[0] = token;\r\n        \r\n        // Call WASM detokenize function for single token\r\n        const resultPtr = exports.detokenize(modelId, tokenPtr, 1);\r\n        \r\n        // Read result\r\n        const resultView = new DataView(memory.buffer);\r\n        const textLength = resultView.getInt32(resultPtr, true);\r\n        const textPtr = resultView.getInt32(resultPtr + 4, true);\r\n        \r\n        // Extract text\r\n        const textBytes = new Uint8Array(memory.buffer, textPtr, textLength);\r\n        const text = new TextDecoder().decode(textBytes);\r\n        \r\n        // Free memory\r\n        exports.freeMemory(tokenPtr);\r\n        exports.freeMemory(resultPtr);\r\n        \r\n        return text;\r\n      }\r\n    };\r\n    \r\n    // Return session and tokenizer\r\n    return {\r\n      session: { modelId },\r\n      tokenizer\r\n    };\r\n  } catch (error) {\r\n    logger.error(`Failed to initialize model: ${error.message}`);\r\n    throw error;\r\n  }\r\n}\r\n\r\n/**\r\n * Tokenize text with a model\r\n * \r\n * @param {Object} model - The model\r\n * @param {string} text - Text to tokenize\r\n * @returns {Promise<number[]>} Array of token IDs\r\n */\r\nasync function tokenize(model, text) {\r\n  if (!model.tokenizer) {\r\n    throw new Error('Model tokenizer not initialized');\r\n  }\r\n  \r\n  return model.tokenizer.encode(text);\r\n}\r\n\r\n/**\r\n * Detokenize tokens to text\r\n * \r\n * @param {Object} model - The model\r\n * @param {number[]} tokens - Tokens to detokenize\r\n * @returns {Promise<string>} Detokenized text\r\n */\r\nasync function detokenize(model, tokens) {\r\n  if (!model.tokenizer) {\r\n    throw new Error('Model tokenizer not initialized');\r\n  }\r\n  \r\n  return model.tokenizer.decode(tokens);\r\n}\r\n\r\n/**\r\n * Detokenize a single token\r\n * \r\n * @param {Object} model - The model\r\n * @param {number} token - Token to detokenize\r\n * @returns {Promise<string>} Detokenized token\r\n */\r\nasync function detokenizeToken(model, token) {\r\n  if (!model.tokenizer) {\r\n    throw new Error('Model tokenizer not initialized');\r\n  }\r\n  \r\n  return model.tokenizer.decodeToken(token);\r\n}\r\n\r\n/**\r\n * Run inference with a model\r\n * \r\n * @param {Object} model - The model\r\n * @param {Object} context - The context containing tokens\r\n * @param {Object} options - Generation options\r\n * @param {Function} isCancelled - Function to check if generation is cancelled\r\n * @returns {Promise<number[]>} Generated tokens\r\n */\r\nasync function runInference(model, context, options, isCancelled) {\r\n  if (!isInitialized || !wasmInstance) {\r\n    throw new Error('WASM not initialized');\r\n  }\r\n  \r\n  const exports = wasmInstance.exports;\r\n  const { modelId } = model.session;\r\n  \r\n  try {\r\n    // Get input tokens\r\n    const inputTokens = context.tokens;\r\n    \r\n    // Allocate memory for input tokens\r\n    const inputPtr = exports.allocateMemory(inputTokens.length * 4);\r\n    \r\n    // Copy input tokens to WASM memory\r\n    const inputView = new Int32Array(exports.memory.buffer, inputPtr, inputTokens.length);\r\n    inputTokens.forEach((token, i) => inputView[i] = token);\r\n    \r\n    // Prepare generation parameters\r\n    const paramsPtr = exports.allocateMemory(40);  // Enough for all parameters\r\n    const paramsView = new DataView(exports.memory.buffer, paramsPtr);\r\n    \r\n    let offset = 0;\r\n    paramsView.setInt32(offset, options.maxTokens || 100, true); offset += 4;\r\n    paramsView.setFloat32(offset, options.temperature || 0.7, true); offset += 4;\r\n    paramsView.setFloat32(offset, options.topP || 0.9, true); offset += 4;\r\n    paramsView.setInt32(offset, options.topK || 40, true); offset += 4;\r\n    paramsView.setFloat32(offset, options.repetitionPenalty || 1.1, true); offset += 4;\r\n    paramsView.setInt32(offset, options.seed || Math.floor(Math.random() * 4294967295), true);\r\n    \r\n    // Call WASM inference function\r\n    const resultPtr = exports.generateText(\r\n      modelId,\r\n      inputPtr,\r\n      inputTokens.length,\r\n      paramsPtr\r\n    );\r\n    \r\n    // Check for cancellation between batches\r\n    const checkCancellation = () => {\r\n      if (isCancelled && isCancelled()) {\r\n        // Call WASM function to cancel generation\r\n        exports.cancelGeneration(modelId);\r\n        return true;\r\n      }\r\n      return false;\r\n    };\r\n    \r\n    // Poll for results\r\n    let completed = false;\r\n    const outputTokens = [];\r\n    \r\n    while (!completed && !checkCancellation()) {\r\n      // Check generation status\r\n      const status = exports.getGenerationStatus(modelId);\r\n      \r\n      if (status === 0) {\r\n        // Still generating, wait a bit\r\n        await new Promise(resolve => setTimeout(resolve, 10));\r\n        continue;\r\n      } else if (status === 1) {\r\n        // Generation complete\r\n        completed = true;\r\n        \r\n        // Read result tokens\r\n        const resultView = new DataView(exports.memory.buffer);\r\n        const tokenCount = resultView.getInt32(resultPtr, true);\r\n        const tokensPtr = resultView.getInt32(resultPtr + 4, true);\r\n        \r\n        // Extract tokens\r\n        const tokenView = new Int32Array(exports.memory.buffer, tokensPtr, tokenCount);\r\n        for (let i = 0; i < tokenCount; i++) {\r\n          outputTokens.push(tokenView[i]);\r\n        }\r\n        \r\n        // Free memory\r\n        exports.freeMemory(inputPtr);\r\n        exports.freeMemory(paramsPtr);\r\n        exports.freeMemory(resultPtr);\r\n      } else {\r\n        // Error\r\n        exports.freeMemory(inputPtr);\r\n        exports.freeMemory(paramsPtr);\r\n        throw new Error(`Generation failed with status code ${status}`);\r\n      }\r\n    }\r\n    \r\n    return outputTokens;\r\n  } catch (error) {\r\n    logger.error(`Inference error: ${error.message}`);\r\n    throw error;\r\n  }\r\n}\r\n\r\n/**\r\n * Run streaming inference with a model\r\n * \r\n * @param {Object} model - The model\r\n * @param {Object} context - The context containing tokens\r\n * @param {Function} onToken - Callback for each generated token\r\n * @param {Object} options - Generation options\r\n * @param {Function} isCancelled - Function to check if generation is cancelled\r\n * @returns {Promise<void>}\r\n */\r\nasync function runInferenceStreaming(model, context, onToken, options, isCancelled) {\r\n  if (!isInitialized || !wasmInstance) {\r\n    throw new Error('WASM not initialized');\r\n  }\r\n  \r\n  const exports = wasmInstance.exports;\r\n  const { modelId } = model.session;\r\n  \r\n  try {\r\n    // Similar setup as runInference\r\n    const inputTokens = context.tokens;\r\n    const inputPtr = exports.allocateMemory(inputTokens.length * 4);\r\n    const inputView = new Int32Array(exports.memory.buffer, inputPtr, inputTokens.length);\r\n    inputTokens.forEach((token, i) => inputView[i] = token);\r\n    \r\n    // Prepare parameters\r\n    const paramsPtr = exports.allocateMemory(40);\r\n    const paramsView = new DataView(exports.memory.buffer, paramsPtr);\r\n    \r\n    let offset = 0;\r\n    paramsView.setInt32(offset, options.maxTokens || 100, true); offset += 4;\r\n    paramsView.setFloat32(offset, options.temperature || 0.7, true); offset += 4;\r\n    paramsView.setFloat32(offset, options.topP || 0.9, true); offset += 4;\r\n    paramsView.setInt32(offset, options.topK || 40, true); offset += 4;\r\n    paramsView.setFloat32(offset, options.repetitionPenalty || 1.1, true); offset += 4;\r\n    paramsView.setInt32(offset, options.seed || Math.floor(Math.random() * 4294967295), true); offset += 4;\r\n    paramsView.setInt32(offset, 1, true); // streaming flag\r\n    \r\n    // Start streaming generation\r\n    exports.startStreamingGeneration(modelId, inputPtr, inputTokens.length, paramsPtr);\r\n    \r\n    // Poll for tokens\r\n    while (true) {\r\n      // Check for cancellation\r\n      if (isCancelled && isCancelled()) {\r\n        exports.cancelGeneration(modelId);\r\n        break;\r\n      }\r\n      \r\n      // Check for new token\r\n      const tokenPtr = exports.getNextToken(modelId);\r\n      \r\n      if (tokenPtr === 0) {\r\n        // No token available yet, wait a bit\r\n        await new Promise(resolve => setTimeout(resolve, 10));\r\n        continue;\r\n      } else if (tokenPtr === -1) {\r\n        // End of generation\r\n        break;\r\n      } else {\r\n        // Got a token, read it\r\n        const token = new DataView(exports.memory.buffer).getInt32(tokenPtr, true);\r\n        \r\n        // Detokenize and call callback\r\n        const tokenText = await detokenizeToken(model, token);\r\n        await onToken(token);\r\n        \r\n        // Free token memory\r\n        exports.freeMemory(tokenPtr);\r\n      }\r\n    }\r\n    \r\n    // Clean up\r\n    exports.freeMemory(inputPtr);\r\n    exports.freeMemory(paramsPtr);\r\n  } catch (error) {\r\n    logger.error(`Streaming inference error: ${error.message}`);\r\n    throw error;\r\n  }\r\n}\r\n\r\n/**\r\n * Free resources used by a model\r\n * \r\n * @param {Object} model - The model to free\r\n */\r\nfunction freeModel(model) {\r\n  if (!isInitialized || !wasmInstance || !model.session) {\r\n    return;\r\n  }\r\n  \r\n  try {\r\n    const { modelId } = model.session;\r\n    wasmInstance.exports.freeModel(modelId);\r\n    logger.info(`Model resources freed: ${modelId}`);\r\n  } catch (error) {\r\n    logger.error(`Error freeing model: ${error.message}`);\r\n  }\r\n}\r\n\r\nmodule.exports = {\r\n  initializeWasm,\r\n  initializeModel,\r\n  tokenize,\r\n  detokenize,\r\n  detokenizeToken,\r\n  runInference,\r\n  runInferenceStreaming,\r\n  freeModel\r\n};","/**\r\n * React hooks and utilities for GGUF models\r\n * @module adapters/react\r\n */\r\n\r\n// Check if React is available in the environment\r\nconst hasReact = typeof window !== 'undefined' && \r\n                 (typeof window.React !== 'undefined' || \r\n                  typeof require === 'function' && (() => {\r\n                    try { require('react'); return true; } \r\n                    catch { return false; }\r\n                  })());\r\n\r\n// Import React if available\r\nlet React;\r\nif (hasReact) {\r\n  try {\r\n    React = typeof window !== 'undefined' && window.React ? \r\n            window.React : require('react');\r\n  } catch (error) {\r\n    console.warn('React not found, hooks will not function properly');\r\n  }\r\n}\r\n\r\nconst { GGUFModel } = require('../core/model');\r\nconst loader = require('../core/loader');\r\nconst inference = require('../core/inference');\r\nconst logger = require('../utils/logger');\r\n\r\n/**\r\n * Use state in a React hook or plain object fallback\r\n * \r\n * @param {any} initialValue - Initial state value\r\n * @returns {Array} [value, setValue]\r\n */\r\nfunction useState(initialValue) {\r\n  if (React && React.useState) {\r\n    return React.useState(initialValue);\r\n  } else {\r\n    // Fallback implementation for non-React environments\r\n    let value = initialValue;\r\n    const setValue = (newValue) => {\r\n      if (typeof newValue === 'function') {\r\n        value = newValue(value);\r\n      } else {\r\n        value = newValue;\r\n      }\r\n    };\r\n    return [value, setValue];\r\n  }\r\n}\r\n\r\n/**\r\n * Use effect in a React hook or plain function fallback\r\n * \r\n * @param {Function} effect - Effect function\r\n * @param {Array} deps - Dependencies array\r\n */\r\nfunction useEffect(effect, deps) {\r\n  if (React && React.useEffect) {\r\n    return React.useEffect(effect, deps);\r\n  } else {\r\n    // Fallback implementation for non-React environments\r\n    // Call effect immediately for non-React environment\r\n    const cleanup = effect();\r\n    \r\n    // Return cleanup function if provided\r\n    return cleanup;\r\n  }\r\n}\r\n\r\n/**\r\n * Use callback in a React hook or plain function fallback\r\n * \r\n * @param {Function} callback - Callback function\r\n * @param {Array} deps - Dependencies array\r\n * @returns {Function} Memoized callback\r\n */\r\nfunction useCallback(callback, deps) {\r\n  if (React && React.useCallback) {\r\n    return React.useCallback(callback, deps);\r\n  } else {\r\n    // Fallback implementation for non-React environments\r\n    return callback;\r\n  }\r\n}\r\n\r\n/**\r\n * Use ref in a React hook or plain object fallback\r\n * \r\n * @param {any} initialValue - Initial ref value\r\n * @returns {Object} Ref object\r\n */\r\nfunction useRef(initialValue) {\r\n  if (React && React.useRef) {\r\n    return React.useRef(initialValue);\r\n  } else {\r\n    // Fallback implementation for non-React environments\r\n    return { current: initialValue };\r\n  }\r\n}\r\n\r\n/**\r\n * Hook to load and manage a GGUF model\r\n * \r\n * @param {Object} adapter - Environment adapter\r\n * @param {Object} [options={}] - Hook options\r\n * @returns {Object} Model state and functions\r\n */\r\nfunction useModel(adapter, options = {}) {\r\n  const [model, setModel] = useState(null);\r\n  const [isLoading, setIsLoading] = useState(false);\r\n  const [error, setError] = useState(null);\r\n  \r\n  // Load a model\r\n  const loadModel = useCallback(async (path, loadOptions = {}) => {\r\n    setIsLoading(true);\r\n    setError(null);\r\n    \r\n    try {\r\n      const loadedModel = await loader.loadModel(path, loadOptions, adapter);\r\n      setModel(loadedModel);\r\n      return loadedModel;\r\n    } catch (err) {\r\n      const message = err instanceof Error ? err.message : String(err);\r\n      setError(message);\r\n      throw err;\r\n    } finally {\r\n      setIsLoading(false);\r\n    }\r\n  }, [adapter]);\r\n  \r\n  // Unload the current model\r\n  const unloadModel = useCallback(async () => {\r\n    if (model) {\r\n      try {\r\n        await loader.unloadModel(model.id, adapter);\r\n        setModel(null);\r\n        return true;\r\n      } catch (err) {\r\n        const message = err instanceof Error ? err.message : String(err);\r\n        setError(message);\r\n        throw err;\r\n      }\r\n    }\r\n    return false;\r\n  }, [model, adapter]);\r\n  \r\n  // Discover models in a directory (Node.js only)\r\n  const discoverModels = useCallback(async (directory) => {\r\n    if (!adapter.discoverModels) {\r\n      setError('Model discovery not supported in this environment');\r\n      return [];\r\n    }\r\n    \r\n    try {\r\n      return await adapter.discoverModels(directory);\r\n    } catch (err) {\r\n      const message = err instanceof Error ? err.message : String(err);\r\n      setError(message);\r\n      throw err;\r\n    }\r\n  }, [adapter]);\r\n  \r\n  // Cleanup on unmount\r\n  useEffect(() => {\r\n    return () => {\r\n      if (model && model.id) {\r\n        // Try to unload the model on unmount\r\n        try {\r\n          loader.unloadModel(model.id, adapter);\r\n        } catch (err) {\r\n          logger.warn(`Failed to unload model on unmount: ${err.message}`);\r\n        }\r\n      }\r\n    };\r\n  }, [model, adapter]);\r\n  \r\n  return {\r\n    model,\r\n    isLoading,\r\n    error,\r\n    loadModel,\r\n    unloadModel,\r\n    discoverModels\r\n  };\r\n}\r\n\r\n/**\r\n * Hook for text generation\r\n * \r\n * @param {Object} adapter - Environment adapter\r\n * @param {Object} [options={}] - Hook options\r\n * @returns {Object} Generation state and functions\r\n */\r\nfunction useGeneration(adapter, options = {}) {\r\n  const [isGenerating, setIsGenerating] = useState(false);\r\n  const [error, setError] = useState(null);\r\n  const [output, setOutput] = useState('');\r\n  const isCancelled = useRef(false);\r\n  \r\n  // Generate text\r\n  const generate = useCallback(async (model, prompt, genOptions = {}) => {\r\n    if (!model || !model.isLoaded) {\r\n      const error = new Error('Model not loaded properly');\r\n      setError(error.message);\r\n      throw error;\r\n    }\r\n    \r\n    setIsGenerating(true);\r\n    setError(null);\r\n    setOutput('');\r\n    isCancelled.current = false;\r\n    \r\n    try {\r\n      const response = await inference.generate(\r\n        model, \r\n        prompt, \r\n        genOptions, \r\n        adapter, \r\n        () => isCancelled.current\r\n      );\r\n      \r\n      setOutput(response);\r\n      return response;\r\n    } catch (err) {\r\n      if (!isCancelled.current) {\r\n        const message = err instanceof Error ? err.message : String(err);\r\n        setError(message);\r\n        throw err;\r\n      }\r\n      return '';\r\n    } finally {\r\n      setIsGenerating(false);\r\n    }\r\n  }, [adapter]);\r\n  \r\n  // Stream generation\r\n  const streamGenerate = useCallback(async (model, prompt, onToken, genOptions = {}) => {\r\n    if (!model || !model.isLoaded) {\r\n      const error = new Error('Model not loaded properly');\r\n      setError(error.message);\r\n      throw error;\r\n    }\r\n    \r\n    setIsGenerating(true);\r\n    setError(null);\r\n    setOutput('');\r\n    isCancelled.current = false;\r\n    \r\n    let fullOutput = '';\r\n    \r\n    // Create a wrapped onToken function that builds the full output\r\n    const wrappedOnToken = (token, isDone) => {\r\n      if (!isCancelled.current) {\r\n        fullOutput += token;\r\n        setOutput(fullOutput);\r\n        onToken(token, isDone);\r\n      }\r\n    };\r\n    \r\n    try {\r\n      await inference.streamGenerate(\r\n        model, \r\n        prompt, \r\n        wrappedOnToken, \r\n        genOptions, \r\n        adapter, \r\n        () => isCancelled.current\r\n      );\r\n      \r\n      return fullOutput;\r\n    } catch (err) {\r\n      if (!isCancelled.current) {\r\n        const message = err instanceof Error ? err.message : String(err);\r\n        setError(message);\r\n        throw err;\r\n      }\r\n      return fullOutput;\r\n    } finally {\r\n      setIsGenerating(false);\r\n    }\r\n  }, [adapter]);\r\n  \r\n  // Cancel generation\r\n  const cancelGeneration = useCallback(() => {\r\n    isCancelled.current = true;\r\n    setIsGenerating(false);\r\n    return true;\r\n  }, []);\r\n  \r\n  return {\r\n    isGenerating,\r\n    error,\r\n    output,\r\n    generate,\r\n    streamGenerate,\r\n    cancelGeneration\r\n  };\r\n}\r\n\r\n/**\r\n * Hook for chat conversation state management\r\n * \r\n * @param {Object} adapter - Environment adapter\r\n * @param {Object} [options={}] - Hook options\r\n * @returns {Object} Chat state and functions\r\n */\r\nfunction useChat(adapter, options = {}) {\r\n  const [messages, setMessages] = useState([]);\r\n  const [isGenerating, setIsGenerating] = useState(false);\r\n  const [error, setError] = useState(null);\r\n  \r\n  // Add a user message\r\n  const addUserMessage = useCallback((content) => {\r\n    const userMessage = {\r\n      id: Date.now(),\r\n      role: 'user',\r\n      content,\r\n      timestamp: new Date().toISOString()\r\n    };\r\n    \r\n    setMessages(prevMessages => [...prevMessages, userMessage]);\r\n    return userMessage;\r\n  }, []);\r\n  \r\n  // Add an assistant message\r\n  const addAssistantMessage = useCallback((content) => {\r\n    const assistantMessage = {\r\n      id: Date.now(),\r\n      role: 'assistant',\r\n      content,\r\n      timestamp: new Date().toISOString()\r\n    };\r\n    \r\n    setMessages(prevMessages => [...prevMessages, assistantMessage]);\r\n    return assistantMessage;\r\n  }, []);\r\n  \r\n  // Format messages into a prompt\r\n  const formatPrompt = useCallback((systemPrompt) => {\r\n    let prompt = '';\r\n    \r\n    // Add system prompt if provided\r\n    if (systemPrompt) {\r\n      prompt += `System: ${systemPrompt}\\n\\n`;\r\n    }\r\n    \r\n    // Add conversation history\r\n    for (const message of messages) {\r\n      if (message.role === 'user') {\r\n        prompt += `User: ${message.content}\\n\\n`;\r\n      } else if (message.role === 'assistant') {\r\n        prompt += `Assistant: ${message.content}\\n\\n`;\r\n      } else if (message.role === 'system') {\r\n        prompt += `System: ${message.content}\\n\\n`;\r\n      }\r\n    }\r\n    \r\n    // Add assistant prompt\r\n    prompt += 'Assistant: ';\r\n    \r\n    return prompt;\r\n  }, [messages]);\r\n  \r\n  // Generate a response\r\n  const generateResponse = useCallback(async (\r\n    model, \r\n    systemPrompt = '',\r\n    genOptions = {}\r\n  ) => {\r\n    if (!model || !model.isLoaded) {\r\n      const error = new Error('Model not loaded properly');\r\n      setError(error.message);\r\n      throw error;\r\n    }\r\n    \r\n    setIsGenerating(true);\r\n    setError(null);\r\n    \r\n    try {\r\n      // Format the conversation into a prompt\r\n      const prompt = formatPrompt(systemPrompt);\r\n      \r\n      // Generate a response\r\n      const response = await inference.generate(\r\n        model,\r\n        prompt,\r\n        genOptions,\r\n        adapter\r\n      );\r\n      \r\n      // Add the response to the conversation\r\n      addAssistantMessage(response);\r\n      \r\n      return response;\r\n    } catch (err) {\r\n      const message = err instanceof Error ? err.message : String(err);\r\n      setError(message);\r\n      throw err;\r\n    } finally {\r\n      setIsGenerating(false);\r\n    }\r\n  }, [formatPrompt, addAssistantMessage, adapter]);\r\n  \r\n  // Stream a response\r\n  const streamResponse = useCallback(async (\r\n    model,\r\n    onToken,\r\n    systemPrompt = '',\r\n    genOptions = {}\r\n  ) => {\r\n    if (!model || !model.isLoaded) {\r\n      const error = new Error('Model not loaded properly');\r\n      setError(error.message);\r\n      throw error;\r\n    }\r\n    \r\n    setIsGenerating(true);\r\n    setError(null);\r\n    \r\n    // Create a placeholder for the assistant message\r\n    const assistantMessage = {\r\n      id: Date.now(),\r\n      role: 'assistant',\r\n      content: '',\r\n      timestamp: new Date().toISOString()\r\n    };\r\n    \r\n    setMessages(prevMessages => [...prevMessages, assistantMessage]);\r\n    \r\n    let fullResponse = '';\r\n    \r\n    // Create a wrapped token handler that updates the message\r\n    const wrappedOnToken = (token, isDone) => {\r\n      if (!isDone) {\r\n        fullResponse += token;\r\n        \r\n        // Update the assistant message\r\n        setMessages(prevMessages => {\r\n          const updatedMessages = [...prevMessages];\r\n          const lastIndex = updatedMessages.length - 1;\r\n          \r\n          if (lastIndex >= 0 && updatedMessages[lastIndex].id === assistantMessage.id) {\r\n            updatedMessages[lastIndex] = {\r\n              ...updatedMessages[lastIndex],\r\n              content: fullResponse\r\n            };\r\n          }\r\n          \r\n          return updatedMessages;\r\n        });\r\n        \r\n        // Call the original callback\r\n        onToken(token, isDone);\r\n      } else {\r\n        onToken('', true);\r\n      }\r\n    };\r\n    \r\n    try {\r\n      // Format the conversation into a prompt\r\n      const prompt = formatPrompt(systemPrompt);\r\n      \r\n      // Generate a streaming response\r\n      await inference.streamGenerate(\r\n        model,\r\n        prompt,\r\n        wrappedOnToken,\r\n        genOptions,\r\n        adapter\r\n      );\r\n      \r\n      return fullResponse;\r\n    } catch (err) {\r\n      const message = err instanceof Error ? err.message : String(err);\r\n      setError(message);\r\n      throw err;\r\n    } finally {\r\n      setIsGenerating(false);\r\n    }\r\n  }, [formatPrompt, adapter]);\r\n  \r\n  // Clear all messages\r\n  const clearMessages = useCallback(() => {\r\n    setMessages([]);\r\n  }, []);\r\n  \r\n  return {\r\n    messages,\r\n    isGenerating,\r\n    error,\r\n    addUserMessage,\r\n    addAssistantMessage,\r\n    generateResponse,\r\n    streamResponse,\r\n    clearMessages,\r\n    formatPrompt\r\n  };\r\n}\r\n\r\n/**\r\n * Create a React context provider for GGUF models\r\n * Only available when React is present\r\n * \r\n * @param {Object} adapter - Environment adapter\r\n * @returns {Object} Context provider and hooks\r\n */\r\nfunction createGGUFContext(adapter) {\r\n  if (!React) {\r\n    throw new Error('React is required to create a context provider');\r\n  }\r\n  \r\n  // Create contexts\r\n  const ModelContext = React.createContext(null);\r\n  const GenerationContext = React.createContext(null);\r\n  \r\n  // Provider component\r\n  const GGUFProvider = ({ children, initialOptions = {} }) => {\r\n    const modelState = useModel(adapter, initialOptions);\r\n    const generationState = useGeneration(adapter, initialOptions);\r\n    \r\n    // Use React.createElement instead of JSX\r\n    return React.createElement(\r\n      ModelContext.Provider,\r\n      { value: modelState },\r\n      React.createElement(\r\n        GenerationContext.Provider,\r\n        { value: generationState },\r\n        children\r\n      )\r\n    );\r\n  };\r\n  \r\n  // Custom hooks to use the contexts\r\n  const useGGUFModel = () => {\r\n    const context = React.useContext(ModelContext);\r\n    if (!context) {\r\n      throw new Error('useGGUFModel must be used within a GGUFProvider');\r\n    }\r\n    return context;\r\n  };\r\n  \r\n  const useGGUFGeneration = () => {\r\n    const context = React.useContext(GenerationContext);\r\n    if (!context) {\r\n      throw new Error('useGGUFGeneration must be used within a GGUFProvider');\r\n    }\r\n    return context;\r\n  };\r\n  \r\n  return {\r\n    GGUFProvider,\r\n    useGGUFModel,\r\n    useGGUFGeneration\r\n  };\r\n}\r\n\r\nmodule.exports = {\r\n  // Core hooks\r\n  useModel,\r\n  useGeneration,\r\n  useChat,\r\n  \r\n  // React context utilities (only available when React is present)\r\n  createGGUFContext: hasReact ? createGGUFContext : undefined,\r\n  \r\n  // Expose React wrappers for testing\r\n  _react: {\r\n    useState,\r\n    useEffect,\r\n    useCallback,\r\n    useRef\r\n  }\r\n};","/**\r\n * Memory management utilities for GGUF.js\r\n * @module utils/memory\r\n */\r\n\r\nconst logger = require('./logger');\r\n\r\n/**\r\n * Memory allocation record\r\n * @typedef {Object} AllocationRecord\r\n * @property {string} id - Unique identifier\r\n * @property {number} size - Size in bytes\r\n * @property {string} [type] - Allocation type\r\n * @property {number} timestamp - Allocation timestamp\r\n * @property {string} [description] - Description\r\n */\r\n\r\n/**\r\n * Memory usage stats\r\n * @typedef {Object} MemoryStats\r\n * @property {number} allocated - Total bytes allocated\r\n * @property {number} freed - Total bytes freed\r\n * @property {number} active - Active bytes (allocated - freed)\r\n * @property {number} peak - Peak memory usage\r\n * @property {number} allocations - Number of allocations\r\n * @property {number} frees - Number of frees\r\n */\r\n\r\n/**\r\n * Memory manager class for tracking and optimizing memory usage\r\n */\r\nclass MemoryManager {\r\n  constructor() {\r\n    /**\r\n     * Map of active allocations\r\n     * @type {Map<string, AllocationRecord>}\r\n     */\r\n    this.allocations = new Map();\r\n    \r\n    /**\r\n     * Allocation counter for generating IDs\r\n     * @type {number}\r\n     */\r\n    this.counter = 0;\r\n    \r\n    /**\r\n     * Memory usage statistics\r\n     * @type {MemoryStats}\r\n     */\r\n    this.stats = {\r\n      allocated: 0,\r\n      freed: 0,\r\n      active: 0,\r\n      peak: 0,\r\n      allocations: 0,\r\n      frees: 0\r\n    };\r\n    \r\n    // Set up automatic browser memory stats if available\r\n    if (typeof window !== 'undefined' && \r\n        window.performance && \r\n        'memory' in window.performance) {\r\n      this.hasBrowserMemoryStats = true;\r\n    } else {\r\n      this.hasBrowserMemoryStats = false;\r\n    }\r\n    \r\n    // Set up automatic Node.js memory stats if available\r\n    if (typeof process !== 'undefined' && \r\n        process.memoryUsage) {\r\n      this.hasNodeMemoryStats = true;\r\n    } else {\r\n      this.hasNodeMemoryStats = false;\r\n    }\r\n  }\r\n  \r\n  /**\r\n   * Track a memory allocation\r\n   * @param {number} size - Size in bytes\r\n   * @param {Object} [options] - Allocation options\r\n   * @param {string} [options.type] - Allocation type\r\n   * @param {string} [options.description] - Allocation description\r\n   * @returns {string} Allocation ID\r\n   */\r\n  allocate(size, { type, description } = {}) {\r\n    const id = `alloc_${++this.counter}`;\r\n    \r\n    // Create allocation record\r\n    const record = {\r\n      id,\r\n      size,\r\n      type: type || 'unknown',\r\n      timestamp: Date.now(),\r\n      description: description || ''\r\n    };\r\n    \r\n    // Store the allocation\r\n    this.allocations.set(id, record);\r\n    \r\n    // Update stats\r\n    this.stats.allocated += size;\r\n    this.stats.active += size;\r\n    this.stats.allocations++;\r\n    \r\n    // Update peak usage\r\n    if (this.stats.active > this.stats.peak) {\r\n      this.stats.peak = this.stats.active;\r\n    }\r\n    \r\n    logger.debug(`Memory allocated: ${formatBytes(size)} (${type || 'unknown'})${description ? ` - ${description}` : ''}`);\r\n    \r\n    return id;\r\n  }\r\n  \r\n  /**\r\n   * Track a memory deallocation\r\n   * @param {string} id - Allocation ID to free\r\n   * @returns {boolean} Whether the allocation was found and freed\r\n   */\r\n  free(id) {\r\n    // Find the allocation\r\n    const allocation = this.allocations.get(id);\r\n    \r\n    if (!allocation) {\r\n      logger.warn(`Attempted to free unknown allocation: ${id}`);\r\n      return false;\r\n    }\r\n    \r\n    // Update stats\r\n    this.stats.freed += allocation.size;\r\n    this.stats.active -= allocation.size;\r\n    this.stats.frees++;\r\n    \r\n    // Remove the allocation\r\n    this.allocations.delete(id);\r\n    \r\n    logger.debug(`Memory freed: ${formatBytes(allocation.size)} (${allocation.type})${allocation.description ? ` - ${allocation.description}` : ''}`);\r\n    \r\n    return true;\r\n  }\r\n  \r\n  /**\r\n   * Free all tracked allocations\r\n   * @returns {number} Number of allocations freed\r\n   */\r\n  freeAll() {\r\n    const count = this.allocations.size;\r\n    \r\n    if (count === 0) {\r\n      return 0;\r\n    }\r\n    \r\n    logger.info(`Freeing all ${count} memory allocations (${formatBytes(this.stats.active)})`);\r\n    \r\n    const allocationIds = Array.from(this.allocations.keys());\r\n    for (const id of allocationIds) {\r\n      this.free(id);\r\n    }\r\n    \r\n    return count;\r\n  }\r\n  \r\n  /**\r\n   * Free allocations of a specific type\r\n   * @param {string} type - Allocation type to free\r\n   * @returns {number} Number of allocations freed\r\n   */\r\n  freeByType(type) {\r\n    let count = 0;\r\n    \r\n    const typeAllocations = Array.from(this.allocations.values())\r\n      .filter(allocation => allocation.type === type);\r\n    \r\n    logger.info(`Freeing ${typeAllocations.length} '${type}' allocations`);\r\n    \r\n    for (const allocation of typeAllocations) {\r\n      if (this.free(allocation.id)) {\r\n        count++;\r\n      }\r\n    }\r\n    \r\n    return count;\r\n  }\r\n  \r\n  /**\r\n   * Get current memory usage statistics\r\n   * @returns {Object} Memory stats\r\n   */\r\n  getStats() {\r\n    // Start with tracked stats\r\n    const stats = {\r\n      ...this.stats,\r\n      // Add user-friendly formatted values\r\n      allocatedFormatted: formatBytes(this.stats.allocated),\r\n      freedFormatted: formatBytes(this.stats.freed),\r\n      activeFormatted: formatBytes(this.stats.active),\r\n      peakFormatted: formatBytes(this.stats.peak),\r\n      activeAllocations: this.allocations.size\r\n    };\r\n    \r\n    // Add browser memory stats if available\r\n    if (this.hasBrowserMemoryStats) {\r\n      const memory = window.performance.memory;\r\n      stats.browser = {\r\n        totalJSHeapSize: memory.totalJSHeapSize,\r\n        usedJSHeapSize: memory.usedJSHeapSize,\r\n        jsHeapSizeLimit: memory.jsHeapSizeLimit,\r\n        totalJSHeapSizeFormatted: formatBytes(memory.totalJSHeapSize),\r\n        usedJSHeapSizeFormatted: formatBytes(memory.usedJSHeapSize),\r\n        jsHeapSizeLimitFormatted: formatBytes(memory.jsHeapSizeLimit)\r\n      };\r\n    }\r\n    \r\n    // Add Node.js memory stats if available\r\n    if (this.hasNodeMemoryStats) {\r\n      const memory = process.memoryUsage();\r\n      stats.node = {\r\n        rss: memory.rss,\r\n        heapTotal: memory.heapTotal,\r\n        heapUsed: memory.heapUsed,\r\n        external: memory.external,\r\n        arrayBuffers: memory.arrayBuffers,\r\n        rssFormatted: formatBytes(memory.rss),\r\n        heapTotalFormatted: formatBytes(memory.heapTotal),\r\n        heapUsedFormatted: formatBytes(memory.heapUsed),\r\n        externalFormatted: formatBytes(memory.external),\r\n        arrayBuffersFormatted: formatBytes(memory.arrayBuffers)\r\n      };\r\n    }\r\n    \r\n    return stats;\r\n  }\r\n  \r\n  /**\r\n   * Get current allocations\r\n   * @returns {AllocationRecord[]} Array of allocations\r\n   */\r\n  getAllocations() {\r\n    return Array.from(this.allocations.values());\r\n  }\r\n  \r\n  /**\r\n   * Get allocations grouped by type\r\n   * @returns {Object} Allocations grouped by type\r\n   */\r\n  getAllocationsByType() {\r\n    const byType = {};\r\n    \r\n    for (const allocation of this.allocations.values()) {\r\n      const type = allocation.type || 'unknown';\r\n      \r\n      if (!byType[type]) {\r\n        byType[type] = {\r\n          count: 0,\r\n          size: 0,\r\n          allocations: []\r\n        };\r\n      }\r\n      \r\n      byType[type].count++;\r\n      byType[type].size += allocation.size;\r\n      byType[type].allocations.push(allocation);\r\n    }\r\n    \r\n    // Add formatted sizes\r\n    for (const type in byType) {\r\n      byType[type].sizeFormatted = formatBytes(byType[type].size);\r\n    }\r\n    \r\n    return byType;\r\n  }\r\n  \r\n  /**\r\n   * Log memory usage information\r\n   * @param {boolean} [detailed=false] - Whether to include detailed allocation info\r\n   */\r\n  logMemoryUsage(detailed = false) {\r\n    const stats = this.getStats();\r\n    \r\n    logger.info('Memory Usage Summary:');\r\n    logger.info(`  Total Allocated: ${stats.allocatedFormatted}`);\r\n    logger.info(`  Total Freed: ${stats.freedFormatted}`);\r\n    logger.info(`  Active: ${stats.activeFormatted}`);\r\n    logger.info(`  Peak: ${stats.peakFormatted}`);\r\n    logger.info(`  Allocations: ${stats.allocations}`);\r\n    logger.info(`  Frees: ${stats.frees}`);\r\n    logger.info(`  Active Allocations: ${stats.activeAllocations}`);\r\n    \r\n    // Log browser memory if available\r\n    if (stats.browser) {\r\n      logger.info('Browser Memory:');\r\n      logger.info(`  Used JS Heap: ${stats.browser.usedJSHeapSizeFormatted}`);\r\n      logger.info(`  Total JS Heap: ${stats.browser.totalJSHeapSizeFormatted}`);\r\n      logger.info(`  JS Heap Limit: ${stats.browser.jsHeapSizeLimitFormatted}`);\r\n    }\r\n    \r\n    // Log Node.js memory if available\r\n    if (stats.node) {\r\n      logger.info('Node.js Memory:');\r\n      logger.info(`  RSS: ${stats.node.rssFormatted}`);\r\n      logger.info(`  Heap Total: ${stats.node.heapTotalFormatted}`);\r\n      logger.info(`  Heap Used: ${stats.node.heapUsedFormatted}`);\r\n      logger.info(`  External: ${stats.node.externalFormatted}`);\r\n      logger.info(`  Array Buffers: ${stats.node.arrayBuffersFormatted}`);\r\n    }\r\n    \r\n    // Log detailed allocation info if requested\r\n    if (detailed) {\r\n      const byType = this.getAllocationsByType();\r\n      \r\n      logger.info('Allocations by Type:');\r\n      for (const type in byType) {\r\n        logger.info(`  ${type}: ${byType[type].count} allocations, ${byType[type].sizeFormatted}`);\r\n      }\r\n      \r\n      if (detailed === 'full') {\r\n        logger.info('All Allocations:');\r\n        for (const allocation of this.allocations.values()) {\r\n          logger.info(`  ${allocation.id}: ${formatBytes(allocation.size)} (${allocation.type})${allocation.description ? ` - ${allocation.description}` : ''}`);\r\n        }\r\n      }\r\n    }\r\n  }\r\n  \r\n  /**\r\n   * Reset memory tracking stats (but keep allocations)\r\n   */\r\n  resetStats() {\r\n    // Calculate current active memory\r\n    const active = this.stats.allocated - this.stats.freed;\r\n    \r\n    // Reset stats but keep active memory\r\n    this.stats = {\r\n      allocated: active,\r\n      freed: 0,\r\n      active,\r\n      peak: active,\r\n      allocations: this.allocations.size,\r\n      frees: 0\r\n    };\r\n    \r\n    logger.debug('Memory stats reset');\r\n  }\r\n}\r\n\r\n/**\r\n * Format bytes to human-readable string\r\n * @param {number} bytes - Bytes to format\r\n * @param {number} [decimals=2] - Number of decimal places\r\n * @returns {string} Formatted string\r\n */\r\nfunction formatBytes(bytes, decimals = 2) {\r\n  if (bytes === 0) return '0 Bytes';\r\n  \r\n  const k = 1024;\r\n  const dm = decimals < 0 ? 0 : decimals;\r\n  const sizes = ['Bytes', 'KB', 'MB', 'GB', 'TB', 'PB', 'EB', 'ZB', 'YB'];\r\n  \r\n  const i = Math.floor(Math.log(bytes) / Math.log(k));\r\n  \r\n  return `${parseFloat((bytes / Math.pow(k, i)).toFixed(dm))} ${sizes[i]}`;\r\n}\r\n\r\n/**\r\n * Create a function wrapper that tracks memory usage\r\n * @param {Function} fn - Function to wrap\r\n * @param {Object} options - Options\r\n * @param {string} [options.type='function'] - Allocation type\r\n * @param {string} [options.description] - Allocation description\r\n * @param {MemoryManager} [options.memoryManager] - Memory manager to use\r\n * @returns {Function} Wrapped function\r\n */\r\nfunction withMemoryTracking(fn, options = {}) {\r\n  const manager = options.memoryManager || globalMemoryManager;\r\n  const type = options.type || 'function';\r\n  const description = options.description || fn.name || 'anonymous function';\r\n  \r\n  return async function(...args) {\r\n    // Track memory before function call\r\n    const beforeStats = manager.getStats();\r\n    \r\n    // Record allocation for function execution\r\n    const id = manager.allocate(0, { \r\n      type, \r\n      description: `Function call: ${description}`\r\n    });\r\n    \r\n    try {\r\n      // Execute the function\r\n      const result = await fn.apply(this, args);\r\n      \r\n      // Calculate memory used\r\n      const afterStats = manager.getStats();\r\n      const memoryUsed = afterStats.active - beforeStats.active;\r\n      \r\n      // Update allocation size\r\n      manager.free(id);\r\n      manager.allocate(memoryUsed, { \r\n        type, \r\n        description: `Result of: ${description}`\r\n      });\r\n      \r\n      return result;\r\n    } catch (error) {\r\n      // Free allocation on error\r\n      manager.free(id);\r\n      throw error;\r\n    }\r\n  };\r\n}\r\n\r\n/**\r\n * Try to trigger garbage collection (only works in debug environments)\r\n * @returns {boolean} Whether GC was triggered\r\n */\r\nfunction triggerGC() {\r\n  if (typeof global !== 'undefined' && global.gc) {\r\n    // Node.js with --expose-gc flag\r\n    global.gc();\r\n    logger.debug('Triggered Node.js garbage collection');\r\n    return true;\r\n  } else if (typeof window !== 'undefined' && window.gc) {\r\n    // Some browsers in debug mode\r\n    window.gc();\r\n    logger.debug('Triggered browser garbage collection');\r\n    return true;\r\n  }\r\n  \r\n  logger.debug('Cannot trigger garbage collection (not available in this environment)');\r\n  return false;\r\n}\r\n\r\n/**\r\n * Create a fixed-size memory pool for reusing allocations\r\n * @param {number} totalSize - Total pool size in bytes\r\n * @returns {Object} Memory pool interface\r\n */\r\nfunction createMemoryPool(totalSize) {\r\n  // Create the underlying buffer\r\n  const buffer = typeof ArrayBuffer !== 'undefined' ? \r\n    new ArrayBuffer(totalSize) : \r\n    Buffer.alloc(totalSize);\r\n  \r\n  // Track allocations\r\n  const allocations = [];\r\n  let freeOffset = 0;\r\n  \r\n  return {\r\n    /**\r\n     * Allocate memory from the pool\r\n     * @param {number} size - Size in bytes\r\n     * @returns {Object} Allocation with buffer view\r\n     */\r\n    allocate(size) {\r\n      if (freeOffset + size > totalSize) {\r\n        throw new Error(`Memory pool out of space (${formatBytes(size)} requested, ${formatBytes(totalSize - freeOffset)} available)`);\r\n      }\r\n      \r\n      const start = freeOffset;\r\n      freeOffset += size;\r\n      \r\n      const view = typeof Uint8Array !== 'undefined' ? \r\n        new Uint8Array(buffer, start, size) : \r\n        Buffer.from(buffer, start, size);\r\n      \r\n      const allocation = { start, size, view };\r\n      allocations.push(allocation);\r\n      \r\n      return allocation;\r\n    },\r\n    \r\n    /**\r\n     * Free a specific allocation\r\n     * @param {Object} allocation - Allocation to free\r\n     * @returns {boolean} Whether the free was successful\r\n     */\r\n    free(allocation) {\r\n      const index = allocations.indexOf(allocation);\r\n      if (index === -1) return false;\r\n      \r\n      // Remove the allocation\r\n      allocations.splice(index, 1);\r\n      \r\n      // This is a simple implementation that doesn't reuse freed space\r\n      // In a real implementation, you'd want to track free blocks and reuse them\r\n      \r\n      return true;\r\n    },\r\n    \r\n    /**\r\n     * Reset the pool (free all allocations)\r\n     */\r\n    reset() {\r\n      allocations.length = 0;\r\n      freeOffset = 0;\r\n    },\r\n    \r\n    /**\r\n     * Get pool usage information\r\n     * @returns {Object} Pool info\r\n     */\r\n    getInfo() {\r\n      return {\r\n        totalSize,\r\n        used: freeOffset,\r\n        free: totalSize - freeOffset,\r\n        allocations: allocations.length,\r\n        usedFormatted: formatBytes(freeOffset),\r\n        freeFormatted: formatBytes(totalSize - freeOffset),\r\n        totalFormatted: formatBytes(totalSize)\r\n      };\r\n    }\r\n  };\r\n}\r\n\r\n// Create a global memory manager instance\r\nconst globalMemoryManager = new MemoryManager();\r\n\r\nmodule.exports = {\r\n  MemoryManager,\r\n  formatBytes,\r\n  withMemoryTracking,\r\n  triggerGC,\r\n  createMemoryPool,\r\n  globalMemoryManager\r\n};","/**\r\n * Streaming response utilities for GGUF.js\r\n * @module utils/streaming\r\n */\r\n\r\nconst logger = require('./logger');\r\n\r\n/**\r\n * A token stream implementation for handling sequential tokens\r\n */\r\nclass TokenStream {\r\n  /**\r\n   * Create a token stream\r\n   * @param {Object} options - Stream options\r\n   * @param {number} [options.bufferSize=1024] - Maximum number of tokens to buffer\r\n   * @param {boolean} [options.autoFlush=true] - Whether to auto-flush the buffer when full\r\n   */\r\n  constructor(options = {}) {\r\n    this.buffer = [];\r\n    this.bufferSize = options.bufferSize || 1024;\r\n    this.autoFlush = options.autoFlush !== false;\r\n    this.isEnded = false;\r\n    this.consumers = [];\r\n    this.doneCallbacks = [];\r\n  }\r\n  \r\n  /**\r\n   * Write a token to the stream\r\n   * @param {string|number} token - Token to write\r\n   * @returns {boolean} Whether the write was successful\r\n   */\r\n  write(token) {\r\n    if (this.isEnded) {\r\n      logger.warn('Attempted to write to a closed token stream');\r\n      return false;\r\n    }\r\n    \r\n    // Add token to buffer\r\n    this.buffer.push(token);\r\n    \r\n    // Notify consumers\r\n    this._notifyConsumers();\r\n    \r\n    // Auto-flush if buffer is full\r\n    if (this.autoFlush && this.buffer.length >= this.bufferSize) {\r\n      this.flush();\r\n    }\r\n    \r\n    return true;\r\n  }\r\n  \r\n  /**\r\n   * Write multiple tokens to the stream\r\n   * @param {Array<string|number>} tokens - Tokens to write\r\n   * @returns {boolean} Whether the write was successful\r\n   */\r\n  writeMany(tokens) {\r\n    if (this.isEnded) {\r\n      logger.warn('Attempted to write to a closed token stream');\r\n      return false;\r\n    }\r\n    \r\n    // Add tokens to buffer\r\n    this.buffer.push(...tokens);\r\n    \r\n    // Notify consumers\r\n    this._notifyConsumers();\r\n    \r\n    // Auto-flush if buffer is full\r\n    if (this.autoFlush && this.buffer.length >= this.bufferSize) {\r\n      this.flush();\r\n    }\r\n    \r\n    return true;\r\n  }\r\n  \r\n  /**\r\n   * End the stream (no more tokens will be accepted)\r\n   */\r\n  end() {\r\n    if (this.isEnded) return;\r\n    \r\n    this.isEnded = true;\r\n    this._notifyConsumers();\r\n    \r\n    // Notify all done callbacks\r\n    for (const callback of this.doneCallbacks) {\r\n      try {\r\n        callback();\r\n      } catch (error) {\r\n        logger.error('Error in stream done callback', error);\r\n      }\r\n    }\r\n    this.doneCallbacks = [];\r\n  }\r\n  \r\n  /**\r\n   * Flush the buffer (clear tokens and free memory)\r\n   */\r\n  flush() {\r\n    this.buffer = [];\r\n  }\r\n  \r\n  /**\r\n   * Subscribe to tokens from the stream\r\n   * @param {Function} onToken - Callback for each token\r\n   * @param {Function} [onDone] - Callback when stream is ended\r\n   * @returns {Function} Unsubscribe function\r\n   */\r\n  subscribe(onToken, onDone) {\r\n    const consumer = { onToken };\r\n    this.consumers.push(consumer);\r\n    \r\n    // Add done callback if provided\r\n    if (onDone) {\r\n      this.doneCallbacks.push(onDone);\r\n    }\r\n    \r\n    // Immediately notify of any existing tokens\r\n    if (this.buffer.length > 0) {\r\n      for (const token of this.buffer) {\r\n        try {\r\n          onToken(token, false);\r\n        } catch (error) {\r\n          logger.error('Error in token consumer', error);\r\n        }\r\n      }\r\n    }\r\n    \r\n    // If stream is already ended, call onDone immediately\r\n    if (this.isEnded && onDone) {\r\n      try {\r\n        onDone();\r\n      } catch (error) {\r\n        logger.error('Error in stream done callback', error);\r\n      }\r\n    }\r\n    \r\n    // Return unsubscribe function\r\n    return () => {\r\n      this.consumers = this.consumers.filter(c => c !== consumer);\r\n      this.doneCallbacks = this.doneCallbacks.filter(cb => cb !== onDone);\r\n    };\r\n  }\r\n  \r\n  /**\r\n   * Read all available tokens\r\n   * @returns {Array} Array of tokens\r\n   */\r\n  read() {\r\n    const tokens = [...this.buffer];\r\n    return tokens;\r\n  }\r\n  \r\n  /**\r\n   * Notify all consumers of new tokens\r\n   * @private\r\n   */\r\n  _notifyConsumers() {\r\n    if (this.consumers.length === 0) return;\r\n    \r\n    while (this.buffer.length > 0) {\r\n      const token = this.buffer.shift();\r\n      \r\n      for (const consumer of this.consumers) {\r\n        try {\r\n          consumer.onToken(token, false);\r\n        } catch (error) {\r\n          logger.error('Error in token consumer', error);\r\n        }\r\n      }\r\n    }\r\n    \r\n    // If stream is ended, notify consumers with the final flag\r\n    if (this.isEnded) {\r\n      for (const consumer of this.consumers) {\r\n        try {\r\n          consumer.onToken(null, true);\r\n        } catch (error) {\r\n          logger.error('Error in token consumer', error);\r\n        }\r\n      }\r\n      \r\n      this.consumers = [];\r\n    }\r\n  }\r\n  \r\n  /**\r\n   * Check if the stream has ended\r\n   * @returns {boolean} Whether the stream has ended\r\n   */\r\n  isDone() {\r\n    return this.isEnded;\r\n  }\r\n  \r\n  /**\r\n   * Get the number of remaining tokens in the buffer\r\n   * @returns {number} Number of tokens in buffer\r\n   */\r\n  bufferedTokens() {\r\n    return this.buffer.length;\r\n  }\r\n}\r\n\r\n/**\r\n * Create a token stream from an array of tokens\r\n * @param {Array<string|number>} tokens - Tokens to stream\r\n * @param {Object} options - Stream options\r\n * @param {number} [options.delay=0] - Delay between tokens in milliseconds\r\n * @param {boolean} [options.jitter=false] - Whether to add random jitter to delay\r\n * @param {number} [options.maxJitter=10] - Maximum jitter in milliseconds\r\n * @returns {TokenStream} Token stream\r\n */\r\nfunction createTokenStream(tokens, options = {}) {\r\n  const stream = new TokenStream();\r\n  \r\n  if (!tokens || tokens.length === 0) {\r\n    stream.end();\r\n    return stream;\r\n  }\r\n  \r\n  const delay = options.delay || 0;\r\n  const jitter = options.jitter || false;\r\n  const maxJitter = options.maxJitter || 10;\r\n  \r\n  let index = 0;\r\n  \r\n  function processNextToken() {\r\n    if (index >= tokens.length) {\r\n      stream.end();\r\n      return;\r\n    }\r\n    \r\n    stream.write(tokens[index++]);\r\n    \r\n    if (index < tokens.length) {\r\n      let tokenDelay = delay;\r\n      \r\n      // Add jitter if enabled\r\n      if (jitter) {\r\n        tokenDelay += Math.random() * maxJitter * 2 - maxJitter;\r\n        tokenDelay = Math.max(0, tokenDelay);\r\n      }\r\n      \r\n      setTimeout(processNextToken, tokenDelay);\r\n    } else {\r\n      stream.end();\r\n    }\r\n  }\r\n  \r\n  // Start processing tokens\r\n  if (delay > 0) {\r\n    setTimeout(processNextToken, 0);\r\n  } else {\r\n    // Immediate processing for no delay\r\n    stream.writeMany(tokens);\r\n    stream.end();\r\n  }\r\n  \r\n  return stream;\r\n}\r\n\r\n/**\r\n * Collect all tokens from a stream into a single value\r\n * @param {TokenStream} stream - Token stream to collect\r\n * @param {Function} [onProgress] - Callback for progress updates\r\n * @returns {Promise<Array>} Array of all tokens\r\n */\r\nfunction collectStream(stream, onProgress = null) {\r\n  return new Promise((resolve, reject) => {\r\n    const tokens = [];\r\n    \r\n    stream.subscribe(\r\n      (token, isDone) => {\r\n        if (token !== null) {\r\n          tokens.push(token);\r\n          \r\n          if (onProgress) {\r\n            try {\r\n              onProgress(tokens.length, token);\r\n            } catch (error) {\r\n              // Ignore errors in progress callback\r\n            }\r\n          }\r\n        }\r\n        \r\n        if (isDone) {\r\n          resolve(tokens);\r\n        }\r\n      },\r\n      () => resolve(tokens)\r\n    );\r\n  });\r\n}\r\n\r\n/**\r\n * Convert a stream of tokens to a string\r\n * @param {TokenStream} stream - Token stream to collect\r\n * @param {Function} [decoder] - Function to decode tokens to strings\r\n * @param {Function} [onProgress] - Callback for progress updates\r\n * @returns {Promise<string>} Collected string\r\n */\r\nfunction streamToString(stream, decoder = String, onProgress = null) {\r\n  let result = '';\r\n  \r\n  return new Promise((resolve, reject) => {\r\n    stream.subscribe(\r\n      (token, isDone) => {\r\n        if (token !== null) {\r\n          const tokenStr = decoder(token);\r\n          result += tokenStr;\r\n          \r\n          if (onProgress) {\r\n            try {\r\n              onProgress(result.length, tokenStr);\r\n            } catch (error) {\r\n              // Ignore errors in progress callback\r\n            }\r\n          }\r\n        }\r\n        \r\n        if (isDone) {\r\n          resolve(result);\r\n        }\r\n      },\r\n      () => resolve(result)\r\n    );\r\n  });\r\n}\r\n\r\n/**\r\n * Fetch a resource with progress tracking\r\n * @param {string} url - URL to fetch\r\n * @param {Function} [onProgress] - Progress callback (loaded, total, percentage)\r\n * @returns {Promise<ArrayBuffer>} Fetched data\r\n */\r\nasync function fetchWithProgress(url, onProgress) {\r\n  // Check for fetch API\r\n  if (typeof fetch !== 'function') {\r\n    throw new Error('Fetch API not available in this environment');\r\n  }\r\n  \r\n  const response = await fetch(url);\r\n  \r\n  if (!response.ok) {\r\n    throw new Error(`HTTP error ${response.status}: ${response.statusText}`);\r\n  }\r\n  \r\n  // Get content length if available\r\n  const contentLength = parseInt(response.headers.get('Content-Length') || '0', 10);\r\n  \r\n  // If we can't stream or track progress, just return the buffer\r\n  if (!response.body || !contentLength) {\r\n    return response.arrayBuffer();\r\n  }\r\n  \r\n  // Create a reader to stream the response\r\n  const reader = response.body.getReader();\r\n  let receivedLength = 0;\r\n  const chunks = [];\r\n  \r\n  // Process the stream\r\n  while (true) {\r\n    const { done, value } = await reader.read();\r\n    \r\n    if (done) {\r\n      break;\r\n    }\r\n    \r\n    chunks.push(value);\r\n    receivedLength += value.length;\r\n    \r\n    // Call progress callback\r\n    if (onProgress) {\r\n      const percentage = contentLength ? Math.round((receivedLength / contentLength) * 100) : 0;\r\n      onProgress(receivedLength, contentLength, percentage);\r\n    }\r\n  }\r\n  \r\n  // Concatenate chunks into a single buffer\r\n  const result = new Uint8Array(receivedLength);\r\n  let position = 0;\r\n  \r\n  for (const chunk of chunks) {\r\n    result.set(chunk, position);\r\n    position += chunk.length;\r\n  }\r\n  \r\n  return result.buffer;\r\n}\r\n\r\n/**\r\n * Create a chunked data loader for large files\r\n * @param {string} url - URL to the file\r\n * @param {number} [chunkSize=10*1024*1024] - Chunk size in bytes (default: 10MB)\r\n * @param {Function} [onProgress] - Progress callback\r\n * @returns {Object} Chunked loader interface\r\n */\r\nfunction createChunkedLoader(url, chunkSize = 10 * 1024 * 1024, onProgress) {\r\n  let contentLength = 0;\r\n  let loaded = 0;\r\n  let cancelled = false;\r\n  \r\n  /**\r\n   * Initialize the loader and get content info\r\n   * @returns {Promise<number>} Content length\r\n   */\r\n  async function init() {\r\n    try {\r\n      // Get content length with HEAD request\r\n      const response = await fetch(url, { method: 'HEAD' });\r\n      \r\n      if (!response.ok) {\r\n        throw new Error(`Failed to initialize: ${response.status} ${response.statusText}`);\r\n      }\r\n      \r\n      contentLength = parseInt(response.headers.get('Content-Length') || '0', 10);\r\n      return contentLength;\r\n    } catch (error) {\r\n      logger.error('Error initializing chunked loader', error);\r\n      throw error;\r\n    }\r\n  }\r\n  \r\n  /**\r\n   * Load a specific chunk\r\n   * @param {number} start - Start byte\r\n   * @param {number} end - End byte\r\n   * @returns {Promise<ArrayBuffer>} Chunk data\r\n   */\r\n  async function loadChunk(start, end) {\r\n    if (cancelled) {\r\n      throw new Error('Loading cancelled');\r\n    }\r\n    \r\n    // Adjust end to not exceed content length\r\n    if (contentLength > 0 && end > contentLength) {\r\n      end = contentLength;\r\n    }\r\n    \r\n    try {\r\n      const response = await fetch(url, {\r\n        headers: { Range: `bytes=${start}-${end - 1}` }\r\n      });\r\n      \r\n      if (!response.ok && response.status !== 206) {\r\n        throw new Error(`Failed to load chunk: ${response.status} ${response.statusText}`);\r\n      }\r\n      \r\n      const buffer = await response.arrayBuffer();\r\n      \r\n      // Update progress\r\n      loaded += buffer.byteLength;\r\n      \r\n      if (onProgress && contentLength > 0) {\r\n        const percentage = Math.round((loaded / contentLength) * 100);\r\n        onProgress(loaded, contentLength, percentage);\r\n      }\r\n      \r\n      return buffer;\r\n    } catch (error) {\r\n      if (!cancelled) {\r\n        logger.error(`Error loading chunk ${start}-${end}`, error);\r\n        throw error;\r\n      }\r\n      throw new Error('Loading cancelled');\r\n    }\r\n  }\r\n  \r\n  /**\r\n   * Load the entire file in chunks\r\n   * @returns {Promise<ArrayBuffer>} Complete file data\r\n   */\r\n  async function loadAll() {\r\n    // Initialize if not already done\r\n    if (contentLength === 0) {\r\n      await init();\r\n    }\r\n    \r\n    // If content length is unknown, fallback to regular fetch\r\n    if (contentLength === 0) {\r\n      return fetchWithProgress(url, onProgress);\r\n    }\r\n    \r\n    const chunks = [];\r\n    let position = 0;\r\n    \r\n    while (position < contentLength) {\r\n      const end = Math.min(position + chunkSize, contentLength);\r\n      const chunk = await loadChunk(position, end);\r\n      chunks.push(chunk);\r\n      position = end;\r\n      \r\n      if (cancelled) {\r\n        throw new Error('Loading cancelled');\r\n      }\r\n    }\r\n    \r\n    // Combine chunks\r\n    const result = new Uint8Array(contentLength);\r\n    let offset = 0;\r\n    \r\n    for (const chunk of chunks) {\r\n      result.set(new Uint8Array(chunk), offset);\r\n      offset += chunk.byteLength;\r\n    }\r\n    \r\n    return result.buffer;\r\n  }\r\n  \r\n  /**\r\n   * Cancel loading\r\n   */\r\n  function cancel() {\r\n    cancelled = true;\r\n  }\r\n  \r\n  /**\r\n   * Get current loading status\r\n   * @returns {Object} Status object\r\n   */\r\n  function getStatus() {\r\n    return {\r\n      contentLength,\r\n      loaded,\r\n      cancelled,\r\n      progress: contentLength ? loaded / contentLength : 0\r\n    };\r\n  }\r\n  \r\n  return {\r\n    init,\r\n    loadChunk,\r\n    loadAll,\r\n    cancel,\r\n    getStatus\r\n  };\r\n}\r\n\r\nmodule.exports = {\r\n  TokenStream,\r\n  createTokenStream,\r\n  collectStream,\r\n  streamToString,\r\n  fetchWithProgress,\r\n  createChunkedLoader\r\n};","/**\r\n * Text formatting utilities for GGUF models\r\n * @module utils/formats\r\n */\r\n\r\n/**\r\n * Formats a prompt for chat completion\r\n * \r\n * @param {Array<Object>} messages - Array of message objects with role and content\r\n * @param {Object} options - Formatting options\r\n * @param {string} [options.systemPrompt=''] - System prompt to prepend\r\n * @param {boolean} [options.useMarkdown=false] - Whether to format in markdown\r\n * @param {string} [options.userLabel='User'] - Label for user messages\r\n * @param {string} [options.assistantLabel='Assistant'] - Label for assistant messages\r\n * @param {string} [options.systemLabel='System'] - Label for system messages\r\n * @returns {string} Formatted prompt\r\n */\r\nfunction formatChatPrompt(messages, options = {}) {\r\n  const {\r\n    systemPrompt = '',\r\n    useMarkdown = false,\r\n    userLabel = 'User',\r\n    assistantLabel = 'Assistant',\r\n    systemLabel = 'System'\r\n  } = options;\r\n\r\n  let prompt = '';\r\n  \r\n  // Add system prompt if provided\r\n  if (systemPrompt) {\r\n    if (useMarkdown) {\r\n      prompt += `**${systemLabel}**: ${systemPrompt}\\n\\n`;\r\n    } else {\r\n      prompt += `${systemLabel}: ${systemPrompt}\\n\\n`;\r\n    }\r\n  }\r\n  \r\n  // Add all messages\r\n  for (const message of messages) {\r\n    let label = '';\r\n    \r\n    // Determine the label based on role\r\n    if (message.role === 'user') {\r\n      label = userLabel;\r\n    } else if (message.role === 'assistant') {\r\n      label = assistantLabel;\r\n    } else if (message.role === 'system') {\r\n      label = systemLabel;\r\n    }\r\n    \r\n    // Add the message with appropriate formatting\r\n    if (useMarkdown) {\r\n      prompt += `**${label}**: ${message.content}\\n\\n`;\r\n    } else {\r\n      prompt += `${label}: ${message.content}\\n\\n`;\r\n    }\r\n  }\r\n  \r\n  // Add final prompt for assistant response\r\n  if (useMarkdown) {\r\n    prompt += `**${assistantLabel}**: `;\r\n  } else {\r\n    prompt += `${assistantLabel}: `;\r\n  }\r\n  \r\n  return prompt;\r\n}\r\n\r\n/**\r\n * Formats a completion prompt\r\n * \r\n * @param {string} prompt - The completion prompt\r\n * @param {Object} options - Formatting options\r\n * @param {string} [options.suffix=''] - Text to append after the prompt\r\n * @param {string} [options.prefix=''] - Text to prepend before the prompt\r\n * @returns {string} Formatted prompt\r\n */\r\nfunction formatCompletionPrompt(prompt, options = {}) {\r\n  const { suffix = '', prefix = '' } = options;\r\n  return `${prefix}${prompt}${suffix}`;\r\n}\r\n\r\n/**\r\n * Formats a list of instructions for instruction-following models\r\n * \r\n * @param {string|Array<string>} instructions - Instruction or list of instructions\r\n * @param {Object} options - Formatting options\r\n * @param {string} [options.prefix='Instructions:'] - Text to prepend before instructions\r\n * @param {string} [options.suffix=''] - Text to append after instructions\r\n * @param {boolean} [options.numbered=false] - Whether to number the instructions\r\n * @returns {string} Formatted instructions\r\n */\r\nfunction formatInstructions(instructions, options = {}) {\r\n  const {\r\n    prefix = 'Instructions:',\r\n    suffix = '',\r\n    numbered = false\r\n  } = options;\r\n  \r\n  let instructionList = Array.isArray(instructions) ? instructions : [instructions];\r\n  let formatted = prefix ? `${prefix}\\n` : '';\r\n  \r\n  // Add each instruction\r\n  instructionList.forEach((instruction, index) => {\r\n    if (numbered) {\r\n      formatted += `${index + 1}. ${instruction}\\n`;\r\n    } else {\r\n      formatted += `- ${instruction}\\n`;\r\n    }\r\n  });\r\n  \r\n  // Add suffix\r\n  if (suffix) {\r\n    formatted += `\\n${suffix}`;\r\n  }\r\n  \r\n  return formatted;\r\n}\r\n\r\n/**\r\n * Cleans and normalizes text output from the model\r\n * \r\n * @param {string} text - Text to clean\r\n * @param {Object} options - Cleaning options\r\n * @param {boolean} [options.trimWhitespace=true] - Whether to trim whitespace\r\n * @param {boolean} [options.removeExtraNewlines=true] - Whether to remove extra newlines\r\n * @param {boolean} [options.removeSpecialTokens=true] - Whether to remove special tokens like <eos>\r\n * @returns {string} Cleaned text\r\n */\r\nfunction cleanOutput(text, options = {}) {\r\n  const {\r\n    trimWhitespace = true,\r\n    removeExtraNewlines = true,\r\n    removeSpecialTokens = true\r\n  } = options;\r\n  \r\n  let cleaned = text;\r\n  \r\n  // Remove special tokens\r\n  if (removeSpecialTokens) {\r\n    cleaned = cleaned.replace(/<[^>]+>/g, '');\r\n  }\r\n  \r\n  // Remove extra newlines\r\n  if (removeExtraNewlines) {\r\n    cleaned = cleaned.replace(/\\n{3,}/g, '\\n\\n');\r\n  }\r\n  \r\n  // Trim whitespace\r\n  if (trimWhitespace) {\r\n    cleaned = cleaned.trim();\r\n  }\r\n  \r\n  return cleaned;\r\n}\r\n\r\n/**\r\n * Extracts and formats JSON response from model output\r\n * \r\n * @param {string} text - Model output text\r\n * @param {Object} options - Formatting options\r\n * @param {boolean} [options.extractFromText=true] - Whether to extract JSON from text\r\n * @param {boolean} [options.fixBrokenJson=true] - Attempt to fix malformed JSON\r\n * @returns {Object|null} Parsed JSON object or null if parsing failed\r\n */\r\nfunction extractJSON(text, options = {}) {\r\n  const {\r\n    extractFromText = true,\r\n    fixBrokenJson = true\r\n  } = options;\r\n  \r\n  let jsonText = text;\r\n  \r\n  // Extract JSON from text if requested\r\n  if (extractFromText) {\r\n    const jsonMatch = text.match(/```(?:json)?\\s*({[\\s\\S]*?})\\s*```/) || \r\n                      text.match(/{[\\s\\S]*?}/);\r\n    \r\n    if (jsonMatch) {\r\n      jsonText = jsonMatch[1];\r\n    }\r\n  }\r\n  \r\n  // Try to parse the JSON\r\n  try {\r\n    return JSON.parse(jsonText);\r\n  } catch (error) {\r\n    // If fixing broken JSON is not requested, return null\r\n    if (!fixBrokenJson) {\r\n      return null;\r\n    }\r\n    \r\n    // Attempt to fix common JSON issues\r\n    try {\r\n      // Replace single quotes with double quotes\r\n      let fixedJson = jsonText.replace(/'/g, '\"');\r\n      \r\n      // Fix unquoted keys\r\n      fixedJson = fixedJson.replace(/([{,]\\s*)(\\w+)(\\s*:)/g, '$1\"$2\"$3');\r\n      \r\n      // Fix trailing commas\r\n      fixedJson = fixedJson.replace(/,\\s*}/g, '}').replace(/,\\s*]/g, ']');\r\n      \r\n      return JSON.parse(fixedJson);\r\n    } catch (fixError) {\r\n      return null;\r\n    }\r\n  }\r\n}\r\n\r\n/**\r\n * Creates a templated prompt with variable substitution\r\n * \r\n * @param {string} template - Template string with {variable} placeholders\r\n * @param {Object} variables - Object containing variable values\r\n * @returns {string} Formatted prompt with substituted variables\r\n */\r\nfunction templatePrompt(template, variables = {}) {\r\n  return template.replace(/\\{(\\w+)\\}/g, (match, variable) => {\r\n    return variables[variable] !== undefined ? variables[variable] : match;\r\n  });\r\n}\r\n\r\n/**\r\n * Truncates a prompt to fit within token limit\r\n * \r\n * @param {string} text - Text to truncate\r\n * @param {number} maxTokens - Maximum number of tokens\r\n * @param {Function} tokenCounter - Function to count tokens\r\n * @param {Object} options - Options for truncation\r\n * @param {boolean} [options.fromStart=true] - Truncate from start (keep end)\r\n * @param {string} [options.ellipsis='...'] - Ellipsis to use\r\n * @returns {string} Truncated text\r\n */\r\nfunction truncatePrompt(text, maxTokens, tokenCounter, options = {}) {\r\n  const {\r\n    fromStart = true,\r\n    ellipsis = '...'\r\n  } = options;\r\n  \r\n  // If text is already within token limit, return as is\r\n  const tokenCount = tokenCounter(text);\r\n  if (tokenCount <= maxTokens) {\r\n    return text;\r\n  }\r\n  \r\n  // Reserve tokens for ellipsis\r\n  const ellipsisTokens = tokenCounter(ellipsis);\r\n  const effectiveMaxTokens = maxTokens - ellipsisTokens;\r\n  \r\n  // Split text into words\r\n  const words = text.split(/\\s+/);\r\n  \r\n  if (fromStart) {\r\n    // Truncate from start (keep the end)\r\n    let result = '';\r\n    let currentTokens = 0;\r\n    \r\n    for (let i = words.length - 1; i >= 0; i--) {\r\n      const word = words[i];\r\n      const wordTokens = tokenCounter(word);\r\n      \r\n      if (currentTokens + wordTokens <= effectiveMaxTokens) {\r\n        result = word + (result ? ' ' + result : '');\r\n        currentTokens += wordTokens;\r\n      } else {\r\n        break;\r\n      }\r\n    }\r\n    \r\n    return ellipsis + ' ' + result;\r\n  } else {\r\n    // Truncate from end (keep the beginning)\r\n    let result = '';\r\n    let currentTokens = 0;\r\n    \r\n    for (let i = 0; i < words.length; i++) {\r\n      const word = words[i];\r\n      const wordTokens = tokenCounter(word);\r\n      \r\n      if (currentTokens + wordTokens <= effectiveMaxTokens) {\r\n        result += (result ? ' ' : '') + word;\r\n        currentTokens += wordTokens;\r\n      } else {\r\n        break;\r\n      }\r\n    }\r\n    \r\n    return result + ' ' + ellipsis;\r\n  }\r\n}\r\n\r\nmodule.exports = {\r\n  formatChatPrompt,\r\n  formatCompletionPrompt,\r\n  formatInstructions,\r\n  cleanOutput,\r\n  extractJSON,\r\n  templatePrompt,\r\n  truncatePrompt\r\n};","/**\r\n * GGUF.js - Lightweight JavaScript package for GGUF language models\r\n * @module gguf\r\n */\r\n\r\n// Core functionality\r\nconst { GGUFModel, GenerationOptions, ModelContext, createModelId } = require('./core/model');\r\nconst loader = require('./core/loader');\r\nconst inference = require('./core/inference');\r\nconst tokenizer = require('./core/tokenizer');\r\n\r\n// Adapters\r\nconst nodeAdapter = require('./adapters/node');\r\nconst browserAdapter = require('./adapters/browser');\r\nconst reactHooks = require('./adapters/react');\r\n\r\n// Utilities\r\nconst logger = require('./utils/logger');\r\nconst memory = require('./utils/memory');\r\nconst streaming = require('./utils/streaming');\r\nconst formats = require('./utils/formats');\r\n\r\n// Determine environment\r\nconst isNode = typeof window === 'undefined' && typeof process !== 'undefined' && process.versions && process.versions.node;\r\nconst isBrowser = typeof window !== 'undefined';\r\nconst isReact = typeof window !== 'undefined' && typeof window.React !== 'undefined';\r\n\r\n// Get the default adapter based on environment\r\nconst getDefaultAdapter = () => {\r\n  if (isNode) {\r\n    return nodeAdapter;\r\n  } else if (isBrowser) {\r\n    return browserAdapter;\r\n  }\r\n  \r\n  // Fallback\r\n  logger.warn('No environment-specific adapter detected, some features may not work properly');\r\n  return null;\r\n};\r\n\r\n/**\r\n * Create an instance of GGUF.js with a specific adapter\r\n * @param {Object} adapter - The adapter to use\r\n * @returns {Object} GGUF.js instance\r\n */\r\nfunction createInstance(adapter) {\r\n  return {\r\n    // Core model functionality\r\n    loadModel: (path, options) => loader.loadModel(path, options, adapter),\r\n    unloadModel: (modelId) => loader.unloadModel(modelId, adapter),\r\n    getModel: (modelId) => loader.getModel(modelId),\r\n    listModels: () => loader.listModels(),\r\n    clearModels: () => loader.clearModels(adapter),\r\n    \r\n    // Text generation\r\n    generate: (model, prompt, options) => inference.generate(model, prompt, options, adapter),\r\n    streamGenerate: (model, prompt, onToken, options) => inference.streamGenerate(model, prompt, onToken, options, adapter),\r\n    cancelGeneration: (modelOrId) => inference.cancelGeneration(modelOrId),\r\n    \r\n    // Tokenization\r\n    tokenize: (model, text) => adapter.tokenize(model, text),\r\n    detokenize: (model, tokens) => adapter.detokenize(model, tokens),\r\n    \r\n    // Additional functionality based on adapter\r\n    ...(adapter.discoverModels ? { discoverModels: (dir) => adapter.discoverModels(dir) } : {}),\r\n    \r\n    // React hooks if in React environment\r\n    ...(isReact ? {\r\n      useModel: (options) => reactHooks.useModel(adapter, options),\r\n      useGeneration: (options) => reactHooks.useGeneration(adapter, options),\r\n      useChat: (options) => reactHooks.useChat(adapter, options)\r\n    } : {})\r\n  };\r\n}\r\n\r\n// Create a default instance with auto-detected adapter\r\nconst defaultAdapter = getDefaultAdapter();\r\nconst defaultInstance = createInstance(defaultAdapter);\r\n\r\n// Export default instance with all functionality\r\nmodule.exports = {\r\n  // Default instance methods\r\n  ...defaultInstance,\r\n  \r\n  // Core classes and functions\r\n  GGUFModel,\r\n  GenerationOptions,\r\n  ModelContext,\r\n  createModelId,\r\n  \r\n  // Factory function for custom instances\r\n  createInstance,\r\n  \r\n  // Adapters\r\n  adapters: {\r\n    node: nodeAdapter,\r\n    browser: browserAdapter,\r\n    react: reactHooks\r\n  },\r\n  \r\n  // Tokenization\r\n  tokenizer,\r\n  \r\n  // Utilities\r\n  utils: {\r\n    logger,\r\n    memory,\r\n    streaming,\r\n    formats\r\n  },\r\n  \r\n  // Environment detection\r\n  environment: {\r\n    isNode,\r\n    isBrowser,\r\n    isReact\r\n  },\r\n  \r\n  // Version information\r\n  version: '0.1.0'\r\n};"],"names":["model","GGUFModel","constructor","id","path","metadata","session","tokenizer","this","lastUsed","Date","now","isLoaded","name","split","pop","createdAt","toISOString","getInfo","updateLastUsed","unload","GenerationOptions","maxTokens","temperature","topP","topK","repetitionPenalty","seed","stopSequences","ModelContext","tokens","contextSize","outputTokens","addTokens","length","slice","addOutputToken","token","push","getContextLength","getRemainingSpace","clear","createModelId","filename","hash","i","charCodeAt","Math","abs","toString","substring","LOG_LEVELS","NONE","ERROR","WARN","INFO","DEBUG","config","level","process","env","NODE_ENV","prefix","useColors","timestamp","showInConsole","COLORS","reset","black","red","green","yellow","blue","magenta","cyan","white","error","warn","info","debug","logStore","log","message","data","toUpperCase","formattedMessage","parts","toLowerCase","join","formatMessage","logEntry","formatted","shift","consoleMethod","console","undefined","errorData","Error","stack","logger","configure","options","levelName","getLogs","clearLogs","createLogger","require$$0","require$$1","loadedModels","Map","loader","loadModel","async","adapter","useCache","quantization","lowMemory","modelId","customModelId","has","cachedModel","get","initializeModel","set","unloadModel","freeModel","delete","getModel","listModels","Array","from","values","map","sort","a","b","clearModels","entries","cancellationFlags","inference","generate","prompt","generationOptions","tokenize","context","maxNewTokens","min","runInference","outputText","detokenize","streamGenerate","onToken","tokenCallback","tokenId","tokenText","detokenizeToken","runInferenceStreaming","cancelGeneration","modelOrId","BaseTokenizer","encode","text","decode","decodeToken","CharacterTokenizer","super","encoder","decoder","char","String","fromCharCode","specialTokens","BOS","EOS","PAD","UNK","Object","forEach","BPETokenizer","vocab","k","v","Number","merges","mergePatterns","buildMergePatterns","pair","first","second","pattern","RegExp","replacement","flatMap","part","trim","c","j","createTokenizer","tokenizer_type","loadTokenizerFromFile","readFile","JSON","parse","extractTokenizerFromModel","extractTokenizer","fs","require$$2","LlamaModel","llamaCpp","require","node","modelPath","existsSync","floor","random","threads","max","require$$4","cpus","absolutePath","resolve","useMlock","batchSize","gpuLayers","isCancelled","params","nPredict","repeatPenalty","stop","Promise","reject","completion","tokenCount","dispose","discoverModels","directory","files","readdirSync","models","filter","file","endsWith","filePath","stats","statSync","baseName","basename","size","birthtime","hasWebAssembly","WebAssembly","instantiate","wasmModule","wasmInstance","isInitialized","initializeWasm","wasmUrl","response","fetch","ok","statusText","wasmBinary","arrayBuffer","compile","memory","Memory","initial","maximum","importObject","consoleLog","ptr","len","bytes","Uint8Array","buffer","TextDecoder","consoleError","browser","modelPathOrData","modelData","byteLength","ArrayBuffer","modelBuffer","exports","modelPtr","allocateMemory","memoryView","textBytes","TextEncoder","textPtr","resultPtr","resultView","DataView","getInt32","tokensPtr","freeMemory","tokensView","Int32Array","textLength","tokenPtr","inputTokens","inputPtr","inputView","paramsPtr","paramsView","offset","setInt32","setFloat32","generateText","checkCancellation","completed","status","getGenerationStatus","tokenView","setTimeout","startStreamingGeneration","getNextToken","hasReact","window","React","require$$3","useState","initialValue","value","setValue","newValue","useEffect","effect","deps","useCallback","callback","useRef","current","useModel","setModel","isLoading","setIsLoading","setError","loadOptions","loadedModel","err","useGeneration","isGenerating","setIsGenerating","output","setOutput","genOptions","fullOutput","wrappedOnToken","isDone","react","useChat","messages","setMessages","addUserMessage","content","userMessage","role","prevMessages","addAssistantMessage","assistantMessage","formatPrompt","systemPrompt","generateResponse","streamResponse","fullResponse","updatedMessages","lastIndex","clearMessages","createGGUFContext","createContext","GenerationContext","GGUFProvider","children","initialOptions","modelState","generationState","createElement","Provider","useGGUFModel","useContext","useGGUFGeneration","_react","MemoryManager","allocations","counter","allocated","freed","active","peak","frees","performance","hasBrowserMemoryStats","memoryUsage","hasNodeMemoryStats","allocate","type","description","record","formatBytes","free","allocation","freeAll","count","allocationIds","keys","freeByType","typeAllocations","getStats","allocatedFormatted","freedFormatted","activeFormatted","peakFormatted","activeAllocations","totalJSHeapSize","usedJSHeapSize","jsHeapSizeLimit","totalJSHeapSizeFormatted","usedJSHeapSizeFormatted","jsHeapSizeLimitFormatted","rss","heapTotal","heapUsed","external","arrayBuffers","rssFormatted","heapTotalFormatted","heapUsedFormatted","externalFormatted","arrayBuffersFormatted","getAllocations","getAllocationsByType","byType","sizeFormatted","logMemoryUsage","detailed","resetStats","decimals","dm","parseFloat","pow","toFixed","globalMemoryManager","withMemoryTracking","fn","manager","memoryManager","args","beforeStats","result","apply","memoryUsed","triggerGC","global","gc","createMemoryPool","totalSize","Buffer","alloc","freeOffset","start","view","index","indexOf","splice","used","usedFormatted","freeFormatted","totalFormatted","TokenStream","bufferSize","autoFlush","isEnded","consumers","doneCallbacks","write","_notifyConsumers","flush","writeMany","end","subscribe","onDone","consumer","cb","read","bufferedTokens","fetchWithProgress","url","onProgress","contentLength","parseInt","headers","body","reader","getReader","receivedLength","chunks","done","round","position","chunk","streaming","createTokenStream","stream","delay","jitter","maxJitter","processNextToken","tokenDelay","collectStream","streamToString","tokenStr","createChunkedLoader","chunkSize","loaded","cancelled","init","method","loadChunk","Range","percentage","loadAll","cancel","getStatus","progress","formats","formatChatPrompt","useMarkdown","userLabel","assistantLabel","systemLabel","label","formatCompletionPrompt","suffix","formatInstructions","instructions","numbered","instructionList","isArray","instruction","cleanOutput","trimWhitespace","removeExtraNewlines","removeSpecialTokens","cleaned","replace","extractJSON","extractFromText","fixBrokenJson","jsonText","jsonMatch","match","fixedJson","fixError","templatePrompt","template","variables","variable","truncatePrompt","tokenCounter","fromStart","ellipsis","effectiveMaxTokens","words","currentTokens","word","wordTokens","nodeAdapter","browserAdapter","require$$5","reactHooks","require$$6","require$$7","require$$8","require$$9","require$$10","isNode","versions","isBrowser","isReact","createInstance","dir","adapters","utils","environment","version"],"mappings":"ukBAuLA,IAAEA,EAAiB,CACnBC,UAhLA,MAUI,WAAAC,EAAYC,GAAEA,EAAEC,KAAEA,EAAIC,SAAEA,EAAW,GAAEC,QAAEA,EAAU,KAAIC,UAAEA,EAAY,OACjEC,KAAKL,GAAKA,EACVK,KAAKJ,KAAOA,EACZI,KAAKH,SAAWA,EAChBG,KAAKF,QAAUA,EACfE,KAAKD,UAAYA,EACjBC,KAAKC,SAAWC,KAAKC,MACrBH,KAAKI,UAAW,EAGXJ,KAAKH,SAASQ,OACjBL,KAAKH,SAASQ,KAAOV,GAAMC,EAAKU,MAAM,KAAKC,OAGxCP,KAAKH,SAASW,YACjBR,KAAKH,SAASW,WAAY,IAAIN,MAAOO,cAExC,CAMD,OAAAC,GACE,MAAO,CACLf,GAAIK,KAAKL,GACTU,KAAML,KAAKH,SAASQ,KACpBT,KAAMI,KAAKJ,KACXQ,SAAUJ,KAAKI,SACfH,SAAUD,KAAKC,YACZD,KAAKH,SAEX,CAKD,cAAAc,GACEX,KAAKC,SAAWC,KAAKC,KACtB,CAKD,MAAAS,GACEZ,KAAKF,QAAU,KACfE,KAAKI,UAAW,CACjB,GAwHLS,kBAlHE,MAYE,WAAAnB,EAAYoB,UACVA,EAAY,IAAGC,YACfA,EAAc,GAAGC,KACjBA,EAAO,GAAGC,KACVA,EAAO,GAAEC,kBACTA,EAAoB,IAAGC,KACvBA,EAAIC,cACJA,EAAgB,IACd,IACFpB,KAAKc,UAAYA,EACjBd,KAAKe,YAAcA,EACnBf,KAAKgB,KAAOA,EACZhB,KAAKiB,KAAOA,EACZjB,KAAKkB,kBAAoBA,EACzBlB,KAAKmB,KAAOA,EACZnB,KAAKoB,cAAgBA,CACtB,GAuFLC,aAjFE,MAOE,WAAA3B,EAAY4B,OAAEA,EAAS,GAAEC,YAAEA,EAAc,MAAS,IAChDvB,KAAKsB,OAASA,EACdtB,KAAKuB,YAAcA,EACnBvB,KAAKwB,aAAe,EACrB,CAMD,SAAAC,CAAUH,GACRtB,KAAKsB,OAAS,IAAItB,KAAKsB,UAAWA,GAG9BtB,KAAKsB,OAAOI,OAAS1B,KAAKuB,cAC5BvB,KAAKsB,OAAStB,KAAKsB,OAAOK,OAAO3B,KAAKuB,aAEzC,CAMD,cAAAK,CAAeC,GACb7B,KAAKwB,aAAaM,KAAKD,EACxB,CAMD,gBAAAE,GACE,OAAO/B,KAAKsB,OAAOI,MACpB,CAMD,iBAAAM,GACE,OAAOhC,KAAKuB,YAAcvB,KAAKsB,OAAOI,MACvC,CAKD,KAAAO,GACEjC,KAAKsB,OAAS,GACdtB,KAAKwB,aAAe,EACrB,GA0BLU,cAlBE,SAAuBtC,GAErB,MAAMuC,EAAWvC,EAAKU,MAAM,KAAKC,MAAMD,MAAM,KAAK,GAGlD,IAAI8B,EAAO,EACX,IAAK,IAAIC,EAAI,EAAGA,EAAIzC,EAAK8B,OAAQW,IAC/BD,GAASA,GAAQ,GAAKA,EAAQxC,EAAK0C,WAAWD,GAC9CD,GAAOA,EAGT,MAAO,GAAGD,KAAYI,KAAKC,IAAIJ,GAAMK,SAAS,IAAIC,UAAU,EAAG,IAChE,GC/KH,MAAMC,EAAa,CACfC,KAAM,EACNC,MAAO,EACPC,KAAM,EACNC,KAAM,EACNC,MAAO,GAIT,IAAIC,EAAS,CACXC,MAAgC,eAAzBC,QAAQC,IAAIC,SAA4BV,EAAWI,KAAOJ,EAAWK,MAC5EM,OAAQ,UACRC,WAAW,EACXC,WAAW,EACXC,eAAe,GAIjB,MAAMC,EAAS,CACbC,MAAO,OACPC,MAAO,QACPC,IAAK,QACLC,MAAO,QACPC,OAAQ,QACRC,KAAM,QACNC,QAAS,QACTC,KAAM,QACNC,MAAO,QAGPC,MAAO,QACPC,KAAM,QACNC,KAAM,QACNC,MAAO,QACPjB,OAAQ,SAIJkB,EAAW,GAiFjB,SAASC,EAAIvB,EAAOwB,EAASC,GAI3B,GAHmBhC,EAAWO,EAAM0B,eAGnB3B,EAAOC,MAAO,OAG/B,MAAM2B,EAlDR,SAAuB3B,EAAOwB,GAC5B,MAAMI,EAAQ,GAGd,GAAI7B,EAAOO,UAAW,CACpB,MAAMrD,GAAM,IAAID,MAAOO,cACnBwC,EAAOM,UACTuB,EAAMhD,KAAK,GAAG4B,EAAOa,SAASpE,KAAOuD,EAAOC,SAE5CmB,EAAMhD,KAAK,IAAI3B,KAElB,CAsBD,OAnBI8C,EAAOK,SACLL,EAAOM,UACTuB,EAAMhD,KAAK,GAAG4B,EAAOJ,UAAUL,EAAOK,UAAUI,EAAOC,SAEvDmB,EAAMhD,KAAK,IAAImB,EAAOK,YAKtBL,EAAOM,UACTuB,EAAMhD,KAAK,GAAG4B,EAAOR,EAAM6B,kBAAkB7B,KAASQ,EAAOC,SAE7DmB,EAAMhD,KAAK,IAAIoB,MAIjB4B,EAAMhD,KAAK4C,GAGJI,EAAME,KAAK,IACnB,CAgB0BC,CAAc/B,EAAOwB,GAGxCQ,EAAW,CACfhC,QACAwB,UACAC,OACAnB,UAAW,IAAItD,KACfiF,UAAWN,GASb,GANAL,EAAS1C,KAAKoD,GACVV,EAAS9C,OAnGO,KAoGlB8C,EAASY,QAIPnC,EAAOQ,cAAe,CACxB,MAAM4B,EAAgBnC,EAAM6B,cAExBO,QAAQD,QACGE,IAATZ,EACFW,QAAQD,GAAeR,EAAkBF,GAEzCW,QAAQD,GAAeR,GAGzBS,QAAQb,IAAII,EAEf,CAED,OAAOK,CACR,CAOD,SAASd,EAAMM,EAASN,GACtB,IAAIoB,EAYJ,OATEA,EADEpB,aAAiBqB,MACP,CACVpF,KAAM+D,EAAM/D,KACZqE,QAASN,EAAMM,QACfgB,MAAOtB,EAAMsB,OAGHtB,EAGPK,EAAI,QAASC,EAASc,EAC9B,CAOD,SAASnB,EAAKK,EAASC,GACrB,OAAOF,EAAI,OAAQC,EAASC,EAC7B,CAOD,SAASL,EAAKI,EAASC,GACrB,OAAOF,EAAI,OAAQC,EAASC,EAC7B,CAOD,SAASJ,EAAMG,EAASC,GACtB,OAAOF,EAAI,QAASC,EAASC,EAC9B,CA+BH,IAAEgB,EAAiB,CACfC,UA5LF,SAAmBC,EAAU,IAC3B,QAAsBN,IAAlBM,EAAQ3C,MACV,GAA6B,iBAAlB2C,EAAQ3C,MAAoB,CACrC,MAAM4C,EAAYD,EAAQ3C,MAAM0B,mBACFW,IAA1B5C,EAAWmD,KACb7C,EAAOC,MAAQP,EAAWmD,GAE7B,KAAmC,iBAAlBD,EAAQ3C,QACxBD,EAAOC,MAAQ2C,EAAQ3C,YAIJqC,IAAnBM,EAAQvC,SAAsBL,EAAOK,OAASuC,EAAQvC,aAChCiC,IAAtBM,EAAQtC,YAAyBN,EAAOM,YAAcsC,EAAQtC,gBACxCgC,IAAtBM,EAAQrC,YAAyBP,EAAOO,YAAcqC,EAAQrC,gBACpC+B,IAA1BM,EAAQpC,gBAA6BR,EAAOQ,gBAAkBoC,EAAQpC,cAC3E,EA6KCW,QACAC,OACAC,OACAC,QACAwB,QA/BF,WACE,MAAO,IAAIvB,EACZ,EA8BCwB,UAzBF,WACExB,EAAS9C,OAAS,CACnB,EAwBCuE,aAjBF,SAAsB3C,GACpB,MAAO,CACLc,MAAO,CAACM,EAASC,IAASP,EAAM,IAAId,MAAWoB,IAAWC,GAC1DN,KAAM,CAACK,EAASC,IAASN,EAAK,IAAIf,MAAWoB,IAAWC,GACxDL,KAAM,CAACI,EAASC,IAASL,EAAK,IAAIhB,MAAWoB,IAAWC,GACxDJ,MAAO,CAACG,EAASC,IAASJ,EAAM,IAAIjB,MAAWoB,IAAWC,GAE7D,EAWChC,cCvPJ,gBAAQlD,EAASyC,cAAEA,GAAkBgE,EAC/BP,EAASQ,EAiBTC,EAAe,IAAIC,IA+IzB,IAAAC,EAAiB,CACfC,UAvIFC,eAAyB5G,EAAMiG,EAAU,CAAA,EAAIY,GAC3C,IAAKA,EACH,MAAM,IAAIhB,MAAM,mEAGlB,MAAMiB,SACJA,GAAW,EAAIC,aACfA,EAAe,OAAMpF,YACrBA,EAAc,KAAIqF,UAClBA,GAAY,EACZC,QAASC,EAAajH,SACtBA,EAAW,CAAE,GACXgG,EAGEgB,EAAUC,GAAiB5E,EAActC,GAG/C,GAAI8G,GAAYN,EAAaW,IAAIF,GAAU,CACzClB,EAAOrB,KAAK,uBAAuBuC,KACnC,MAAMG,EAAcZ,EAAaa,IAAIJ,GAErC,OADAG,EAAYrG,iBACLqG,CACR,CAEDrB,EAAOrB,KAAK,uBAAuB1E,KACnC+F,EAAOpB,MAAM,mBAAoB,CAAEoC,eAAcpF,cAAaqF,cAE9D,IAEE,MAAMpH,EAAQ,IAAIC,EAAU,CAC1BE,GAAIkH,EACJjH,OACAC,SAAU,IACLA,EACH8G,eACApF,cACAqF,gBAKE9G,QAAEA,EAAOC,UAAEA,SAAoB0G,EAAQS,gBAAgBtH,EAAM,CACjE+G,eACApF,cACAqF,cAaF,OAVApH,EAAMM,QAAUA,EAChBN,EAAMO,UAAYA,EAClBP,EAAMY,UAAW,EAGbsG,GACFN,EAAae,IAAIN,EAASrH,GAG5BmG,EAAOrB,KAAK,8BAA8BuC,KACnCrH,CACR,CAAC,MAAO4E,GAEP,MADAuB,EAAOvB,MAAM,yBAAyBA,EAAMM,WACtC,IAAIe,MAAM,8BAA8BrB,EAAMM,UACrD,CACH,EAyEE0C,YAjEF,SAAqBP,EAASJ,GAC5B,GAAIL,EAAaW,IAAIF,GAAU,CAC7B,MAAMrH,EAAQ4G,EAAaa,IAAIJ,GAY/B,OATIJ,GAAWA,EAAQY,WACrBZ,EAAQY,UAAU7H,GAIpBA,EAAMoB,SACNwF,EAAakB,OAAOT,GAEpBlB,EAAOrB,KAAK,mBAAmBuC,MACxB,CACR,CAGD,OADAlB,EAAOtB,KAAK,kCAAkCwC,MACvC,CACT,EA+CEU,SAxCF,SAAkBV,GAChB,GAAIT,EAAaW,IAAIF,GAAU,CAC7B,MAAMrH,EAAQ4G,EAAaa,IAAIJ,GAE/B,OADArH,EAAMmB,iBACCnB,CACR,CACD,OAAO,IACT,EAkCEgI,WA5BF,WACE,OAAOC,MAAMC,KAAKtB,EAAauB,UAC5BC,KAAIpI,GAASA,EAAMkB,YACnBmH,MAAK,CAACC,EAAGC,IAAMA,EAAE9H,SAAW6H,EAAE7H,UACnC,EAyBE+H,YAnBF,SAAqBvB,GACnB,IAAK,MAAOI,EAASrH,KAAU4G,EAAa6B,UAEtCxB,GAAWA,EAAQY,WACrBZ,EAAQY,UAAU7H,GAGpBA,EAAMoB,SAGRwF,EAAanE,QACb0D,EAAOrB,KAAK,kCACd,GC/JA,mBAAQjD,EAAYR,kBAAEA,GAAsBqF,EACtCP,EAASQ,EAMT+B,EAAoB,IAAI7B,IAoM9B,IAAA8B,EAAiB,CACfC,SA3LF5B,eAAwBhH,EAAO6I,EAAQxC,EAAU,CAAA,EAAIY,GACnD,IAAKA,EACH,MAAM,IAAIhB,MAAM,6EAGlB,IAAKjG,IAAUA,EAAMY,WAAaZ,EAAMM,QACtC,MAAM,IAAI2F,MAAM,6BAIlByC,EAAkBf,IAAI3H,EAAMG,IAAI,GAGhC,MAAM2I,EAAoB,IAAIzH,EAAkBgF,GAEhDF,EAAOrB,KAAK,0BAA0B9E,EAAMG,MAC5CgG,EAAOpB,MAAM,UAAW8D,EAAO3F,UAAU,EAAG,MAAQ2F,EAAO3G,OAAS,IAAM,MAAQ,KAClFiE,EAAOpB,MAAM,WAAY+D,GAEzB,IAEE,MAAMhH,QAAemF,EAAQ8B,SAAS/I,EAAO6I,GAC7C1C,EAAOpB,MAAM,qBAAqBjD,EAAOI,iBAGzC,MAAM8G,EAAU,IAAInH,EAAa,CAC/BC,SACAC,YAAa/B,EAAMK,SAAS0B,aAAe,OAIzCiH,EAAQzG,oBAAsByG,EAAQjH,cACxCoE,EAAOtB,KAAK,gCAAgCmE,EAAQzG,6BAEpDyG,EAAQlH,OAASkH,EAAQlH,OAAOK,MAA6B,IAAtB6G,EAAQjH,cAIjD,MAAMkH,EAAelG,KAAKmG,IACxBJ,EAAkBxH,UAClB0H,EAAQxG,oBAAsB,IAI1BR,QAAqBiF,EAAQkC,aACjCnJ,EACAgJ,EACA,IACKF,EACHxH,UAAW2H,IAEb,IAAMP,EAAkBjB,IAAIzH,EAAMG,MAI9BiJ,QAAmBnC,EAAQoC,WAAWrJ,EAAOgC,GAGnD,OADAmE,EAAOrB,KAAK,aAAa9C,EAAaE,iBAC/BkH,CACR,CAAC,MAAOxE,GACP,GAAI8D,EAAkBjB,IAAIzH,EAAMG,IAE9B,OADAgG,EAAOrB,KAAK,4BACL,GAIT,MADAqB,EAAOvB,MAAM,sBAAsBA,EAAMM,WACnC,IAAIe,MAAM,2BAA2BrB,EAAMM,UACrD,CAAY,QACRlF,EAAMmB,gBACP,CACH,EAsHEmI,eApGFtC,eAA8BhH,EAAO6I,EAAQU,EAASlD,EAAU,CAAE,EAAEY,GAClE,IAAKA,EACH,MAAM,IAAIhB,MAAM,6EAGlB,IAAKjG,IAAUA,EAAMY,WAAaZ,EAAMM,QACtC,MAAM,IAAI2F,MAAM,6BAIlByC,EAAkBf,IAAI3H,EAAMG,IAAI,GAGhC,MAAM2I,EAAoB,IAAIzH,EAAkBgF,GAEhDF,EAAOrB,KAAK,yBAAyB9E,EAAMG,MAC3CgG,EAAOpB,MAAM,UAAW8D,EAAO3F,UAAU,EAAG,MAAQ2F,EAAO3G,OAAS,IAAM,MAAQ,KAElF,IAEE,MAAMJ,QAAemF,EAAQ8B,SAAS/I,EAAO6I,GAGvCG,EAAU,IAAInH,EAAa,CAC/BC,SACAC,YAAa/B,EAAMK,SAAS0B,aAAe,OAIzCiH,EAAQzG,oBAAsByG,EAAQjH,cACxCoE,EAAOtB,KAAK,gCAAgCmE,EAAQzG,6BACpDyG,EAAQlH,OAASkH,EAAQlH,OAAOK,MAA6B,IAAtB6G,EAAQjH,cAIjD,MAAMkH,EAAelG,KAAKmG,IACxBJ,EAAkBxH,UAClB0H,EAAQxG,oBAAsB,IAI1BgH,EAAgBxC,MAAOyC,IAC3B,IACE,MAAMC,QAAkBzC,EAAQ0C,gBAAgB3J,EAAOyJ,GACvDF,EAAQG,GAAW,EACpB,CAAC,MAAO9E,GACPuB,EAAOvB,MAAM,yBAAyBA,EAAMM,UAC7C,SAIG+B,EAAQ2C,sBACZ5J,EACAgJ,EACAQ,EACA,IACKV,EACHxH,UAAW2H,IAEb,IAAMP,EAAkBjB,IAAIzH,EAAMG,MAIpCoJ,EAAQ,IAAI,GACZpD,EAAOrB,KAAK,gCACb,CAAC,MAAOF,GACP,GAAI8D,EAAkBjB,IAAIzH,EAAMG,IAG9B,OAFAgG,EAAOrB,KAAK,gCACZyE,EAAQ,IAAI,GAMd,MAFApD,EAAOvB,MAAM,qBAAqBA,EAAMM,WACxCqE,EAAQ,IAAI,GACN,IAAItD,MAAM,qCAAqCrB,EAAMM,UAC/D,CAAY,QACRlF,EAAMmB,gBACP,CACH,EAuBE0I,iBAhBF,SAA0BC,GACxB,MAAMzC,EAA+B,iBAAdyC,EAAyBA,EAAYA,EAAU3J,GAEtE,OAAKkH,GAKLlB,EAAOrB,KAAK,oCAAoCuC,KAChDqB,EAAkBf,IAAIN,GAAS,IACxB,IANLlB,EAAOtB,KAAK,0CACL,EAMX,GCzMA,MAAMsB,EAASO,EAKf,MAAMqD,EAMJ,MAAAC,CAAOC,GACL,MAAM,IAAIhE,MAAM,kDACjB,CAOD,MAAAiE,CAAOpI,GACL,MAAM,IAAImE,MAAM,kDACjB,CAOD,WAAAkE,CAAY9H,GACV,OAAO7B,KAAK0J,OAAO,CAAC7H,GACrB,EAOH,MAAM+H,UAA2BL,EAC/B,WAAA7J,GACEmK,QACA7J,KAAK8J,QAAU,IAAIzD,IACnBrG,KAAK+J,QAAU,IAAI1D,IAGnB,IAAK,IAAIhE,EAAI,EAAGA,EAAI,IAAKA,IAAK,CAC5B,MAAM2H,EAAOC,OAAOC,aAAa7H,GACjCrC,KAAK8J,QAAQ3C,IAAI6C,EAAM3H,GACvBrC,KAAK+J,QAAQ5C,IAAI9E,EAAG2H,EACrB,CAGDhK,KAAKmK,cAAgB,CACnBC,IAAK,IACLC,IAAK,IACLC,IAAK,IACLC,IAAK,KAGPC,OAAOvC,QAAQjI,KAAKmK,eAAeM,SAAQ,EAAEpK,EAAMV,MACjDK,KAAK+J,QAAQ5C,IAAIxH,EAAI,IAAIU,KAAQ,GAEpC,CAOD,MAAAmJ,CAAOC,GACL,OAAOhC,MAAMC,KAAK+B,GAAM7B,KAAIoC,GACtBhK,KAAK8J,QAAQ/C,IAAIiD,GACZhK,KAAK8J,QAAQ7C,IAAI+C,GAEnBhK,KAAKmK,cAAcI,KAE7B,CAOD,MAAAb,CAAOpI,GACL,OAAOA,EAAOsG,KAAI/F,GACZ7B,KAAK+J,QAAQhD,IAAIlF,GACZ7B,KAAK+J,QAAQ9C,IAAIpF,GAEnB,KACNmD,KAAK,GACT,EAOH,MAAM0F,UAAqBnB,EAOzB,WAAA7J,CAAYuD,GACV4G,QACA7J,KAAK2K,MAAQ,IAAItE,IAAImE,OAAOvC,QAAQhF,EAAO0H,OAAS,IAAI/C,KAAI,EAAEgD,EAAGC,KAAO,CAACD,EAAGE,OAAOD,OACnF7K,KAAK+J,QAAU,IAAI1D,IAAIoB,MAAMC,KAAK1H,KAAK2K,MAAM1C,WAAWL,KAAI,EAAEgD,EAAGC,KAAO,CAACA,EAAGD,MAC5E5K,KAAK+K,OAAS9H,EAAO8H,QAAU,GAC/B/K,KAAKmK,cAAgBlH,EAAOkH,eAAiB,CAAA,EAG7CnK,KAAKgL,cAAgB,IAAI3E,IACzBrG,KAAKiL,oBACN,CAKD,kBAAAA,GACEjL,KAAK+K,OAAON,SAAQ,CAACS,EAAM7I,KACzB,MAAO8I,EAAOC,GAAUF,EAClBG,EAAU,IAAIC,OAAO,cAAcH,MAAUC,cAAoB,KACvEpL,KAAKgL,cAAc7D,IAAI9E,EAAG,CACxBgJ,UACAE,YAAa,GAAGJ,IAAQC,KACxB,GAEL,CAOD,MAAA5B,CAAOC,GACL,OAAKA,EAGEA,EAAKnJ,MAAM,SAASkL,SAAQC,IACjC,IAAKA,EAAKC,OAAQ,CAGhB,MAAO,CADc1L,KAAK2K,MAAM1D,IAAIwE,IAASzL,KAAK2K,MAAM1D,IAAI,UAAY,EAEzE,CAGD,IAAI3F,EAASmG,MAAMC,KAAK+D,GAAM7D,KAAI+D,GAAKA,IAGvC,IAAK,IAAItJ,EAAI,EAAGA,EAAIrC,KAAK+K,OAAOrJ,OAAQW,IAAK,CAC3C,MAAO8I,EAAOC,GAAUpL,KAAK+K,OAAO1I,GAEpC,IAAIuJ,EAAI,EACR,KAAOA,EAAItK,EAAOI,OAAS,GACrBJ,EAAOsK,KAAOT,GAAS7J,EAAOsK,EAAI,KAAOR,EAC3C9J,EAAS,IACJA,EAAOK,MAAM,EAAGiK,GACnBT,EAAQC,KACL9J,EAAOK,MAAMiK,EAAI,IAGtBA,GAGL,CAGD,OAAOtK,EAAOsG,KAAI/F,GACZ7B,KAAK2K,MAAM5D,IAAIlF,GACV7B,KAAK2K,MAAM1D,IAAIpF,GAGjB7B,KAAK2K,MAAM1D,IAAI,UAAY,GAClC,IAtCc,EAwCnB,CAOD,MAAAyC,CAAOpI,GACL,OAAOA,EAAOsG,KAAI/F,GACZ7B,KAAK+J,QAAQhD,IAAIlF,GACZ7B,KAAK+J,QAAQ9C,IAAIpF,GAEnB,KACNmD,KAAK,GACT,EAQH,SAAS6G,EAAgB5I,GACvB,OAAKA,EAMyB,QAA1BA,EAAO6I,gBAA4B7I,EAAO8H,QAC5CpF,EAAOrB,KAAK,0BACL,IAAIoG,EAAazH,KAM1B0C,EAAOtB,KAAK,8DACL,IAAIuF,IAdTjE,EAAOtB,KAAK,2EACL,IAAIuF,EAcf,CA4CA,IAAA7J,EAAiB,CACfwJ,gBACAK,qBACAc,eACAmB,kBACAE,sBAzCFvF,eAAqC5G,EAAM6G,GACzC,IACE,IAAKA,IAAYA,EAAQuF,SACvB,MAAM,IAAIvG,MAAM,qDAGlB,MAAMd,QAAa8B,EAAQuF,SAASpM,GAEpC,OAAOiM,EADQI,KAAKC,MAAMvH,GAE3B,CAAC,MAAOP,GAEP,OADAuB,EAAOvB,MAAM,iCAAiCxE,MAASwE,EAAMM,WACtD,IAAIkF,CACZ,CACH,EA6BEuC,0BApBF3F,eAAyChH,EAAOiH,GAC9C,IACE,IAAKA,IAAYA,EAAQ2F,iBACvB,MAAM,IAAI3G,MAAM,6DAIlB,OAAOoG,QADcpF,EAAQ2F,iBAAiB5M,GAE/C,CAAC,MAAO4E,GAEP,OADAuB,EAAOvB,MAAM,2CAA2CA,EAAMM,WACvD,IAAIkF,CACZ,CACH,GCjQA,MAAMyC,EAAKnG,EACLtG,EAAOuG,EACPR,EAAS2G,EAIf,IAAIC,EACJ,IACE,MAAMC,EAAWC,QAAQ,kBACzBF,EAAaC,EAASD,UACxB,CAAE,MAAOnI,GACPuB,EAAOtB,KAAK,yEACZkI,EAAa,IACf,CA2RA,IAAAG,EAAiB,CACjBxF,gBApRAV,eAA+BmG,EAAW9G,EAAU,IAClD,IAAK0G,EACH,MAAM,IAAI9G,MAAM,gDAIlB,IAAK4G,EAAGO,WAAWD,GACjB,MAAM,IAAIlH,MAAM,yBAAyBkH,KAG3ChH,EAAOrB,KAAK,iCAAiCqI,KAG7C,MAAMpL,YACJA,EAAc,KAAIoF,aAClBA,EAAe,OAAMC,UACrBA,GAAY,EAAKzF,KACjBA,EAAOoB,KAAKsK,MAAsB,WAAhBtK,KAAKuK,UAAsBC,QAC7CA,EAAUxK,KAAKyK,IAAI,EAAGC,EAAcC,OAAOxL,OAAS,IAClDmE,EAGEsH,EAAevN,EAAKwN,QAAQT,GAElC,IAEE,MAAMnN,EAAQ,IAAI+M,EAAW,CAC3BI,UAAWQ,EACX5L,cACAJ,OACA4L,UACAM,UAAWzG,EACX0G,UAAW,IAEXC,UAAW,IAIPxN,EAAY,CAChByJ,OAAQhD,MAAOiD,GAASjK,EAAM+I,SAASkB,GACvCC,OAAQlD,MAAOlF,GAAW9B,EAAMqJ,WAAWvH,GAC3CqI,YAAanD,MAAO3E,GAAUrC,EAAMqJ,WAAW,CAAChH,KAKlD,OAFA8D,EAAOrB,KAAK,iDAAiD/C,eAAyBwL,MAE/E,CACLjN,QAASN,EACTO,YAEH,CAAC,MAAOqE,GAEP,MADAuB,EAAOvB,MAAM,+BAA+BA,EAAMM,WAC5CN,CACP,CACH,EA+NAmE,SAvNA/B,eAAwBhH,EAAOiK,GAC7B,IAAKjK,EAAMM,QACT,MAAM,IAAI2F,MAAM,iCAGlB,OAAOjG,EAAMM,QAAQyI,SAASkB,EAChC,EAkNAZ,WA1MArC,eAA0BhH,EAAO8B,GAC/B,IAAK9B,EAAMM,QACT,MAAM,IAAI2F,MAAM,iCAGlB,OAAOjG,EAAMM,QAAQ+I,WAAWvH,EAClC,EAqMA6H,gBA7LA3C,eAA+BhH,EAAOqC,GACpC,IAAKrC,EAAMM,QACT,MAAM,IAAI2F,MAAM,iCAGlB,OAAOjG,EAAMM,QAAQ+I,WAAW,CAAChH,GACnC,EAwLA8G,aA9KAnC,eAA4BhH,EAAOgJ,EAAS3C,EAAS2H,GACnD,IAAKhO,EAAMM,QACT,MAAM,IAAI2F,MAAM,iCAIlB,MAAMgI,EAAS,CACbC,SAAU7H,EAAQ/E,UAClBC,YAAa8E,EAAQ9E,YACrBC,KAAM6E,EAAQ7E,KACdC,KAAM4E,EAAQ5E,KACd0M,cAAe9H,EAAQ3E,kBACvBC,KAAM0E,EAAQ1E,KAEdyM,KAAM/H,EAAQzE,eAAiB,IAIjC,OAAO,IAAIyM,SAAQ,CAACT,EAASU,KAC3B,IAEE,MAAMC,EAAavO,EAAMM,QAAQsI,SAAS,CACxC9G,OAAQkH,EAAQlH,UACbmM,IAGCjM,EAAe,GAGrB,IAAK,MAAMK,KAASkM,EAAY,CAE9B,GAAIP,GAAeA,IAAe,CAChC7H,EAAOrB,KAAK,wBACZ,KACD,CAKD,GAHA9C,EAAaM,KAAKD,GAGdL,EAAaE,QAAU+L,EAAOC,SAChC,KAEH,CAEDN,EAAQ5L,EACT,CAAC,MAAO4C,GACP0J,EAAO1J,EACR,IAEL,EA8HAgF,sBAnHA5C,eAAqChH,EAAOgJ,EAASO,EAASlD,EAAS2H,GACrE,IAAKhO,EAAMM,QACT,MAAM,IAAI2F,MAAM,iCAIlB,MAAMgI,EAAS,CACbC,SAAU7H,EAAQ/E,UAClBC,YAAa8E,EAAQ9E,YACrBC,KAAM6E,EAAQ7E,KACdC,KAAM4E,EAAQ5E,KACd0M,cAAe9H,EAAQ3E,kBACvBC,KAAM0E,EAAQ1E,KACdyM,KAAM/H,EAAQzE,eAAiB,IAGjC,OAAO,IAAIyM,SAAQ,CAACT,EAASU,KAC3B,IAEE,MAAMC,EAAavO,EAAMM,QAAQsI,SAAS,CACxC9G,OAAQkH,EAAQlH,UACbmM,IAIL,IAAIO,EAAa,EAEjB,IAAK,MAAMnM,KAASkM,EAAY,CAE9B,GAAIP,GAAeA,IAAe,CAChC7H,EAAOrB,KAAK,kCACZ,KACD,CAOD,GAJAyE,EAAQlH,GACRmM,IAGIA,GAAcP,EAAOC,SACvB,KAEH,CAEDN,GACD,CAAC,MAAOhJ,GACP0J,EAAO1J,EACR,IAEL,EAmEAiD,UA7DA,SAAmB7H,GACbA,EAAMM,SAA4C,mBAA1BN,EAAMM,QAAQmO,UACxCzO,EAAMM,QAAQmO,UACdtI,EAAOrB,KAAK,0BAA0B9E,EAAMG,MAEhD,EAyDEuO,eAlDF1H,eAA8B2H,GAC5B,IAAK9B,EAAGO,WAAWuB,GAEjB,OADAxI,EAAOtB,KAAK,8BAA8B8J,KACnC,GAGTxI,EAAOrB,KAAK,iCAAiC6J,KAE7C,IACE,MAAMC,EAAQ/B,EAAGgC,YAAYF,GAIvBG,EAHaF,EAAMG,QAAOC,GAAQA,EAAKzJ,cAAc0J,SAAS,WAG1C7G,KAAI4G,IAC5B,MAAME,EAAW9O,EAAKoF,KAAKmJ,EAAWK,GAChCG,EAAQtC,EAAGuC,SAASF,GAIpBG,EAAWjP,EAAKkP,SAASN,EAAM,SAC/B1J,EAAQ+J,EAASvO,MAAM,KAI7B,MAAO,CACLX,GAAIkP,EACJxO,KALWyE,EAAM,GAMjBlF,KAAM8O,EACN/H,aANmB7B,EAAMpD,OAAS,EAAIoD,EAAM,GAAK,UAOjDiK,KAAMJ,EAAMI,KACZvO,UAAWmO,EAAMK,UAAUvO,cAC5B,IAIH,OADAkF,EAAOrB,KAAK,SAASgK,EAAO5M,sBACrB4M,CACR,CAAC,MAAOlK,GAEP,OADAuB,EAAOvB,MAAM,6BAA6BA,EAAMM,WACzC,EACR,CACH,GCtSA,MAAMiB,EAASO,EAGT+I,EAAwC,iBAAhBC,aAC6B,mBAA5BA,YAAYC,YAG3C,IAAIC,EAAa,KACbC,EAAe,KACfC,GAAgB,EAYpB9I,eAAe+I,EAAe1J,EAAU,IACtC,GAAIyJ,EACF,OAAO,EAGT,IAAKL,EAEH,OADAtJ,EAAOvB,MAAM,iDACN,EAGT,MAAMoL,EAAU3J,EAAQ2J,SAnBD,+DAqBvB,IACE7J,EAAOrB,KAAK,qBAAqBkL,KAGjC,MAAMC,QAAiBC,MAAMF,GAC7B,IAAKC,EAASE,GACZ,MAAM,IAAIlK,MAAM,yBAAyBgK,EAASG,cAGpD,MAAMC,QAAmBJ,EAASK,cAGlCV,QAAmBF,YAAYa,QAAQF,GAGvC,MAAMG,EAAS,IAAId,YAAYe,OAAO,CAAEC,QAAS,IAAKC,QAAS,OAGzDC,EAAe,CACnBhN,IAAK,CACH4M,SAEAK,WAAY,CAACC,EAAKC,KAChB,MAAMC,EAAQ,IAAIC,WAAWT,EAAOU,OAAQJ,EAAKC,GAC3C9G,GAAO,IAAIkH,aAAcjH,OAAO8G,GACtClL,QAAQb,IAAI,UAAUgF,IAAO,EAE/BmH,aAAc,CAACN,EAAKC,KAClB,MAAMC,EAAQ,IAAIC,WAAWT,EAAOU,OAAQJ,EAAKC,GAC3C9G,GAAO,IAAIkH,aAAcjH,OAAO8G,GACtClL,QAAQlB,MAAM,UAAUqF,IAAO,IAUrC,OAJA4F,QAAqBH,YAAYC,YAAYC,EAAYgB,GAEzDd,GAAgB,EAChB3J,EAAOrB,KAAK,kCACL,CACR,CAAC,MAAOF,GAEP,OADAuB,EAAOvB,MAAM,8BAA8BA,EAAMM,YAC1C,CACR,CACH,CA+MA8B,eAAe2C,EAAgB3J,EAAOqC,GACpC,IAAKrC,EAAMO,UACT,MAAM,IAAI0F,MAAM,mCAGlB,OAAOjG,EAAMO,UAAU4J,YAAY9H,EACrC,CA8MA,IAAAgP,EAAiB,CACftB,iBACArI,gBA5ZFV,eAA+BsK,EAAiBjL,EAAU,IAExD,IAAKyJ,EAAe,CAElB,UAD0BC,EAAe1J,GAEvC,MAAM,IAAIJ,MAAM,oCAEnB,CAED,IAAIsL,EAGJ,GAA+B,iBAApBD,EAA8B,CACvCnL,EAAOrB,KAAK,uBAAuBwM,KAEnC,IACE,MAAMrB,QAAiBC,MAAMoB,GAC7B,IAAKrB,EAASE,GACZ,MAAM,IAAIlK,MAAM,0BAA0BgK,EAASG,cAGrDmB,QAAkBtB,EAASK,cAC3BnK,EAAOrB,KAAK,kBAAkByM,EAAUC,mBACzC,CAAC,MAAO5M,GACP,MAAM,IAAIqB,MAAM,0BAA0BrB,EAAMM,UACjD,CACL,KAAS,MAAIoM,aAA2BG,aAIpC,MAAM,IAAIxL,MAAM,6CAHhBsL,EAAYD,EACZnL,EAAOrB,KAAK,8BAA8ByM,EAAUC,mBAGrD,CAED,IAEE,MAAME,EAAc,IAAIT,WAAWM,GAM7BI,EAAU9B,EAAa8B,QAGvBC,EAAWD,EAAQE,eAAeH,EAAYxP,QAG9CsO,EAASmB,EAAQnB,OACjBsB,EAAa,IAAIb,WAAWT,EAAOU,QAGzCY,EAAWnK,IAAI+J,EAAaE,GAG5B,MAAMvK,EAAUsK,EAAQ5K,UAAU6K,EAAUF,EAAYxP,OAAQmE,EAAQtE,aAAe,MAEvF,GAAIsF,EAAU,EACZ,MAAM,IAAIpB,MAAM,oCAAoCoB,KAGtDlB,EAAOrB,KAAK,yBAAyBuC,KA2FrC,MAAO,CACL/G,QAAS,CAAE+G,WACX9G,UA1FgB,CAChByJ,OAAQhD,MAAOiD,IAEb,MAAM8H,GAAY,IAAIC,aAAchI,OAAOC,GAGrCgI,EAAUN,EAAQE,eAAeE,EAAU7P,QAGjD4P,EAAWnK,IAAIoK,EAAWE,GAG1B,MAAMC,EAAYP,EAAQ5I,SAAS1B,EAAS4K,EAASF,EAAU7P,QAGzDiQ,EAAa,IAAIC,SAAS5B,EAAOU,QACjC1C,EAAa2D,EAAWE,SAASH,GAAW,GAC5CI,EAAYH,EAAWE,SAASH,EAAY,GAAG,GAG/CpQ,EAAS,GACf,IAAK,IAAIe,EAAI,EAAGA,EAAI2L,EAAY3L,IAC9Bf,EAAOQ,KAAK6P,EAAWE,SAASC,EAAgB,EAAJzP,GAAO,IAOrD,OAHA8O,EAAQY,WAAWN,GACnBN,EAAQY,WAAWL,GAEZpQ,CAAM,EAGfoI,OAAQlD,MAAOlF,IAEb,MAAMwQ,EAAYX,EAAQE,eAA+B,EAAhB/P,EAAOI,QAG1CsQ,EAAa,IAAIC,WAAWjC,EAAOU,OAAQoB,EAAWxQ,EAAOI,QACnEJ,EAAOmJ,SAAQ,CAAC5I,EAAOQ,IAAM2P,EAAW3P,GAAKR,IAG7C,MAAM6P,EAAYP,EAAQtI,WAAWhC,EAASiL,EAAWxQ,EAAOI,QAG1DiQ,EAAa,IAAIC,SAAS5B,EAAOU,QACjCwB,EAAaP,EAAWE,SAASH,GAAW,GAC5CD,EAAUE,EAAWE,SAASH,EAAY,GAAG,GAG7CH,EAAY,IAAId,WAAWT,EAAOU,OAAQe,EAASS,GACnDzI,GAAO,IAAIkH,aAAcjH,OAAO6H,GAMtC,OAHAJ,EAAQY,WAAWD,GACnBX,EAAQY,WAAWL,GAEZjI,CAAI,EAGbE,YAAanD,MAAO3E,IAElB,MAAMsQ,EAAWhB,EAAQE,eAAe,GAGtB,IAAIY,WAAWjC,EAAOU,OAAQyB,EAAU,GAChD,GAAKtQ,EAGf,MAAM6P,EAAYP,EAAQtI,WAAWhC,EAASsL,EAAU,GAGlDR,EAAa,IAAIC,SAAS5B,EAAOU,QACjCwB,EAAaP,EAAWE,SAASH,GAAW,GAC5CD,EAAUE,EAAWE,SAASH,EAAY,GAAG,GAG7CH,EAAY,IAAId,WAAWT,EAAOU,OAAQe,EAASS,GACnDzI,GAAO,IAAIkH,aAAcjH,OAAO6H,GAMtC,OAHAJ,EAAQY,WAAWI,GACnBhB,EAAQY,WAAWL,GAEZjI,CAAI,GAShB,CAAC,MAAOrF,GAEP,MADAuB,EAAOvB,MAAM,+BAA+BA,EAAMM,WAC5CN,CACP,CACH,EA8PEmE,SArPF/B,eAAwBhH,EAAOiK,GAC7B,IAAKjK,EAAMO,UACT,MAAM,IAAI0F,MAAM,mCAGlB,OAAOjG,EAAMO,UAAUyJ,OAAOC,EAChC,EAgPEZ,WAvOFrC,eAA0BhH,EAAO8B,GAC/B,IAAK9B,EAAMO,UACT,MAAM,IAAI0F,MAAM,mCAGlB,OAAOjG,EAAMO,UAAU2J,OAAOpI,EAChC,EAkOE6H,kBACAR,aAzMFnC,eAA4BhH,EAAOgJ,EAAS3C,EAAS2H,GACnD,IAAK8B,IAAkBD,EACrB,MAAM,IAAI5J,MAAM,wBAGlB,MAAM0L,EAAU9B,EAAa8B,SACvBtK,QAAEA,GAAYrH,EAAMM,QAE1B,IAEE,MAAMsS,EAAc5J,EAAQlH,OAGtB+Q,EAAWlB,EAAQE,eAAoC,EAArBe,EAAY1Q,QAG9C4Q,EAAY,IAAIL,WAAWd,EAAQnB,OAAOU,OAAQ2B,EAAUD,EAAY1Q,QAC9E0Q,EAAY3H,SAAQ,CAAC5I,EAAOQ,IAAMiQ,EAAUjQ,GAAKR,IAGjD,MAAM0Q,EAAYpB,EAAQE,eAAe,IACnCmB,EAAa,IAAIZ,SAAST,EAAQnB,OAAOU,OAAQ6B,GAEvD,IAAIE,EAAS,EACbD,EAAWE,SAASD,EAAQ5M,EAAQ/E,WAAa,KAAK,GAAO2R,GAAU,EACvED,EAAWG,WAAWF,EAAQ5M,EAAQ9E,aAAe,IAAK,GAAO0R,GAAU,EAC3ED,EAAWG,WAAWF,EAAQ5M,EAAQ7E,MAAQ,IAAK,GAAOyR,GAAU,EACpED,EAAWE,SAASD,EAAQ5M,EAAQ5E,MAAQ,IAAI,GAAOwR,GAAU,EACjED,EAAWG,WAAWF,EAAQ5M,EAAQ3E,mBAAqB,KAAK,GAAOuR,GAAU,EACjFD,EAAWE,SAASD,EAAQ5M,EAAQ1E,MAAQoB,KAAKsK,MAAsB,WAAhBtK,KAAKuK,WAAwB,GAGpF,MAAM4E,EAAYP,EAAQyB,aACxB/L,EACAwL,EACAD,EAAY1Q,OACZ6Q,GAIIM,EAAoB,OACpBrF,IAAeA,OAEjB2D,EAAQ9H,iBAAiBxC,IAClB,GAMX,IAAIiM,GAAY,EAChB,MAAMtR,EAAe,GAErB,MAAQsR,IAAcD,KAAqB,CAEzC,MAAME,EAAS5B,EAAQ6B,oBAAoBnM,GAE3C,GAAe,IAAXkM,EAAJ,CAIO,GAAe,IAAXA,EAuBT,MAFA5B,EAAQY,WAAWM,GACnBlB,EAAQY,WAAWQ,GACb,IAAI9M,MAAM,sCAAsCsN,KAvB/B,CAEvBD,GAAY,EAGZ,MAAMnB,EAAa,IAAIC,SAAST,EAAQnB,OAAOU,QACzC1C,EAAa2D,EAAWE,SAASH,GAAW,GAC5CI,EAAYH,EAAWE,SAASH,EAAY,GAAG,GAG/CuB,EAAY,IAAIhB,WAAWd,EAAQnB,OAAOU,OAAQoB,EAAW9D,GACnE,IAAK,IAAI3L,EAAI,EAAGA,EAAI2L,EAAY3L,IAC9Bb,EAAaM,KAAKmR,EAAU5Q,IAI9B8O,EAAQY,WAAWM,GACnBlB,EAAQY,WAAWQ,GACnBpB,EAAQY,WAAWL,EAC3B,CAKO,YA1BO,IAAI7D,SAAQT,GAAW8F,WAAW9F,EAAS,KA2BpD,CAED,OAAO5L,CACR,CAAC,MAAO4C,GAEP,MADAuB,EAAOvB,MAAM,oBAAoBA,EAAMM,WACjCN,CACP,CACH,EA6GEgF,sBAjGF5C,eAAqChH,EAAOgJ,EAASO,EAASlD,EAAS2H,GACrE,IAAK8B,IAAkBD,EACrB,MAAM,IAAI5J,MAAM,wBAGlB,MAAM0L,EAAU9B,EAAa8B,SACvBtK,QAAEA,GAAYrH,EAAMM,QAE1B,IAEE,MAAMsS,EAAc5J,EAAQlH,OACtB+Q,EAAWlB,EAAQE,eAAoC,EAArBe,EAAY1Q,QAC9C4Q,EAAY,IAAIL,WAAWd,EAAQnB,OAAOU,OAAQ2B,EAAUD,EAAY1Q,QAC9E0Q,EAAY3H,SAAQ,CAAC5I,EAAOQ,IAAMiQ,EAAUjQ,GAAKR,IAGjD,MAAM0Q,EAAYpB,EAAQE,eAAe,IACnCmB,EAAa,IAAIZ,SAAST,EAAQnB,OAAOU,OAAQ6B,GAEvD,IAAIE,EAAS,EAab,IAZAD,EAAWE,SAASD,EAAQ5M,EAAQ/E,WAAa,KAAK,GAAO2R,GAAU,EACvED,EAAWG,WAAWF,EAAQ5M,EAAQ9E,aAAe,IAAK,GAAO0R,GAAU,EAC3ED,EAAWG,WAAWF,EAAQ5M,EAAQ7E,MAAQ,IAAK,GAAOyR,GAAU,EACpED,EAAWE,SAASD,EAAQ5M,EAAQ5E,MAAQ,IAAI,GAAOwR,GAAU,EACjED,EAAWG,WAAWF,EAAQ5M,EAAQ3E,mBAAqB,KAAK,GAAOuR,GAAU,EACjFD,EAAWE,SAASD,EAAQ5M,EAAQ1E,MAAQoB,KAAKsK,MAAsB,WAAhBtK,KAAKuK,WAAwB,GAAO2F,GAAU,EACrGD,EAAWE,SAASD,EAAQ,GAAG,GAG/BtB,EAAQgC,yBAAyBtM,EAASwL,EAAUD,EAAY1Q,OAAQ6Q,KAG3D,CAEX,GAAI/E,GAAeA,IAAe,CAChC2D,EAAQ9H,iBAAiBxC,GACzB,KACD,CAGD,MAAMsL,EAAWhB,EAAQiC,aAAavM,GAEtC,GAAiB,IAAbsL,EAAJ,CAIO,IAAkB,IAAdA,EAET,MACK,CAEL,MAAMtQ,EAAQ,IAAI+P,SAAST,EAAQnB,OAAOU,QAAQmB,SAASM,GAAU,SAG7ChJ,EAAgB3J,EAAOqC,SACzCkH,EAAQlH,GAGdsP,EAAQY,WAAWI,EACpB,aAfO,IAAItE,SAAQT,GAAW8F,WAAW9F,EAAS,KAgBpD,CAGD+D,EAAQY,WAAWM,GACnBlB,EAAQY,WAAWQ,EACpB,CAAC,MAAOnO,GAEP,MADAuB,EAAOvB,MAAM,8BAA8BA,EAAMM,WAC3CN,CACP,CACH,EA6BEiD,UAtBF,SAAmB7H,GACjB,GAAK8P,GAAkBD,GAAiB7P,EAAMM,QAI9C,IACE,MAAM+G,QAAEA,GAAYrH,EAAMM,QAC1BuP,EAAa8B,QAAQ9J,UAAUR,GAC/BlB,EAAOrB,KAAK,0BAA0BuC,IACvC,CAAC,MAAOzC,GACPuB,EAAOvB,MAAM,wBAAwBA,EAAMM,UAC5C,CACH,GC9eA,MAAM2O,EAA6B,oBAAXC,cACkB,IAAjBA,OAAOC,OACmB,MAC/B,IAAwB,OAAlB9G,QAAQ,UAAiB,CAAO,CACtC,MAAQ,OAAO,CAAQ,CACxB,EAHgC,IAMnD,IAAI8G,EACJ,GAAIF,EACF,IACEE,EAA0B,oBAAXD,QAA0BA,OAAOC,MACxCD,OAAOC,MAAQ9G,QAAQ,QAChC,CAAC,MAAOrI,GACPkB,QAAQjB,KAAK,oDACd,CAIH,MAAMiC,EAASgG,EACTnE,EAAYqL,EACZ7N,EAASsH,EAQf,SAASwG,EAASC,GAChB,GAAIH,GAASA,EAAME,SACjB,OAAOF,EAAME,SAASC,GACjB,CAEL,IAAIC,EAAQD,EACZ,MAAME,EAAYC,IAEdF,EADsB,mBAAbE,EACDA,EAASF,GAETE,CACT,EAEH,MAAO,CAACF,EAAOC,EAChB,CACH,CAQA,SAASE,EAAUC,EAAQC,GACzB,GAAIT,GAASA,EAAMO,UACjB,OAAOP,EAAMO,UAAUC,EAAQC,GAO/B,OAHgBD,GAKpB,CASA,SAASE,EAAYC,EAAUF,GAC7B,OAAIT,GAASA,EAAMU,YACVV,EAAMU,YAAYC,EAAUF,GAG5BE,CAEX,CAQA,SAASC,EAAOT,GACd,OAAIH,GAASA,EAAMY,OACVZ,EAAMY,OAAOT,GAGb,CAAEU,QAASV,EAEtB,CASA,SAASW,GAAS5N,EAASZ,EAAU,IACnC,MAAOrG,EAAO8U,GAAYb,EAAS,OAC5Bc,EAAWC,GAAgBf,GAAS,IACpCrP,EAAOqQ,GAAYhB,EAAS,MAG7BlN,EAAY0N,GAAYzN,MAAO5G,EAAM8U,EAAc,CAAA,KACvDF,GAAa,GACbC,EAAS,MAET,IACE,MAAME,QAAoBrO,EAAOC,UAAU3G,EAAM8U,EAAajO,GAE9D,OADA6N,EAASK,GACFA,CACR,CAAC,MAAOC,GACP,MAAMlQ,EAAUkQ,aAAenP,MAAQmP,EAAIlQ,QAAUuF,OAAO2K,GAE5D,MADAH,EAAS/P,GACHkQ,CACZ,CAAc,QACRJ,GAAa,EACd,IACA,CAAC/N,IAGEW,EAAc6M,GAAYzN,UAC9B,GAAIhH,EACF,IAGE,aAFM8G,EAAOc,YAAY5H,EAAMG,GAAI8G,GACnC6N,EAAS,OACF,CACR,CAAC,MAAOM,GACP,MAAMlQ,EAAUkQ,aAAenP,MAAQmP,EAAIlQ,QAAUuF,OAAO2K,GAE5D,MADAH,EAAS/P,GACHkQ,CACP,CAEH,OAAO,CAAK,GACX,CAACpV,EAAOiH,IAGLyH,EAAiB+F,GAAYzN,MAAO2H,IACxC,IAAK1H,EAAQyH,eAEX,OADAuG,EAAS,qDACF,GAGT,IACE,aAAahO,EAAQyH,eAAeC,EACrC,CAAC,MAAOyG,GACP,MAAMlQ,EAAUkQ,aAAenP,MAAQmP,EAAIlQ,QAAUuF,OAAO2K,GAE5D,MADAH,EAAS/P,GACHkQ,CACP,IACA,CAACnO,IAgBJ,OAbAqN,GAAU,IACD,KACL,GAAItU,GAASA,EAAMG,GAEjB,IACE2G,EAAOc,YAAY5H,EAAMG,GAAI8G,EAC9B,CAAC,MAAOmO,GACPjP,EAAOtB,KAAK,sCAAsCuQ,EAAIlQ,UACvD,CACF,GAEF,CAAClF,EAAOiH,IAEJ,CACLjH,QACA+U,YACAnQ,QACAmC,YACAa,cACA8G,iBAEJ,CASA,SAAS2G,GAAcpO,EAASZ,EAAU,IACxC,MAAOiP,EAAcC,GAAmBtB,GAAS,IAC1CrP,EAAOqQ,GAAYhB,EAAS,OAC5BuB,EAAQC,GAAaxB,EAAS,IAC/BjG,EAAc2G,GAAO,GAGrB/L,EAAW6L,GAAYzN,MAAOhH,EAAO6I,EAAQ6M,EAAa,MAC9D,IAAK1V,IAAUA,EAAMY,SAAU,CAC7B,MAAMgE,EAAQ,IAAIqB,MAAM,6BAExB,MADAgP,EAASrQ,EAAMM,SACTN,CACP,CAED2Q,GAAgB,GAChBN,EAAS,MACTQ,EAAU,IACVzH,EAAY4G,SAAU,EAEtB,IACE,MAAM3E,QAAiBtH,EAAUC,SAC/B5I,EACA6I,EACA6M,EACAzO,GACA,IAAM+G,EAAY4G,UAIpB,OADAa,EAAUxF,GACHA,CACR,CAAC,MAAOmF,GACP,IAAKpH,EAAY4G,QAAS,CACxB,MAAM1P,EAAUkQ,aAAenP,MAAQmP,EAAIlQ,QAAUuF,OAAO2K,GAE5D,MADAH,EAAS/P,GACHkQ,CACP,CACD,MAAO,EACb,CAAc,QACRG,GAAgB,EACjB,IACA,CAACtO,IAGEqC,EAAiBmL,GAAYzN,MAAOhH,EAAO6I,EAAQU,EAASmM,EAAa,MAC7E,IAAK1V,IAAUA,EAAMY,SAAU,CAC7B,MAAMgE,EAAQ,IAAIqB,MAAM,6BAExB,MADAgP,EAASrQ,EAAMM,SACTN,CACP,CAED2Q,GAAgB,GAChBN,EAAS,MACTQ,EAAU,IACVzH,EAAY4G,SAAU,EAEtB,IAAIe,EAAa,GAGjB,MAAMC,EAAiB,CAACvT,EAAOwT,KACxB7H,EAAY4G,UACfe,GAActT,EACdoT,EAAUE,GACVpM,EAAQlH,EAAOwT,GAChB,EAGH,IAUE,aATMlN,EAAUW,eACdtJ,EACA6I,EACA+M,EACAF,EACAzO,GACA,IAAM+G,EAAY4G,UAGbe,CACR,CAAC,MAAOP,GACP,IAAKpH,EAAY4G,QAAS,CACxB,MAAM1P,EAAUkQ,aAAenP,MAAQmP,EAAIlQ,QAAUuF,OAAO2K,GAE5D,MADAH,EAAS/P,GACHkQ,CACP,CACD,OAAOO,CACb,CAAc,QACRJ,GAAgB,EACjB,IACA,CAACtO,IASJ,MAAO,CACLqO,eACA1Q,QACA4Q,SACA5M,WACAU,iBACAO,iBAZuB4K,GAAY,KACnCzG,EAAY4G,SAAU,EACtBW,GAAgB,IACT,IACN,IAUL,CAmQA,IAAAO,GAAiB,CAEfjB,YACAQ,iBACAU,QA9PF,SAAiB9O,EAASZ,EAAU,IAClC,MAAO2P,EAAUC,GAAehC,EAAS,KAClCqB,EAAcC,GAAmBtB,GAAS,IAC1CrP,EAAOqQ,GAAYhB,EAAS,MAG7BiC,EAAiBzB,GAAa0B,IAClC,MAAMC,EAAc,CAClBjW,GAAIO,KAAKC,MACT0V,KAAM,OACNF,UACAnS,WAAW,IAAItD,MAAOO,eAIxB,OADAgV,GAAYK,GAAgB,IAAIA,EAAcF,KACvCA,CAAW,GACjB,IAGGG,EAAsB9B,GAAa0B,IACvC,MAAMK,EAAmB,CACvBrW,GAAIO,KAAKC,MACT0V,KAAM,YACNF,UACAnS,WAAW,IAAItD,MAAOO,eAIxB,OADAgV,GAAYK,GAAgB,IAAIA,EAAcE,KACvCA,CAAgB,GACtB,IAGGC,EAAehC,GAAaiC,IAChC,IAAI7N,EAAS,GAGT6N,IACF7N,GAAU,WAAW6N,SAIvB,IAAK,MAAMxR,KAAW8Q,EACC,SAAjB9Q,EAAQmR,KACVxN,GAAU,SAAS3D,EAAQiR,cACD,cAAjBjR,EAAQmR,KACjBxN,GAAU,cAAc3D,EAAQiR,cACN,WAAjBjR,EAAQmR,OACjBxN,GAAU,WAAW3D,EAAQiR,eAOjC,OAFAtN,GAAU,cAEHA,CAAM,GACZ,CAACmN,IAGEW,EAAmBlC,GAAYzN,MACnChH,EACA0W,EAAe,GACfhB,EAAa,CAAE,KAEf,IAAK1V,IAAUA,EAAMY,SAAU,CAC7B,MAAMgE,EAAQ,IAAIqB,MAAM,6BAExB,MADAgP,EAASrQ,EAAMM,SACTN,CACP,CAED2Q,GAAgB,GAChBN,EAAS,MAET,IAEE,MAAMpM,EAAS4N,EAAaC,GAGtBzG,QAAiBtH,EAAUC,SAC/B5I,EACA6I,EACA6M,EACAzO,GAMF,OAFAsP,EAAoBtG,GAEbA,CACR,CAAC,MAAOmF,GACP,MAAMlQ,EAAUkQ,aAAenP,MAAQmP,EAAIlQ,QAAUuF,OAAO2K,GAE5D,MADAH,EAAS/P,GACHkQ,CACZ,CAAc,QACRG,GAAgB,EACjB,IACA,CAACkB,EAAcF,EAAqBtP,IAGjC2P,EAAiBnC,GAAYzN,MACjChH,EACAuJ,EACAmN,EAAe,GACfhB,EAAa,CAAE,KAEf,IAAK1V,IAAUA,EAAMY,SAAU,CAC7B,MAAMgE,EAAQ,IAAIqB,MAAM,6BAExB,MADAgP,EAASrQ,EAAMM,SACTN,CACP,CAED2Q,GAAgB,GAChBN,EAAS,MAGT,MAAMuB,EAAmB,CACvBrW,GAAIO,KAAKC,MACT0V,KAAM,YACNF,QAAS,GACTnS,WAAW,IAAItD,MAAOO,eAGxBgV,GAAYK,GAAgB,IAAIA,EAAcE,KAE9C,IAAIK,EAAe,GAGnB,MAAMjB,EAAiB,CAACvT,EAAOwT,KACxBA,EAqBHtM,EAAQ,IAAI,IApBZsN,GAAgBxU,EAGhB4T,GAAYK,IACV,MAAMQ,EAAkB,IAAIR,GACtBS,EAAYD,EAAgB5U,OAAS,EAS3C,OAPI6U,GAAa,GAAKD,EAAgBC,GAAW5W,KAAOqW,EAAiBrW,KACvE2W,EAAgBC,GAAa,IACxBD,EAAgBC,GACnBZ,QAASU,IAINC,CAAe,IAIxBvN,EAAQlH,EAAOwT,GAGhB,EAGH,IAEE,MAAMhN,EAAS4N,EAAaC,GAW5B,aARM/N,EAAUW,eACdtJ,EACA6I,EACA+M,EACAF,EACAzO,GAGK4P,CACR,CAAC,MAAOzB,GACP,MAAMlQ,EAAUkQ,aAAenP,MAAQmP,EAAIlQ,QAAUuF,OAAO2K,GAE5D,MADAH,EAAS/P,GACHkQ,CACZ,CAAc,QACRG,GAAgB,EACjB,IACA,CAACkB,EAAcxP,IAGZ+P,EAAgBvC,GAAY,KAChCwB,EAAY,GAAG,GACd,IAEH,MAAO,CACLD,WACAV,eACA1Q,QACAsR,iBACAK,sBACAI,mBACAC,iBACAI,gBACAP,eAEJ,EAkEEQ,kBAAmBpD,EAzDrB,SAA2B5M,GACzB,IAAK8M,EACH,MAAM,IAAI9N,MAAM,kDAIlB,MAAMpE,EAAekS,EAAMmD,cAAc,MACnCC,EAAoBpD,EAAMmD,cAAc,MAoC9C,MAAO,CACLE,aAlCmB,EAAGC,WAAUC,iBAAiB,CAAE,MACnD,MAAMC,EAAa1C,GAAS5N,EAASqQ,GAC/BE,EAAkBnC,GAAcpO,EAASqQ,GAG/C,OAAOvD,EAAM0D,cACX5V,EAAa6V,SACb,CAAEvD,MAAOoD,GACTxD,EAAM0D,cACJN,EAAkBO,SAClB,CAAEvD,MAAOqD,GACTH,GAEH,EAsBDM,aAlBmB,KACnB,MAAM3O,EAAU+K,EAAM6D,WAAW/V,GACjC,IAAKmH,EACH,MAAM,IAAI/C,MAAM,mDAElB,OAAO+C,CAAO,EAcd6O,kBAXwB,KACxB,MAAM7O,EAAU+K,EAAM6D,WAAWT,GACjC,IAAKnO,EACH,MAAM,IAAI/C,MAAM,wDAElB,OAAO+C,CAAO,EAQlB,OASoDjD,EAGlD+R,OAAQ,CACN7D,WACAK,YACAG,cACAE,WCvjBJ,MAAMxO,GAASO,EA0Bf,MAAMqR,GACJ,WAAA7X,GAKEM,KAAKwX,YAAc,IAAInR,IAMvBrG,KAAKyX,QAAU,EAMfzX,KAAK2O,MAAQ,CACX+I,UAAW,EACXC,MAAO,EACPC,OAAQ,EACRC,KAAM,EACNL,YAAa,EACbM,MAAO,GAIa,oBAAXxE,QACPA,OAAOyE,aACP,WAAYzE,OAAOyE,YACrB/X,KAAKgY,uBAAwB,EAE7BhY,KAAKgY,uBAAwB,EAIR,oBAAZ7U,SACPA,QAAQ8U,YACVjY,KAAKkY,oBAAqB,EAE1BlY,KAAKkY,oBAAqB,CAE7B,CAUD,QAAAC,CAASpJ,GAAMqJ,KAAEA,EAAIC,YAAEA,GAAgB,CAAA,GACrC,MAAM1Y,EAAK,YAAWK,KAAKyX,QAGrBa,EAAS,CACb3Y,KACAoP,OACAqJ,KAAMA,GAAQ,UACd5U,UAAWtD,KAAKC,MAChBkY,YAAaA,GAAe,IAkB9B,OAdArY,KAAKwX,YAAYrQ,IAAIxH,EAAI2Y,GAGzBtY,KAAK2O,MAAM+I,WAAa3I,EACxB/O,KAAK2O,MAAMiJ,QAAU7I,EACrB/O,KAAK2O,MAAM6I,cAGPxX,KAAK2O,MAAMiJ,OAAS5X,KAAK2O,MAAMkJ,OACjC7X,KAAK2O,MAAMkJ,KAAO7X,KAAK2O,MAAMiJ,QAG/BjS,GAAOpB,MAAM,qBAAqBgU,GAAYxJ,OAAUqJ,GAAQ,aAAaC,EAAc,MAAMA,IAAgB,MAE1G1Y,CACR,CAOD,IAAA6Y,CAAK7Y,GAEH,MAAM8Y,EAAazY,KAAKwX,YAAYvQ,IAAItH,GAExC,OAAK8Y,GAMLzY,KAAK2O,MAAMgJ,OAASc,EAAW1J,KAC/B/O,KAAK2O,MAAMiJ,QAAUa,EAAW1J,KAChC/O,KAAK2O,MAAMmJ,QAGX9X,KAAKwX,YAAYlQ,OAAO3H,GAExBgG,GAAOpB,MAAM,iBAAiBgU,GAAYE,EAAW1J,UAAU0J,EAAWL,QAAQK,EAAWJ,YAAc,MAAMI,EAAWJ,cAAgB,OAErI,IAdL1S,GAAOtB,KAAK,yCAAyC1E,MAC9C,EAcV,CAMD,OAAA+Y,GACE,MAAMC,EAAQ3Y,KAAKwX,YAAYzI,KAE/B,GAAc,IAAV4J,EACF,OAAO,EAGThT,GAAOrB,KAAK,eAAeqU,yBAA6BJ,GAAYvY,KAAK2O,MAAMiJ,YAE/E,MAAMgB,EAAgBnR,MAAMC,KAAK1H,KAAKwX,YAAYqB,QAClD,IAAK,MAAMlZ,KAAMiZ,EACf5Y,KAAKwY,KAAK7Y,GAGZ,OAAOgZ,CACR,CAOD,UAAAG,CAAWV,GACT,IAAIO,EAAQ,EAEZ,MAAMI,EAAkBtR,MAAMC,KAAK1H,KAAKwX,YAAY7P,UACjD4G,QAAOkK,GAAcA,EAAWL,OAASA,IAE5CzS,GAAOrB,KAAK,WAAWyU,EAAgBrX,WAAW0W,kBAElD,IAAK,MAAMK,KAAcM,EACnB/Y,KAAKwY,KAAKC,EAAW9Y,KACvBgZ,IAIJ,OAAOA,CACR,CAMD,QAAAK,GAEE,MAAMrK,EAAQ,IACT3O,KAAK2O,MAERsK,mBAAoBV,GAAYvY,KAAK2O,MAAM+I,WAC3CwB,eAAgBX,GAAYvY,KAAK2O,MAAMgJ,OACvCwB,gBAAiBZ,GAAYvY,KAAK2O,MAAMiJ,QACxCwB,cAAeb,GAAYvY,KAAK2O,MAAMkJ,MACtCwB,kBAAmBrZ,KAAKwX,YAAYzI,MAItC,GAAI/O,KAAKgY,sBAAuB,CAC9B,MAAMhI,EAASsD,OAAOyE,YAAY/H,OAClCrB,EAAMkC,QAAU,CACdyI,gBAAiBtJ,EAAOsJ,gBACxBC,eAAgBvJ,EAAOuJ,eACvBC,gBAAiBxJ,EAAOwJ,gBACxBC,yBAA0BlB,GAAYvI,EAAOsJ,iBAC7CI,wBAAyBnB,GAAYvI,EAAOuJ,gBAC5CI,yBAA0BpB,GAAYvI,EAAOwJ,iBAEhD,CAGD,GAAIxZ,KAAKkY,mBAAoB,CAC3B,MAAMlI,EAAS7M,QAAQ8U,cACvBtJ,EAAMjC,KAAO,CACXkN,IAAK5J,EAAO4J,IACZC,UAAW7J,EAAO6J,UAClBC,SAAU9J,EAAO8J,SACjBC,SAAU/J,EAAO+J,SACjBC,aAAchK,EAAOgK,aACrBC,aAAc1B,GAAYvI,EAAO4J,KACjCM,mBAAoB3B,GAAYvI,EAAO6J,WACvCM,kBAAmB5B,GAAYvI,EAAO8J,UACtCM,kBAAmB7B,GAAYvI,EAAO+J,UACtCM,sBAAuB9B,GAAYvI,EAAOgK,cAE7C,CAED,OAAOrL,CACR,CAMD,cAAA2L,GACE,OAAO7S,MAAMC,KAAK1H,KAAKwX,YAAY7P,SACpC,CAMD,oBAAA4S,GACE,MAAMC,EAAS,CAAA,EAEf,IAAK,MAAM/B,KAAczY,KAAKwX,YAAY7P,SAAU,CAClD,MAAMyQ,EAAOK,EAAWL,MAAQ,UAE3BoC,EAAOpC,KACVoC,EAAOpC,GAAQ,CACbO,MAAO,EACP5J,KAAM,EACNyI,YAAa,KAIjBgD,EAAOpC,GAAMO,QACb6B,EAAOpC,GAAMrJ,MAAQ0J,EAAW1J,KAChCyL,EAAOpC,GAAMZ,YAAY1V,KAAK2W,EAC/B,CAGD,IAAK,MAAML,KAAQoC,EACjBA,EAAOpC,GAAMqC,cAAgBlC,GAAYiC,EAAOpC,GAAMrJ,MAGxD,OAAOyL,CACR,CAMD,cAAAE,CAAeC,GAAW,GACxB,MAAMhM,EAAQ3O,KAAKgZ,WA8BnB,GA5BArT,GAAOrB,KAAK,yBACZqB,GAAOrB,KAAK,sBAAsBqK,EAAMsK,sBACxCtT,GAAOrB,KAAK,kBAAkBqK,EAAMuK,kBACpCvT,GAAOrB,KAAK,aAAaqK,EAAMwK,mBAC/BxT,GAAOrB,KAAK,WAAWqK,EAAMyK,iBAC7BzT,GAAOrB,KAAK,kBAAkBqK,EAAM6I,eACpC7R,GAAOrB,KAAK,YAAYqK,EAAMmJ,SAC9BnS,GAAOrB,KAAK,yBAAyBqK,EAAM0K,qBAGvC1K,EAAMkC,UACRlL,GAAOrB,KAAK,mBACZqB,GAAOrB,KAAK,mBAAmBqK,EAAMkC,QAAQ6I,2BAC7C/T,GAAOrB,KAAK,oBAAoBqK,EAAMkC,QAAQ4I,4BAC9C9T,GAAOrB,KAAK,oBAAoBqK,EAAMkC,QAAQ8I,6BAI5ChL,EAAMjC,OACR/G,GAAOrB,KAAK,mBACZqB,GAAOrB,KAAK,UAAUqK,EAAMjC,KAAKuN,gBACjCtU,GAAOrB,KAAK,iBAAiBqK,EAAMjC,KAAKwN,sBACxCvU,GAAOrB,KAAK,gBAAgBqK,EAAMjC,KAAKyN,qBACvCxU,GAAOrB,KAAK,eAAeqK,EAAMjC,KAAK0N,qBACtCzU,GAAOrB,KAAK,oBAAoBqK,EAAMjC,KAAK2N,0BAIzCM,EAAU,CACZ,MAAMH,EAASxa,KAAKua,uBAEpB5U,GAAOrB,KAAK,wBACZ,IAAK,MAAM8T,KAAQoC,EACjB7U,GAAOrB,KAAK,KAAK8T,MAASoC,EAAOpC,GAAMO,sBAAsB6B,EAAOpC,GAAMqC,iBAG5E,GAAiB,SAAbE,EAAqB,CACvBhV,GAAOrB,KAAK,oBACZ,IAAK,MAAMmU,KAAczY,KAAKwX,YAAY7P,SACxChC,GAAOrB,KAAK,KAAKmU,EAAW9Y,OAAO4Y,GAAYE,EAAW1J,UAAU0J,EAAWL,QAAQK,EAAWJ,YAAc,MAAMI,EAAWJ,cAAgB,KAEpJ,CACF,CACF,CAKD,UAAAuC,GAEE,MAAMhD,EAAS5X,KAAK2O,MAAM+I,UAAY1X,KAAK2O,MAAMgJ,MAGjD3X,KAAK2O,MAAQ,CACX+I,UAAWE,EACXD,MAAO,EACPC,SACAC,KAAMD,EACNJ,YAAaxX,KAAKwX,YAAYzI,KAC9B+I,MAAO,GAGTnS,GAAOpB,MAAM,qBACd,EASH,SAASgU,GAAY/H,EAAOqK,EAAW,GACrC,GAAc,IAAVrK,EAAa,MAAO,UAExB,MACMsK,EAAKD,EAAW,EAAI,EAAIA,EAGxBxY,EAAIE,KAAKsK,MAAMtK,KAAKkC,IAAI+L,GAASjO,KAAKkC,IAJlC,OAMV,MAAO,GAAGsW,YAAYvK,EAAQjO,KAAKyY,IANzB,KAMgC3Y,IAAI4Y,QAAQH,OAJxC,CAAC,QAAS,KAAM,KAAM,KAAM,KAAM,KAAM,KAAM,KAAM,MAIEzY,IACtE,CA2JA,MAAM6Y,GAAsB,IAAI3D,GAEhC,IAAAvH,GAAiB,CACfuH,iBACAgB,eACA4C,mBArJF,SAA4BC,EAAIvV,EAAU,IACxC,MAAMwV,EAAUxV,EAAQyV,eAAiBJ,GACnC9C,EAAOvS,EAAQuS,MAAQ,WACvBC,EAAcxS,EAAQwS,aAAe+C,EAAG/a,MAAQ,qBAEtD,OAAOmG,kBAAkB+U,GAEvB,MAAMC,EAAcH,EAAQrC,WAGtBrZ,EAAK0b,EAAQlD,SAAS,EAAG,CAC7BC,OACAC,YAAa,kBAAkBA,MAGjC,IAEE,MAAMoD,QAAeL,EAAGM,MAAM1b,KAAMub,GAI9BI,EADaN,EAAQrC,WACGpB,OAAS4D,EAAY5D,OASnD,OANAyD,EAAQ7C,KAAK7Y,GACb0b,EAAQlD,SAASwD,EAAY,CAC3BvD,OACAC,YAAa,cAAcA,MAGtBoD,CACR,CAAC,MAAOrX,GAGP,MADAiX,EAAQ7C,KAAK7Y,GACPyE,CACP,CACL,CACA,EAiHEwX,UA3GF,WACE,YAAsB,IAAXC,GAA0BA,EAAOC,IAE1CD,EAAOC,KACPnW,GAAOpB,MAAM,yCACN,GACoB,oBAAX+O,QAA0BA,OAAOwI,IAEjDxI,OAAOwI,KACPnW,GAAOpB,MAAM,yCACN,IAGToB,GAAOpB,MAAM,0EACN,EACT,EA6FEwX,iBAtFF,SAA0BC,GAExB,MAAMtL,EAAgC,oBAAhBO,YACpB,IAAIA,YAAY+K,GAChBC,OAAOC,MAAMF,GAGTxE,EAAc,GACpB,IAAI2E,EAAa,EAEjB,MAAO,CAML,QAAAhE,CAASpJ,GACP,GAAIoN,EAAapN,EAAOiN,EACtB,MAAM,IAAIvW,MAAM,6BAA6B8S,GAAYxJ,iBAAoBwJ,GAAYyD,EAAYG,iBAGvG,MAAMC,EAAQD,EACdA,GAAcpN,EAEd,MAIM0J,EAAa,CAAE2D,QAAOrN,OAAMsN,KAJC,oBAAf5L,WAClB,IAAIA,WAAWC,EAAQ0L,EAAOrN,GAC9BkN,OAAOvU,KAAKgJ,EAAQ0L,EAAOrN,IAK7B,OAFAyI,EAAY1V,KAAK2W,GAEVA,CACR,EAOD,IAAAD,CAAKC,GACH,MAAM6D,EAAQ9E,EAAY+E,QAAQ9D,GAClC,OAAe,IAAX6D,IAGJ9E,EAAYgF,OAAOF,EAAO,IAKnB,EACR,EAKD,KAAA3Y,GACE6T,EAAY9V,OAAS,EACrBya,EAAa,CACd,EAMDzb,QAAO,KACE,CACLsb,YACAS,KAAMN,EACN3D,KAAMwD,EAAYG,EAClB3E,YAAaA,EAAY9V,OACzBgb,cAAenE,GAAY4D,GAC3BQ,cAAepE,GAAYyD,EAAYG,GACvCS,eAAgBrE,GAAYyD,KAIpC,EAWEd,wBCvgBF,MAAMvV,GAASO,EAKf,MAAM2W,GAOJ,WAAAnd,CAAYmG,EAAU,IACpB7F,KAAK0Q,OAAS,GACd1Q,KAAK8c,WAAajX,EAAQiX,YAAc,KACxC9c,KAAK+c,WAAkC,IAAtBlX,EAAQkX,UACzB/c,KAAKgd,SAAU,EACfhd,KAAKid,UAAY,GACjBjd,KAAKkd,cAAgB,EACtB,CAOD,KAAAC,CAAMtb,GACJ,OAAI7B,KAAKgd,SACPrX,GAAOtB,KAAK,gDACL,IAITrE,KAAK0Q,OAAO5O,KAAKD,GAGjB7B,KAAKod,mBAGDpd,KAAK+c,WAAa/c,KAAK0Q,OAAOhP,QAAU1B,KAAK8c,YAC/C9c,KAAKqd,SAGA,EACR,CAOD,SAAAC,CAAUhc,GACR,OAAItB,KAAKgd,SACPrX,GAAOtB,KAAK,gDACL,IAITrE,KAAK0Q,OAAO5O,QAAQR,GAGpBtB,KAAKod,mBAGDpd,KAAK+c,WAAa/c,KAAK0Q,OAAOhP,QAAU1B,KAAK8c,YAC/C9c,KAAKqd,SAGA,EACR,CAKD,GAAAE,GACE,IAAIvd,KAAKgd,QAAT,CAEAhd,KAAKgd,SAAU,EACfhd,KAAKod,mBAGL,IAAK,MAAMlJ,KAAYlU,KAAKkd,cAC1B,IACEhJ,GACD,CAAC,MAAO9P,GACPuB,GAAOvB,MAAM,gCAAiCA,EAC/C,CAEHpE,KAAKkd,cAAgB,EAbI,CAc1B,CAKD,KAAAG,GACErd,KAAK0Q,OAAS,EACf,CAQD,SAAA8M,CAAUzU,EAAS0U,GACjB,MAAMC,EAAW,CAAE3U,WASnB,GARA/I,KAAKid,UAAUnb,KAAK4b,GAGhBD,GACFzd,KAAKkd,cAAcpb,KAAK2b,GAItBzd,KAAK0Q,OAAOhP,OAAS,EACvB,IAAK,MAAMG,KAAS7B,KAAK0Q,OACvB,IACE3H,EAAQlH,GAAO,EAChB,CAAC,MAAOuC,GACPuB,GAAOvB,MAAM,0BAA2BA,EACzC,CAKL,GAAIpE,KAAKgd,SAAWS,EAClB,IACEA,GACD,CAAC,MAAOrZ,GACPuB,GAAOvB,MAAM,gCAAiCA,EAC/C,CAIH,MAAO,KACLpE,KAAKid,UAAYjd,KAAKid,UAAU1O,QAAO5C,GAAKA,IAAM+R,IAClD1d,KAAKkd,cAAgBld,KAAKkd,cAAc3O,QAAOoP,GAAMA,IAAOF,GAAO,CAEtE,CAMD,IAAAG,GAEE,MADe,IAAI5d,KAAK0Q,OAEzB,CAMD,gBAAA0M,GACE,GAA8B,IAA1Bpd,KAAKid,UAAUvb,OAAnB,CAEA,KAAO1B,KAAK0Q,OAAOhP,OAAS,GAAG,CAC7B,MAAMG,EAAQ7B,KAAK0Q,OAAOtL,QAE1B,IAAK,MAAMsY,KAAY1d,KAAKid,UAC1B,IACES,EAAS3U,QAAQlH,GAAO,EACzB,CAAC,MAAOuC,GACPuB,GAAOvB,MAAM,0BAA2BA,EACzC,CAEJ,CAGD,GAAIpE,KAAKgd,QAAS,CAChB,IAAK,MAAMU,KAAY1d,KAAKid,UAC1B,IACES,EAAS3U,QAAQ,MAAM,EACxB,CAAC,MAAO3E,GACPuB,GAAOvB,MAAM,0BAA2BA,EACzC,CAGHpE,KAAKid,UAAY,EAClB,CAzBuC,CA0BzC,CAMD,MAAA5H,GACE,OAAOrV,KAAKgd,OACb,CAMD,cAAAa,GACE,OAAO7d,KAAK0Q,OAAOhP,MACpB,EAuIH8E,eAAesX,GAAkBC,EAAKC,GAEpC,GAAqB,mBAAVtO,MACT,MAAM,IAAIjK,MAAM,+CAGlB,MAAMgK,QAAiBC,MAAMqO,GAE7B,IAAKtO,EAASE,GACZ,MAAM,IAAIlK,MAAM,cAAcgK,EAASsD,WAAWtD,EAASG,cAI7D,MAAMqO,EAAgBC,SAASzO,EAAS0O,QAAQlX,IAAI,mBAAqB,IAAK,IAG9E,IAAKwI,EAAS2O,OAASH,EACrB,OAAOxO,EAASK,cAIlB,MAAMuO,EAAS5O,EAAS2O,KAAKE,YAC7B,IAAIC,EAAiB,EACrB,MAAMC,EAAS,GAGf,OAAa,CACX,MAAMC,KAAEA,EAAI9K,MAAEA,SAAgB0K,EAAOT,OAErC,GAAIa,EACF,MAOF,GAJAD,EAAO1c,KAAK6R,GACZ4K,GAAkB5K,EAAMjS,OAGpBsc,EAAY,CAEdA,EAAWO,EAAgBN,EADRA,EAAgB1b,KAAKmc,MAAOH,EAAiBN,EAAiB,KAAO,EAEzF,CACF,CAGD,MAAMxC,EAAS,IAAIhL,WAAW8N,GAC9B,IAAII,EAAW,EAEf,IAAK,MAAMC,KAASJ,EAClB/C,EAAOtU,IAAIyX,EAAOD,GAClBA,GAAYC,EAAMld,OAGpB,OAAO+Z,EAAO/K,MAChB,CAsJA,IAAAmO,GAAiB,CACfhC,eACAiC,kBAxUF,SAA2Bxd,EAAQuE,EAAU,IAC3C,MAAMkZ,EAAS,IAAIlC,GAEnB,IAAKvb,GAA4B,IAAlBA,EAAOI,OAEpB,OADAqd,EAAOxB,MACAwB,EAGT,MAAMC,EAAQnZ,EAAQmZ,OAAS,EACzBC,EAASpZ,EAAQoZ,SAAU,EAC3BC,EAAYrZ,EAAQqZ,WAAa,GAEvC,IAAI5C,EAAQ,EAkCZ,OARI0C,EAAQ,EACV9L,YAzBF,SAASiM,IACP,GAAI7C,GAAShb,EAAOI,OAClBqd,EAAOxB,WAMT,GAFAwB,EAAO5B,MAAM7b,EAAOgb,MAEhBA,EAAQhb,EAAOI,OAAQ,CACzB,IAAI0d,EAAaJ,EAGbC,IACFG,GAAc7c,KAAKuK,SAAWoS,EAAY,EAAIA,EAC9CE,EAAa7c,KAAKyK,IAAI,EAAGoS,IAG3BlM,WAAWiM,EAAkBC,EACnC,MACML,EAAOxB,KAEV,GAI8B,IAG7BwB,EAAOzB,UAAUhc,GACjByd,EAAOxB,OAGFwB,CACT,EA0REM,cAlRF,SAAuBN,EAAQf,EAAa,MAC1C,OAAO,IAAInQ,SAAQ,CAACT,EAASU,KAC3B,MAAMxM,EAAS,GAEfyd,EAAOvB,WACL,CAAC3b,EAAOwT,KACN,GAAc,OAAVxT,IACFP,EAAOQ,KAAKD,GAERmc,GACF,IACEA,EAAW1c,EAAOI,OAAQG,EAC3B,CAAC,MAAOuC,GAER,CAIDiR,GACFjI,EAAQ9L,EACT,IAEH,IAAM8L,EAAQ9L,IACf,GAEL,EA0PEge,eAjPF,SAAwBP,EAAQhV,EAAUE,OAAQ+T,EAAa,MAC7D,IAAIvC,EAAS,GAEb,OAAO,IAAI5N,SAAQ,CAACT,EAASU,KAC3BiR,EAAOvB,WACL,CAAC3b,EAAOwT,KACN,GAAc,OAAVxT,EAAgB,CAClB,MAAM0d,EAAWxV,EAAQlI,GAGzB,GAFA4Z,GAAU8D,EAENvB,EACF,IACEA,EAAWvC,EAAO/Z,OAAQ6d,EAC3B,CAAC,MAAOnb,GAER,CAEJ,CAEGiR,GACFjI,EAAQqO,EACT,IAEH,IAAMrO,EAAQqO,IACf,GAEL,EAwNEqC,qBACA0B,oBAnJF,SAA6BzB,EAAK0B,EAAY,SAAkBzB,GAC9D,IAAIC,EAAgB,EAChByB,EAAS,EACTC,GAAY,EAMhBnZ,eAAeoZ,IACb,IAEE,MAAMnQ,QAAiBC,MAAMqO,EAAK,CAAE8B,OAAQ,SAE5C,IAAKpQ,EAASE,GACZ,MAAM,IAAIlK,MAAM,yBAAyBgK,EAASsD,UAAUtD,EAASG,cAIvE,OADAqO,EAAgBC,SAASzO,EAAS0O,QAAQlX,IAAI,mBAAqB,IAAK,IACjEgX,CACR,CAAC,MAAO7Z,GAEP,MADAuB,GAAOvB,MAAM,oCAAqCA,GAC5CA,CACP,CACF,CAQDoC,eAAesZ,EAAU1D,EAAOmB,GAC9B,GAAIoC,EACF,MAAM,IAAIla,MAAM,qBAIdwY,EAAgB,GAAKV,EAAMU,IAC7BV,EAAMU,GAGR,IACE,MAAMxO,QAAiBC,MAAMqO,EAAK,CAChCI,QAAS,CAAE4B,MAAO,SAAS3D,KAASmB,EAAM,OAG5C,IAAK9N,EAASE,IAA0B,MAApBF,EAASsD,OAC3B,MAAM,IAAItN,MAAM,yBAAyBgK,EAASsD,UAAUtD,EAASG,cAGvE,MAAMc,QAAejB,EAASK,cAK9B,GAFA4P,GAAUhP,EAAOM,WAEbgN,GAAcC,EAAgB,EAAG,CACnC,MAAM+B,EAAazd,KAAKmc,MAAOgB,EAASzB,EAAiB,KACzDD,EAAW0B,EAAQzB,EAAe+B,EACnC,CAED,OAAOtP,CACR,CAAC,MAAOtM,GACP,IAAKub,EAEH,MADAha,GAAOvB,MAAM,uBAAuBgY,KAASmB,IAAOnZ,GAC9CA,EAER,MAAM,IAAIqB,MAAM,oBACjB,CACF,CA+DD,MAAO,CACLma,OACAE,YACAG,QA5DFzZ,iBAOE,GALsB,IAAlByX,SACI2B,IAIc,IAAlB3B,EACF,OAAOH,GAAkBC,EAAKC,GAGhC,MAAMQ,EAAS,GACf,IAAIG,EAAW,EAEf,KAAOA,EAAWV,GAAe,CAC/B,MAAMV,EAAMhb,KAAKmG,IAAIiW,EAAWc,EAAWxB,GACrCW,QAAckB,EAAUnB,EAAUpB,GAIxC,GAHAiB,EAAO1c,KAAK8c,GACZD,EAAWpB,EAEPoC,EACF,MAAM,IAAIla,MAAM,oBAEnB,CAGD,MAAMgW,EAAS,IAAIhL,WAAWwN,GAC9B,IAAIxL,EAAS,EAEb,IAAK,MAAMmM,KAASJ,EAClB/C,EAAOtU,IAAI,IAAIsJ,WAAWmO,GAAQnM,GAClCA,GAAUmM,EAAM5N,WAGlB,OAAOyK,EAAO/K,MACf,EA0BCwP,OArBF,WACEP,GAAY,CACb,EAoBCQ,UAdF,WACE,MAAO,CACLlC,gBACAyB,SACAC,YACAS,SAAUnC,EAAgByB,EAASzB,EAAgB,EAEtD,EASH,GCrPA,IAAAoC,GAAiB,CACfC,iBApRF,SAA0B9K,EAAU3P,EAAU,IAC5C,MAAMqQ,aACJA,EAAe,GAAEqK,YACjBA,GAAc,EAAKC,UACnBA,EAAY,OAAMC,eAClBA,EAAiB,YAAWC,YAC5BA,EAAc,UACZ7a,EAEJ,IAAIwC,EAAS,GAGT6N,IAEA7N,GADEkY,EACQ,KAAKG,QAAkBxK,QAEvB,GAAGwK,MAAgBxK,SAKjC,IAAK,MAAMxR,KAAW8Q,EAAU,CAC9B,IAAImL,EAAQ,GAGS,SAAjBjc,EAAQmR,KACV8K,EAAQH,EACkB,cAAjB9b,EAAQmR,KACjB8K,EAAQF,EACkB,WAAjB/b,EAAQmR,OACjB8K,EAAQD,GAKRrY,GADEkY,EACQ,KAAKI,QAAYjc,EAAQiR,cAEzB,GAAGgL,MAAUjc,EAAQiR,aAElC,CASD,OALEtN,GADEkY,EACQ,KAAKE,QAEL,GAAGA,MAGRpY,CACT,EAoOEuY,uBAzNF,SAAgCvY,EAAQxC,EAAU,IAChD,MAAMgb,OAAEA,EAAS,GAAEvd,OAAEA,EAAS,IAAOuC,EACrC,MAAO,GAAGvC,IAAS+E,IAASwY,GAC9B,EAuNEC,mBA3MF,SAA4BC,EAAclb,EAAU,IAClD,MAAMvC,OACJA,EAAS,gBAAeud,OACxBA,EAAS,GAAEG,SACXA,GAAW,GACTnb,EAEJ,IAAIob,EAAkBxZ,MAAMyZ,QAAQH,GAAgBA,EAAe,CAACA,GAChE5b,EAAY7B,EAAS,GAAGA,MAAa,GAgBzC,OAbA2d,EAAgBxW,SAAQ,CAAC0W,EAAa7E,KAElCnX,GADE6b,EACW,GAAG1E,EAAQ,MAAM6E,MAEjB,KAAKA,KACnB,IAICN,IACF1b,GAAa,KAAK0b,KAGb1b,CACT,EAmLEic,YAvKF,SAAqB3X,EAAM5D,EAAU,IACnC,MAAMwb,eACJA,GAAiB,EAAIC,oBACrBA,GAAsB,EAAIC,oBAC1BA,GAAsB,GACpB1b,EAEJ,IAAI2b,EAAU/X,EAiBd,OAdI8X,IACFC,EAAUA,EAAQC,QAAQ,WAAY,KAIpCH,IACFE,EAAUA,EAAQC,QAAQ,UAAW,SAInCJ,IACFG,EAAUA,EAAQ9V,QAGb8V,CACT,EA+IEE,YApIF,SAAqBjY,EAAM5D,EAAU,IACnC,MAAM8b,gBACJA,GAAkB,EAAIC,cACtBA,GAAgB,GACd/b,EAEJ,IAAIgc,EAAWpY,EAGf,GAAIkY,EAAiB,CACnB,MAAMG,EAAYrY,EAAKsY,MAAM,sCACXtY,EAAKsY,MAAM,cAEzBD,IACFD,EAAWC,EAAU,GAExB,CAGD,IACE,OAAO7V,KAAKC,MAAM2V,EACnB,CAAC,MAAOzd,GAEP,IAAKwd,EACH,OAAO,KAIT,IAEE,IAAII,EAAYH,EAASJ,QAAQ,KAAM,KAQvC,OALAO,EAAYA,EAAUP,QAAQ,wBAAyB,YAGvDO,EAAYA,EAAUP,QAAQ,SAAU,KAAKA,QAAQ,SAAU,KAExDxV,KAAKC,MAAM8V,EACnB,CAAC,MAAOC,GACP,OAAO,IACR,CACF,CACH,EA0FEC,eAjFF,SAAwBC,EAAUC,EAAY,IAC5C,OAAOD,EAASV,QAAQ,cAAc,CAACM,EAAOM,SACb9c,IAAxB6c,EAAUC,GAA0BD,EAAUC,GAAYN,GAErE,EA8EEO,eAjEF,SAAwB7Y,EAAM3I,EAAWyhB,EAAc1c,EAAU,CAAA,GAC/D,MAAM2c,UACJA,GAAY,EAAIC,SAChBA,EAAW,OACT5c,EAIJ,GADmB0c,EAAa9Y,IACd3I,EAChB,OAAO2I,EAIT,MACMiZ,EAAqB5hB,EADJyhB,EAAaE,GAI9BE,EAAQlZ,EAAKnJ,MAAM,OAEzB,GAAIkiB,EAAW,CAEb,IAAI/G,EAAS,GACTmH,EAAgB,EAEpB,IAAK,IAAIvgB,EAAIsgB,EAAMjhB,OAAS,EAAGW,GAAK,EAAGA,IAAK,CAC1C,MAAMwgB,EAAOF,EAAMtgB,GACbygB,EAAaP,EAAaM,GAEhC,KAAID,EAAgBE,GAAcJ,GAIhC,MAHAjH,EAASoH,GAAQpH,EAAS,IAAMA,EAAS,IACzCmH,GAAiBE,CAIpB,CAED,OAAOL,EAAW,IAAMhH,CAC5B,CAAS,CAEL,IAAIA,EAAS,GACTmH,EAAgB,EAEpB,IAAK,IAAIvgB,EAAI,EAAGA,EAAIsgB,EAAMjhB,OAAQW,IAAK,CACrC,MAAMwgB,EAAOF,EAAMtgB,GACbygB,EAAaP,EAAaM,GAEhC,KAAID,EAAgBE,GAAcJ,GAIhC,MAHAjH,IAAWA,EAAS,IAAM,IAAMoH,EAChCD,GAAiBE,CAIpB,CAED,OAAOrH,EAAS,IAAMgH,CACvB,CACH,GC5RA,MAAMhjB,UAAEA,GAASoB,kBAAEA,GAAiBQ,aAAEA,GAAYa,cAAEA,IAAkBgE,EAChEI,GAASH,EACTgC,GAAYmE,EACZvM,GAAYyT,EAGZuP,GAAc9V,EACd+V,GAAiBC,EACjBC,GAAaC,GAGbxd,GAASyd,EACTpT,GAASqT,GACTxE,GAAYyE,GACZjD,GAAUkD,GAGVC,GAA2B,oBAAXlQ,QAA6C,oBAAZnQ,SAA2BA,QAAQsgB,UAAYtgB,QAAQsgB,SAAS/W,KACjHgX,GAA8B,oBAAXpQ,OACnBqQ,GAA4B,oBAAXrQ,aAAkD,IAAjBA,OAAOC,MAoB/D,SAASqQ,GAAend,GACtB,MAAO,CAELF,UAAW,CAAC3G,EAAMiG,IAAYS,GAAOC,UAAU3G,EAAMiG,EAASY,GAC9DW,YAAcP,GAAYP,GAAOc,YAAYP,EAASJ,GACtDc,SAAWV,GAAYP,GAAOiB,SAASV,GACvCW,WAAY,IAAMlB,GAAOkB,aACzBQ,YAAa,IAAM1B,GAAO0B,YAAYvB,GAGtC2B,SAAU,CAAC5I,EAAO6I,EAAQxC,IAAYsC,GAAUC,SAAS5I,EAAO6I,EAAQxC,EAASY,GACjFqC,eAAgB,CAACtJ,EAAO6I,EAAQU,EAASlD,IAAYsC,GAAUW,eAAetJ,EAAO6I,EAAQU,EAASlD,EAASY,GAC/G4C,iBAAmBC,GAAcnB,GAAUkB,iBAAiBC,GAG5Df,SAAU,CAAC/I,EAAOiK,IAAShD,EAAQ8B,SAAS/I,EAAOiK,GACnDZ,WAAY,CAACrJ,EAAO8B,IAAWmF,EAAQoC,WAAWrJ,EAAO8B,MAGrDmF,EAAQyH,eAAiB,CAAEA,eAAiB2V,GAAQpd,EAAQyH,eAAe2V,IAAS,MAGpFF,GAAU,CACZtP,SAAWxO,GAAYqd,GAAW7O,SAAS5N,EAASZ,GACpDgP,cAAgBhP,GAAYqd,GAAWrO,cAAcpO,EAASZ,GAC9D0P,QAAU1P,GAAYqd,GAAW3N,QAAQ9O,EAASZ,IAChD,GAER,UAOiB,IAHO+d,GAhDlBJ,GACKT,GACEW,GACFV,IAITrd,GAAOtB,KAAK,iFACL,OAgDP5E,aACAoB,qBACAQ,gBACAa,iBAGA0hB,kBAGAE,SAAU,CACRpX,KAAMqW,GACNlS,QAASmS,GACT1N,MAAO4N,IAITnjB,aAGAgkB,MAAO,CACLpe,UACAqK,UACA6O,aACAwB,YAIF2D,YAAa,CACXR,UACAE,aACAC,YAIFM,QAAS"}